{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ultimate Kubernetes Bootcamp Welcome to Kubernetes Fundamentals by School of Devops This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops. For information about the devops trainign courses visit schoolofdevops.com . Team Gourav Shah Vijayboopathy Venkat","title":"Home"},{"location":"#ultimate-kubernetes-bootcamp","text":"Welcome to Kubernetes Fundamentals by School of Devops This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops. For information about the devops trainign courses visit schoolofdevops.com .","title":"Ultimate Kubernetes Bootcamp"},{"location":"#team","text":"Gourav Shah Vijayboopathy Venkat","title":"Team"},{"location":"10_kubernetes_autoscaling/","text":"Kubernetes Horizonntal Pod Autoscaling With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics). The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user Prerequisites Metrics Server . This needs to be setup if you are using kubeadm etc. and replaces heapster starting with kubernetes version 1.8. Resource Requests and Limits. Defining CPU as well as Memory requirements for containers in Pod Spec is a must Deploying Metrics Server Kubernetes Horizontal Pod Autoscaler along with kubectl top command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component. Earlier, you would have to install a additional component called heapster in order to collect this data and feed it to the hpa controller. With 1.8 version of Kubernetes, this behavior is changed, and now metrics-server would provide this data. Metric server is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store. If you try to pull monitoring information using the following commands kubectl top pod kubectl top node it does not show it, rather gives you a error message similar to [output] Error from server (NotFound): the server could not find the requested resource (get services http:heapster:) Even though the error mentions heapster, its replaced with metrics server by default now. Deploy metric server with the following commands, git clone https://github.com/kubernetes-incubator/metrics-server.git kubectl apply -f kubectl create -f metrics-server/deploy/1.8+/ Validate kubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server' [sample output] NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/metrics-server 1 1 1 1 28m NAME READY STATUS RESTARTS AGE pod/metrics-server-6fbfb84cdd-74jww 1/1 Running 0 28m Monitoring has been setup. Fixing issues with Metrics deployment There is a known issue as off Dec 2018 with Metrics Server where is fails to work event after deploying it using above commands. This can be fixed with a patch using steps below. To apply a patch to metrics server, wget -c https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml kubectl patch deploy metrics-server -p \"$(cat k8s-metrics-server.patch.yaml)\" -n kube-system Now validate with kubectl top node kubectl top pod where expected output shoudl be similar to, # kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% vis-01 145m 7% 2215Mi 57% vis-13 36m 1% 1001Mi 26% vis-14 71m 3% 1047Mi 27% Defining Resource Requests and Limits file: vote-deploy.yaml .... spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP envFrom: - configMapRef: name: vote resources: limits: cpu: \"200m\" memory: \"250Mi\" requests: cpu: \"100m\" memory: \"50Mi\" And apply kubectl apply -f vote-deploy.yaml Exercise: Define the value of cpu.request > cpu.limit Try to apply and observe. Define the values for memory.request and memory.limit higher than the total system memory. Apply and observe the deployment and pods. Create a HPA To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image file: vote-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: vote spec: minReplicas: 4 maxReplicas: 15 targetCPUUtilizationPercentage: 40 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: vote apply kubectl apply -f vote-hpa.yaml Validate kubectl get hpa kubectl describe hpa vote kubectl get pod,deploy Load Test file: loadtest-job.yaml apiVersion: batch/v1 kind: Job metadata: name: loadtest spec: template: spec: containers: - name: siege image: schoolofdevops/loadtest:v1 command: [\"siege\", \"--concurrent=5\", \"--benchmark\", \"--time=10m\", \"http://vote\"] restartPolicy: Never backoffLimit: 4 And launch the loadtest kubectl apply -f loadtest-job.yaml To monitor while the load test is running , watch kubectl top pods To get information about the job kubectl get jobs kubectl describe job loadtest To check the load test output kubectl logs -f loadtest-xxxx [replace loadtest-xxxx with the actual pod id.] [Sample Output] ** SIEGE 3.0.8 ** Preparing 15 concurrent users for battle. root@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f ** SIEGE 3.0.8 ** Preparing 15 concurrent users for battle. ..... Lifting the server siege... done. Transactions: 41618 hits Availability: 99.98 % Elapsed time: 299.13 secs Data transferred: 127.05 MB Response time: 0.11 secs Transaction rate: 139.13 trans/sec Throughput: 0.42 MB/sec Concurrency: 14.98 Successful transactions: 41618 Failed transactions: 8 Longest transaction: 3.70 Shortest transaction: 0.00 FILE: /var/log/siege.log You can disable this annoying message by editing the .siegerc file in your home directory; change the directive 'show-logfile' to false. Now check the job status again, kubectl get jobs NAME DESIRED SUCCESSFUL AGE vote-loadtest 1 1 10m Reading List Kubernetes Monitoring Architecture Core Metrics Pipeline Metrics Server Assigning Resources to Containers and Pods Horizontal Pod Autoscaler","title":"Auto Scaling Capacity with HPA"},{"location":"10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling","text":"With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics). The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user","title":"Kubernetes Horizonntal Pod Autoscaling"},{"location":"10_kubernetes_autoscaling/#prerequisites","text":"Metrics Server . This needs to be setup if you are using kubeadm etc. and replaces heapster starting with kubernetes version 1.8. Resource Requests and Limits. Defining CPU as well as Memory requirements for containers in Pod Spec is a must","title":"Prerequisites"},{"location":"10_kubernetes_autoscaling/#deploying-metrics-server","text":"Kubernetes Horizontal Pod Autoscaler along with kubectl top command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component. Earlier, you would have to install a additional component called heapster in order to collect this data and feed it to the hpa controller. With 1.8 version of Kubernetes, this behavior is changed, and now metrics-server would provide this data. Metric server is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store. If you try to pull monitoring information using the following commands kubectl top pod kubectl top node it does not show it, rather gives you a error message similar to [output] Error from server (NotFound): the server could not find the requested resource (get services http:heapster:) Even though the error mentions heapster, its replaced with metrics server by default now. Deploy metric server with the following commands, git clone https://github.com/kubernetes-incubator/metrics-server.git kubectl apply -f kubectl create -f metrics-server/deploy/1.8+/ Validate kubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server' [sample output] NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/metrics-server 1 1 1 1 28m NAME READY STATUS RESTARTS AGE pod/metrics-server-6fbfb84cdd-74jww 1/1 Running 0 28m Monitoring has been setup.","title":"Deploying Metrics Server"},{"location":"10_kubernetes_autoscaling/#fixing-issues-with-metrics-deployment","text":"There is a known issue as off Dec 2018 with Metrics Server where is fails to work event after deploying it using above commands. This can be fixed with a patch using steps below. To apply a patch to metrics server, wget -c https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml kubectl patch deploy metrics-server -p \"$(cat k8s-metrics-server.patch.yaml)\" -n kube-system Now validate with kubectl top node kubectl top pod where expected output shoudl be similar to, # kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% vis-01 145m 7% 2215Mi 57% vis-13 36m 1% 1001Mi 26% vis-14 71m 3% 1047Mi 27%","title":"Fixing issues with Metrics deployment"},{"location":"10_kubernetes_autoscaling/#defining-resource-requests-and-limits","text":"file: vote-deploy.yaml .... spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP envFrom: - configMapRef: name: vote resources: limits: cpu: \"200m\" memory: \"250Mi\" requests: cpu: \"100m\" memory: \"50Mi\" And apply kubectl apply -f vote-deploy.yaml Exercise: Define the value of cpu.request > cpu.limit Try to apply and observe. Define the values for memory.request and memory.limit higher than the total system memory. Apply and observe the deployment and pods.","title":"Defining Resource Requests and Limits"},{"location":"10_kubernetes_autoscaling/#create-a-hpa","text":"To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image file: vote-hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: vote spec: minReplicas: 4 maxReplicas: 15 targetCPUUtilizationPercentage: 40 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: vote apply kubectl apply -f vote-hpa.yaml Validate kubectl get hpa kubectl describe hpa vote kubectl get pod,deploy","title":"Create a HPA"},{"location":"10_kubernetes_autoscaling/#load-test","text":"file: loadtest-job.yaml apiVersion: batch/v1 kind: Job metadata: name: loadtest spec: template: spec: containers: - name: siege image: schoolofdevops/loadtest:v1 command: [\"siege\", \"--concurrent=5\", \"--benchmark\", \"--time=10m\", \"http://vote\"] restartPolicy: Never backoffLimit: 4 And launch the loadtest kubectl apply -f loadtest-job.yaml To monitor while the load test is running , watch kubectl top pods To get information about the job kubectl get jobs kubectl describe job loadtest To check the load test output kubectl logs -f loadtest-xxxx [replace loadtest-xxxx with the actual pod id.] [Sample Output] ** SIEGE 3.0.8 ** Preparing 15 concurrent users for battle. root@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f ** SIEGE 3.0.8 ** Preparing 15 concurrent users for battle. ..... Lifting the server siege... done. Transactions: 41618 hits Availability: 99.98 % Elapsed time: 299.13 secs Data transferred: 127.05 MB Response time: 0.11 secs Transaction rate: 139.13 trans/sec Throughput: 0.42 MB/sec Concurrency: 14.98 Successful transactions: 41618 Failed transactions: 8 Longest transaction: 3.70 Shortest transaction: 0.00 FILE: /var/log/siege.log You can disable this annoying message by editing the .siegerc file in your home directory; change the directive 'show-logfile' to false. Now check the job status again, kubectl get jobs NAME DESIRED SUCCESSFUL AGE vote-loadtest 1 1 10m","title":"Load Test"},{"location":"10_kubernetes_autoscaling/#reading-list","text":"Kubernetes Monitoring Architecture Core Metrics Pipeline Metrics Server Assigning Resources to Containers and Pods Horizontal Pod Autoscaler","title":"Reading List"},{"location":"11_deploying_sample_app/","text":"Mini Project: Deploying Multi Tier Application Stack In this project , you would write definitions for deploying the vote application stack with all components/tiers which include, vote ui redis worker db results ui Tasks Create deployments for all applications Define services for each tier applicable Launch/apply the definitions Following table depicts the state of readiness of the above services. App Deployment Service vote ready ready redis ready ready worker TODO n/a db ready ready results TODO TODO Specs: worker image: schoolofdevops/worker:latest results image: schoolofdevops/vote-result port: 80 service type: NodePort Deploying the sample application To create deploy the sample applications, kubectl create -f projects/instavote/dev Sample output is like: deployment \"db\" created service \"db\" created deployment \"redis\" created service \"redis\" created deployment \"vote\" created service \"vote\" created deployment \"worker\" created deployment \"results\" created service \"results\" created To Validate: kubectl get svc -n instavote Sample Output is: kubectl get service vote NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote 10.97.104.243 <pending> 80:31808/TCP 1h Here the port assigned is 31808, go to the browser and enter masterip:31808 This will load the page where you can vote. To check the result: kubectl get service result Sample Output is: kubectl get service result NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE result 10.101.112.16 <pending> 80:32511/TCP 1h Here the port assigned is 32511, go to the browser and enter masterip:32511 This is the page where you should see the results for the vote application stack.","title":"Mini Project"},{"location":"11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack","text":"In this project , you would write definitions for deploying the vote application stack with all components/tiers which include, vote ui redis worker db results ui","title":"Mini Project: Deploying Multi Tier Application Stack"},{"location":"11_deploying_sample_app/#tasks","text":"Create deployments for all applications Define services for each tier applicable Launch/apply the definitions Following table depicts the state of readiness of the above services. App Deployment Service vote ready ready redis ready ready worker TODO n/a db ready ready results TODO TODO Specs: worker image: schoolofdevops/worker:latest results image: schoolofdevops/vote-result port: 80 service type: NodePort","title":"Tasks"},{"location":"11_deploying_sample_app/#deploying-the-sample-application","text":"To create deploy the sample applications, kubectl create -f projects/instavote/dev Sample output is like: deployment \"db\" created service \"db\" created deployment \"redis\" created service \"redis\" created deployment \"vote\" created service \"vote\" created deployment \"worker\" created deployment \"results\" created service \"results\" created","title":"Deploying the sample application"},{"location":"11_deploying_sample_app/#to-validate","text":"kubectl get svc -n instavote Sample Output is: kubectl get service vote NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote 10.97.104.243 <pending> 80:31808/TCP 1h Here the port assigned is 31808, go to the browser and enter masterip:31808 This will load the page where you can vote. To check the result: kubectl get service result Sample Output is: kubectl get service result NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE result 10.101.112.16 <pending> 80:32511/TCP 1h Here the port assigned is 32511, go to the browser and enter masterip:32511 This is the page where you should see the results for the vote application stack.","title":"To Validate:"},{"location":"12_troubleshooting/","text":"Troubleshooting the Kubernetes cluster In this chapter we will learn about how to trouble shoot our Kubernetes cluster at control plane level and at application level . Troubleshooting the control plane Listing the nodes in a cluster First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster. kubectl get nodes Make sure that all nodes are in Ready state. List the control plane pods If your nodes are up and running, next thing to check is the status of Kubernetes components. Run, kubectl get pods -n kube-system If any of the pod is restarting or crashing , look in to the issue. This can be done by getting the pod's description. For example, in my cluster kube-dns is crashing. In order to fix this first check the deployment for errors. kubectl describe deployment -n kube-system kube-dns Log files Master If your deployment is good, the next thing to look for is log files. The locations of log files are given below... /var/log/kube-apiserver.log - For API Server logs /var/log/kube-scheduler.log - For Scheduler logs /var/log/kube-controller-manager.log - For Replication Controller logs If your Kubernetes components are running as pods, then you can get their logs by following the steps given below, Keep in mind that the actual pod's name may differ from cluster to cluster... kubectl logs -n kube-system -f kube-apiserver-node1 kubectl logs -n kube-system -f kube-scheduler-node1 kubectl logs -n kube-system -f kube-controller-manager-node1 Worker Nodes In your worker, you will need to check for errors in kubelet's log... sudo journalctl -u kubelet Troubleshooting the application Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot. Getting detailed status of an object (pods, deployments) object.status shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues. Example kubectl get pod vote -o yaml example output snippet when a wrong image was used to create a pod. status: ... containerStatuses: .... state: waiting: message: 'rpc error: code = Unknown desc = Error response from daemon: manifest for schoolofdevops/vote:latst not found' reason: ErrImagePull hostIP: 139.59.232.248 Checking the status of Deployment For this example I have a sample deployment called nginx. FILE: nginx-deployment.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: ngnix:latest ports: - containerPort: 80 List the deployment to check for the availability of pods kubectl get deployment nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 0 20h It is clear that my pod is unavailable. Lets dig further. Check the events of your deployment. kubectl describe deployment nginx List the pods to check for any registry related error kubectl get pods NAME READY STATUS RESTARTS AGE nginx-57c88d7bb8-c6kpc 0/1 ImagePullBackOff 0 7m As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further. kubectl describe pod nginx-57c88d7bb8-c6kpc Check the events of the pod's description. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m default-scheduler Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal Normal SuccessfulMountVolume 9m kubelet, ip-11-0-1-111.us-west-2.compute.internal MountVolume.SetUp succeeded for volume \"default-token-8cwn4\" Normal Pulling 8m (x4 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal pulling image \"ngnix\" Warning Failed 8m (x4 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Failed to pull image \"ngnix\": rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access Normal BackOff 7m (x6 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Back-off pulling image \"ngnix\" Warning FailedSync 4m (x24 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Error syncing pod Bingo! The name of the image is ngnix instead of nginx . So fix the typo in your deployment file and redo the deployment . Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod. kubectl logs -f nginx-57c88d7bb8-c6kpc If you have any errors it will get populated in your logs.","title":"Troubleshooting Tips"},{"location":"12_troubleshooting/#troubleshooting-the-kubernetes-cluster","text":"In this chapter we will learn about how to trouble shoot our Kubernetes cluster at control plane level and at application level .","title":"Troubleshooting the Kubernetes cluster"},{"location":"12_troubleshooting/#troubleshooting-the-control-plane","text":"","title":"Troubleshooting the control plane"},{"location":"12_troubleshooting/#listing-the-nodes-in-a-cluster","text":"First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster. kubectl get nodes Make sure that all nodes are in Ready state.","title":"Listing the nodes in a cluster"},{"location":"12_troubleshooting/#list-the-control-plane-pods","text":"If your nodes are up and running, next thing to check is the status of Kubernetes components. Run, kubectl get pods -n kube-system If any of the pod is restarting or crashing , look in to the issue. This can be done by getting the pod's description. For example, in my cluster kube-dns is crashing. In order to fix this first check the deployment for errors. kubectl describe deployment -n kube-system kube-dns","title":"List the control plane pods"},{"location":"12_troubleshooting/#log-files","text":"","title":"Log files"},{"location":"12_troubleshooting/#master","text":"If your deployment is good, the next thing to look for is log files. The locations of log files are given below... /var/log/kube-apiserver.log - For API Server logs /var/log/kube-scheduler.log - For Scheduler logs /var/log/kube-controller-manager.log - For Replication Controller logs If your Kubernetes components are running as pods, then you can get their logs by following the steps given below, Keep in mind that the actual pod's name may differ from cluster to cluster... kubectl logs -n kube-system -f kube-apiserver-node1 kubectl logs -n kube-system -f kube-scheduler-node1 kubectl logs -n kube-system -f kube-controller-manager-node1","title":"Master"},{"location":"12_troubleshooting/#worker-nodes","text":"In your worker, you will need to check for errors in kubelet's log... sudo journalctl -u kubelet","title":"Worker Nodes"},{"location":"12_troubleshooting/#troubleshooting-the-application","text":"Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.","title":"Troubleshooting the application"},{"location":"12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments","text":"object.status shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues. Example kubectl get pod vote -o yaml example output snippet when a wrong image was used to create a pod. status: ... containerStatuses: .... state: waiting: message: 'rpc error: code = Unknown desc = Error response from daemon: manifest for schoolofdevops/vote:latst not found' reason: ErrImagePull hostIP: 139.59.232.248","title":"Getting detailed status of an object (pods, deployments)"},{"location":"12_troubleshooting/#checking-the-status-of-deployment","text":"For this example I have a sample deployment called nginx. FILE: nginx-deployment.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: ngnix:latest ports: - containerPort: 80 List the deployment to check for the availability of pods kubectl get deployment nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 0 20h It is clear that my pod is unavailable. Lets dig further. Check the events of your deployment. kubectl describe deployment nginx List the pods to check for any registry related error kubectl get pods NAME READY STATUS RESTARTS AGE nginx-57c88d7bb8-c6kpc 0/1 ImagePullBackOff 0 7m As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further. kubectl describe pod nginx-57c88d7bb8-c6kpc Check the events of the pod's description. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m default-scheduler Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal Normal SuccessfulMountVolume 9m kubelet, ip-11-0-1-111.us-west-2.compute.internal MountVolume.SetUp succeeded for volume \"default-token-8cwn4\" Normal Pulling 8m (x4 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal pulling image \"ngnix\" Warning Failed 8m (x4 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Failed to pull image \"ngnix\": rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access Normal BackOff 7m (x6 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Back-off pulling image \"ngnix\" Warning FailedSync 4m (x24 over 9m) kubelet, ip-11-0-1-111.us-west-2.compute.internal Error syncing pod Bingo! The name of the image is ngnix instead of nginx . So fix the typo in your deployment file and redo the deployment . Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod. kubectl logs -f nginx-57c88d7bb8-c6kpc If you have any errors it will get populated in your logs.","title":"Checking the status of Deployment"},{"location":"13_redis_statefulset/","text":"Deploying Redis Cluster with StatefulSets What will you learn Statefulsets initContainers Redis Service We will use Redis as Statefulsets for our Vote application. It is similar to Deployment, but Statefulsets requires a Service Name . So we will create a headless service (service without endpoints) first. file: redis-svc.yml kind: Service metadata: name: redis labels: app: redis spec: type: ClusterIP ports: - name: redis port: 6379 targetPort: redis clusterIP: None selector: app: redis role: master apply kubectl apply -f redis-svc.yml Note: clusterIP's value is set to None . Redis ConfigMap Redis ConfigMap has two sections. * master.conf - for Redis master * slave.conf - for Redis slave file: redis-cm.yml apiVersion: v1 kind: ConfigMap metadata: name: redis data: master.conf: | bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \"\" slave.conf: | slaveof redis-0.redis 6379 apply kubectl apply -f redis-svc.yml Redis initContainers We have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has 0 as Ordinal number , then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration. file: redis-sts.yml [...] initContainers: - name: init-redis image: redis:4.0.9 command: - bash - \"-c\" - | set -ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/master.conf /etc/redis.conf else cp /mnt/config-map/slave.conf /etc/redis.conf fi volumeMounts: - name: conf mountPath: /etc subPath: redis.conf - name: config-map mountPath: /mnt/config-map Redis Statefulsets These redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume, conf , from the initContainers which has the proper Redis configuration. file: redis-sts.yaml [...] containers: - name: redis image: redis:4.0.9 command: [\"redis-server\"] args: [\"/etc/redis.conf\"] env: - name: ALLOW_EMPTY_PASSWORD value: \"yes\" ports: - name: redis containerPort: 6379 volumeMounts: - name: redis-data mountPath: /data - name: conf mountPath: /etc/ subPath: redis.conf To apply kubectl apply -f redis-sts.yml Reading List Redis Replication Run Replicated Statefulsets Applications Init Containers Search Keywords init containers kubernetes statefulsets redis replication","title":"Creating Replicated Redis Cluster with Statefulsets"},{"location":"13_redis_statefulset/#deploying-redis-cluster-with-statefulsets","text":"What will you learn Statefulsets initContainers","title":"Deploying Redis Cluster with StatefulSets"},{"location":"13_redis_statefulset/#redis-service","text":"We will use Redis as Statefulsets for our Vote application. It is similar to Deployment, but Statefulsets requires a Service Name . So we will create a headless service (service without endpoints) first. file: redis-svc.yml kind: Service metadata: name: redis labels: app: redis spec: type: ClusterIP ports: - name: redis port: 6379 targetPort: redis clusterIP: None selector: app: redis role: master apply kubectl apply -f redis-svc.yml Note: clusterIP's value is set to None .","title":"Redis Service"},{"location":"13_redis_statefulset/#redis-configmap","text":"Redis ConfigMap has two sections. * master.conf - for Redis master * slave.conf - for Redis slave file: redis-cm.yml apiVersion: v1 kind: ConfigMap metadata: name: redis data: master.conf: | bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \"\" slave.conf: | slaveof redis-0.redis 6379 apply kubectl apply -f redis-svc.yml","title":"Redis ConfigMap"},{"location":"13_redis_statefulset/#redis-initcontainers","text":"We have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has 0 as Ordinal number , then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration. file: redis-sts.yml [...] initContainers: - name: init-redis image: redis:4.0.9 command: - bash - \"-c\" - | set -ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/master.conf /etc/redis.conf else cp /mnt/config-map/slave.conf /etc/redis.conf fi volumeMounts: - name: conf mountPath: /etc subPath: redis.conf - name: config-map mountPath: /mnt/config-map","title":"Redis initContainers"},{"location":"13_redis_statefulset/#redis-statefulsets","text":"These redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume, conf , from the initContainers which has the proper Redis configuration. file: redis-sts.yaml [...] containers: - name: redis image: redis:4.0.9 command: [\"redis-server\"] args: [\"/etc/redis.conf\"] env: - name: ALLOW_EMPTY_PASSWORD value: \"yes\" ports: - name: redis containerPort: 6379 volumeMounts: - name: redis-data mountPath: /data - name: conf mountPath: /etc/ subPath: redis.conf To apply kubectl apply -f redis-sts.yml","title":"Redis Statefulsets"},{"location":"13_redis_statefulset/#reading-list","text":"Redis Replication Run Replicated Statefulsets Applications Init Containers Search Keywords init containers kubernetes statefulsets redis replication","title":"Reading List"},{"location":"2_kube_cluster_vagrant/","text":"Install VirtualBox and Vagrant TOOL VERSION LINK VirtualBox 5.1.26 https://www.virtualbox.org/wiki/Downloads Vagrant 1.9.7 https://www.vagrantup.com/downloads.html Importing a VM Template If you have already copied/downloaded the box file ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section. vagrant box list vagrant box add ubuntu/xenial64 ubuntu-xenial64.box vagrant box list Provisioning Vagrant Nodes Clone repo if not already git clone https://github.com/schoolofdevops/lab-setup.git Launch environments with Vagrant cd lab-setup/kubernetes/vagrant-kube-cluster vagrant up Login to nodes Open three different terminals to login to 3 nodes created with above command Terminal 1 vagrant ssh kube-01 sudo su Terminal 2 vagrant ssh kube-02 sudo su Terminal 3 vagrant ssh kube-03 sudo su Once the environment is setup, follow Initialization of Master onwards from the following tutorial https://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md","title":"Provisioning VMs with Vagrant"},{"location":"2_kube_cluster_vagrant/#install-virtualbox-and-vagrant","text":"TOOL VERSION LINK VirtualBox 5.1.26 https://www.virtualbox.org/wiki/Downloads Vagrant 1.9.7 https://www.vagrantup.com/downloads.html","title":"Install VirtualBox and Vagrant"},{"location":"2_kube_cluster_vagrant/#importing-a-vm-template","text":"If you have already copied/downloaded the box file ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section. vagrant box list vagrant box add ubuntu/xenial64 ubuntu-xenial64.box vagrant box list","title":"Importing a VM Template"},{"location":"2_kube_cluster_vagrant/#provisioning-vagrant-nodes","text":"Clone repo if not already git clone https://github.com/schoolofdevops/lab-setup.git Launch environments with Vagrant cd lab-setup/kubernetes/vagrant-kube-cluster vagrant up Login to nodes Open three different terminals to login to 3 nodes created with above command Terminal 1 vagrant ssh kube-01 sudo su Terminal 2 vagrant ssh kube-02 sudo su Terminal 3 vagrant ssh kube-03 sudo su Once the environment is setup, follow Initialization of Master onwards from the following tutorial https://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md","title":"Provisioning Vagrant Nodes"},{"location":"3_install_kubernetes/","text":"Kubeadm : Bring Your Own Nodes (BYON) This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses kubeadm to install and configure kubernetes cluster. Compatibility Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. The below steps are applicable for the below mentioned OS OS Version Codename Ubuntu 16.04 Xenial Base Setup (Skip if using vagrant) Skip this step and scroll to Initializing Master if you have setup nodes with vagrant On all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also download and run this script Create Kubernetes Repository We need to create a repository to download Kubernetes. curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat <<EOF > /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF Installation of the packages We should update the machines before installing so that we can update the repository. apt-get update -y Installing all the packages with dependencies: apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni rm -rf /var/lib/kubelet/* Setup sysctl configs In order for many container networks to work, the following needs to be enabled on each node. sysctl net.bridge.bridge-nf-call-iptables=1 The above steps has to be followed in all the nodes. Initializing Master This tutorial assumes kube-01 as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly. To initialize master, run this on kube-01 kubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16 Initialization of the Nodes (Previously Minions) After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster. e.g. kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0 Copy and paste it on all node. Troubleshooting Tips If you lose the join token, you could retrieve it using kubeadm token list On successfully joining the master, you should see output similar to following, root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0 [kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters. [preflight] Running pre-flight checks [discovery] Trying to connect to API Server \"159.203.170.84:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://159.203.170.84:6443\" [discovery] Requesting info from \"https://159.203.170.84:6443\" again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"159.203.170.84:6443\" [discovery] Successfully established connection with API Server \"159.203.170.84:6443\" [bootstrap] Detected server version: v1.8.2 [bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1) Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. Setup the admin client - Kubectl On Master Node mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Installing CNI with Weave Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster. There are various overlay networking drivers available for kubernetes. We are going to use Weave Net . export kubever=$(kubectl version | base64 | tr -d '\\n') kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$kubever\" Validating the Setup You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands. To check if nodes are ready kubectl get nodes kubectl get cs [ Expected output ] root@kube-01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-01 Ready master 9m v1.8.2 kube-02 Ready <none> 4m v1.8.2 kube-03 Ready <none> 4m v1.8.2 Additional Status Commands kubectl version kubectl cluster-info kubectl get pods -n kube-system kubectl get events It will take a few minutes to have the cluster up and running with all the services. Possible Issues Nodes are node in Ready status kube-dns is crashing constantly Some of the systems services are not up Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques, Troubleshooting Tips Check events kubectl get events Check Logs kubectl get pods -n kube-system [get the name of the pod which has a problem] kubectl logs <pod> -n kube-system e.g. root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system Error from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of: [kubedns dnsmasq sidecar] root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 kubedns -n kube-system I1106 14:41:15.542409 1 dns.go:48] version: 1.14.4-2-g5584e04 I1106 14:41:15.543487 1 server.go:70] Using .... Enable Kubernetes Dashboard After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard. Installing Dashboard: kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml This will create a pod for the Kubernetes Dashboard. To access the Dashboard in th browser, run the below command kubectl describe svc kubernetes-dashboard -n kube-system Sample output: kubectl describe svc kubernetes-dashboard -n kube-system Name: kubernetes-dashboard Namespace: kube-system Labels: app=kubernetes-dashboard Selector: app=kubernetes-dashboard Type: NodePort IP: 10.98.148.82 Port: <unset> 80/TCP NodePort: <unset> 31000/TCP Endpoints: 10.40.0.1:9090 Session Affinity: None Now check for the node port, here it is 31000, and go to the browser, and access the dashboard with the following URL do not use the IP above, use master node IP instead http://NODEIP:31000 The Dashboard Looks like: Check out the supporting code Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow. git clone https://github.com/schoolofdevops/k8s-code.git","title":"Setup Kubernetes Cluster"},{"location":"3_install_kubernetes/#kubeadm-bring-your-own-nodes-byon","text":"This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses kubeadm to install and configure kubernetes cluster.","title":"Kubeadm : Bring Your Own Nodes (BYON)"},{"location":"3_install_kubernetes/#compatibility","text":"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. The below steps are applicable for the below mentioned OS OS Version Codename Ubuntu 16.04 Xenial","title":"Compatibility"},{"location":"3_install_kubernetes/#base-setup-skip-if-using-vagrant","text":"Skip this step and scroll to Initializing Master if you have setup nodes with vagrant On all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also download and run this script","title":"Base Setup (Skip if using vagrant)"},{"location":"3_install_kubernetes/#create-kubernetes-repository","text":"We need to create a repository to download Kubernetes. curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat <<EOF > /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF","title":"Create Kubernetes Repository"},{"location":"3_install_kubernetes/#installation-of-the-packages","text":"We should update the machines before installing so that we can update the repository. apt-get update -y Installing all the packages with dependencies: apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni rm -rf /var/lib/kubelet/*","title":"Installation of the packages"},{"location":"3_install_kubernetes/#setup-sysctl-configs","text":"In order for many container networks to work, the following needs to be enabled on each node. sysctl net.bridge.bridge-nf-call-iptables=1 The above steps has to be followed in all the nodes.","title":"Setup sysctl configs"},{"location":"3_install_kubernetes/#initializing-master","text":"This tutorial assumes kube-01 as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly. To initialize master, run this on kube-01 kubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16","title":"Initializing Master"},{"location":"3_install_kubernetes/#initialization-of-the-nodes-previously-minions","text":"After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster. e.g. kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0 Copy and paste it on all node.","title":"Initialization of the Nodes (Previously Minions)"},{"location":"3_install_kubernetes/#troubleshooting-tips","text":"If you lose the join token, you could retrieve it using kubeadm token list On successfully joining the master, you should see output similar to following, root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0 [kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters. [preflight] Running pre-flight checks [discovery] Trying to connect to API Server \"159.203.170.84:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://159.203.170.84:6443\" [discovery] Requesting info from \"https://159.203.170.84:6443\" again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"159.203.170.84:6443\" [discovery] Successfully established connection with API Server \"159.203.170.84:6443\" [bootstrap] Detected server version: v1.8.2 [bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1) Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join.","title":"Troubleshooting Tips"},{"location":"3_install_kubernetes/#setup-the-admin-client-kubectl","text":"On Master Node mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Setup the admin client - Kubectl"},{"location":"3_install_kubernetes/#installing-cni-with-weave","text":"Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster. There are various overlay networking drivers available for kubernetes. We are going to use Weave Net . export kubever=$(kubectl version | base64 | tr -d '\\n') kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$kubever\"","title":"Installing CNI with Weave"},{"location":"3_install_kubernetes/#validating-the-setup","text":"You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands. To check if nodes are ready kubectl get nodes kubectl get cs [ Expected output ] root@kube-01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-01 Ready master 9m v1.8.2 kube-02 Ready <none> 4m v1.8.2 kube-03 Ready <none> 4m v1.8.2 Additional Status Commands kubectl version kubectl cluster-info kubectl get pods -n kube-system kubectl get events It will take a few minutes to have the cluster up and running with all the services.","title":"Validating the Setup"},{"location":"3_install_kubernetes/#possible-issues","text":"Nodes are node in Ready status kube-dns is crashing constantly Some of the systems services are not up Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques, Troubleshooting Tips Check events kubectl get events Check Logs kubectl get pods -n kube-system [get the name of the pod which has a problem] kubectl logs <pod> -n kube-system e.g. root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system Error from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of: [kubedns dnsmasq sidecar] root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 kubedns -n kube-system I1106 14:41:15.542409 1 dns.go:48] version: 1.14.4-2-g5584e04 I1106 14:41:15.543487 1 server.go:70] Using ....","title":"Possible Issues"},{"location":"3_install_kubernetes/#enable-kubernetes-dashboard","text":"After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard. Installing Dashboard: kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml This will create a pod for the Kubernetes Dashboard. To access the Dashboard in th browser, run the below command kubectl describe svc kubernetes-dashboard -n kube-system Sample output: kubectl describe svc kubernetes-dashboard -n kube-system Name: kubernetes-dashboard Namespace: kube-system Labels: app=kubernetes-dashboard Selector: app=kubernetes-dashboard Type: NodePort IP: 10.98.148.82 Port: <unset> 80/TCP NodePort: <unset> 31000/TCP Endpoints: 10.40.0.1:9090 Session Affinity: None Now check for the node port, here it is 31000, and go to the browser, and access the dashboard with the following URL do not use the IP above, use master node IP instead http://NODEIP:31000 The Dashboard Looks like:","title":"Enable Kubernetes Dashboard"},{"location":"3_install_kubernetes/#check-out-the-supporting-code","text":"Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow. git clone https://github.com/schoolofdevops/k8s-code.git","title":"Check out the supporting code"},{"location":"5-vote-deploying_pods/","text":"Deploying Pods Life of a pod Pending : in progress Running Succeeded : successfully exited Failed Unknown Resource Configs Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON. Here is the syntax to create a YAML specification. AKMS => Resource Configs Specs apiVersion: v1 kind: metadata: spec: Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/ To list supported version of apis kubectl api-versions Writing Pod Spec Lets now create the Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows. Filename: k8s-code/pods/vote-pod.yaml apiVersion: kind: Pod metadata: spec: Lets edit this and add the pod specs Filename: k8s-code/pods/vote-pod.yaml apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 Use this link to refer to pod spec Launching and operating a Pod To launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured. watch -n 1 kubectl get pods,deploy,rs,svc kubectl Syntax: kubectl kubectl apply --help kubectl apply -f FILE To Launch pod using configs above, kubectl apply -f vote-pod.yaml To view pods kubectl get pods kubectl get po -o wide kubectl get pods vote To get detailed info kubectl describe pods vote [Output:] Name: vote Namespace: default Node: kube-3/192.168.0.80 Start Time: Tue, 07 Feb 2017 16:16:40 +0000 Labels: app=voting Status: Running IP: 10.40.0.2 Controllers: <none> Containers: vote: Container ID: docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d Image: schoolofdevops/vote:latest Image ID: docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731 Port: 80/TCP State: Running Started: Tue, 07 Feb 2017 16:16:52 +0000 Ready: True Restart Count: 0 Volume Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro) Environment Variables: <none> Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-2n6j1: Type: Secret (a volume populated by a Secret) SecretName: default-token-2n6j1 QoS Class: BestEffort Tolerations: <none> Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 21s 21s 1 {default-scheduler } Normal Scheduled Successfully assigned vote to kube-3 20s 20s 1 {kubelet kube-3} spec.containers{vote} Normal Pulling pulling image \"schoolofdevops/vote:latest\" 10s 10s 1 {kubelet kube-3} spec.containers{vote} Normal Pulled Successfully pulled image \"schoolofdevops/vote:latest\" 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Created Created container with docker id 48304b35b945; Security:[seccomp=unconfined] 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Started Started container with docker id 48304b35b945 Commands to operate the pod kubectl logs vote kubectl exec -it vote sh Inside the container in a pod ifconfig cat /etc/issue hostname cat /proc/cpuinfo ps aux Attach a Volume to the Pod Lets create a pod for database and attach a volume to it. To achieve this we will need to create a volumes definition attach volume to container using VolumeMounts property Local host volumes are of two types: * emptyDir * hostPath We will pick hostPath. Refer to this doc to read more about hostPath. File: db-pod.yaml apiVersion: v1 kind: Pod metadata: name: db labels: app: postgres role: database tier: back spec: containers: - name: db image: postgres:9.4 ports: - containerPort: 5432 volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data volumes: - name: db-data hostPath: path: /var/lib/pgdata type: DirectoryOrCreate To create this pod, kubectl apply -f db-pod.yaml kubectl describe pod db kubectl get events Exercise : Examine /var/lib/pgdata on the systems to check if the directory is been created and if the data is present. Creating Multi Container Pods file: multi_container_pod.yml apiVersion: v1 kind: Pod metadata: name: web labels: tier: front app: nginx role: ui spec: containers: - name: nginx image: nginx:stable-alpine ports: - containerPort: 80 protocol: TCP volumeMounts: - name: data mountPath: /var/www/html-sample-app - name: sync image: schoolofdevops/sync:v2 volumeMounts: - name: data mountPath: /var/www/app volumes: - name: data emptyDir: {} To create this pod kubectl apply -f multi_container_pod.yml Check Status root@kube-01:~# kubectl get pods NAME READY STATUS RESTARTS AGE nginx 0/2 ContainerCreating 0 7s vote 1/1 Running 0 3m Checking logs, logging in kubectl logs web -c sync kubectl logs web -c nginx kubectl exec -it web sh -c nginx kubectl exec -it web sh -c sync Observe whats common and whats isolated in two containers running inside the same pod using the following commands, shared hostname ifconfig isolated cat /etc/issue ps aux df -h Exercise Create a pod definition for redis and deploy. Reading List PodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core Managing Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/ Node Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/","title":"Launching Pods"},{"location":"5-vote-deploying_pods/#deploying-pods","text":"Life of a pod Pending : in progress Running Succeeded : successfully exited Failed Unknown","title":"Deploying Pods"},{"location":"5-vote-deploying_pods/#resource-configs","text":"Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON. Here is the syntax to create a YAML specification. AKMS => Resource Configs Specs apiVersion: v1 kind: metadata: spec: Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/ To list supported version of apis kubectl api-versions","title":"Resource Configs"},{"location":"5-vote-deploying_pods/#writing-pod-spec","text":"Lets now create the Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows. Filename: k8s-code/pods/vote-pod.yaml apiVersion: kind: Pod metadata: spec: Lets edit this and add the pod specs Filename: k8s-code/pods/vote-pod.yaml apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 Use this link to refer to pod spec","title":"Writing Pod Spec"},{"location":"5-vote-deploying_pods/#launching-and-operating-a-pod","text":"To launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured. watch -n 1 kubectl get pods,deploy,rs,svc kubectl Syntax: kubectl kubectl apply --help kubectl apply -f FILE To Launch pod using configs above, kubectl apply -f vote-pod.yaml To view pods kubectl get pods kubectl get po -o wide kubectl get pods vote To get detailed info kubectl describe pods vote [Output:] Name: vote Namespace: default Node: kube-3/192.168.0.80 Start Time: Tue, 07 Feb 2017 16:16:40 +0000 Labels: app=voting Status: Running IP: 10.40.0.2 Controllers: <none> Containers: vote: Container ID: docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d Image: schoolofdevops/vote:latest Image ID: docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731 Port: 80/TCP State: Running Started: Tue, 07 Feb 2017 16:16:52 +0000 Ready: True Restart Count: 0 Volume Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro) Environment Variables: <none> Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-2n6j1: Type: Secret (a volume populated by a Secret) SecretName: default-token-2n6j1 QoS Class: BestEffort Tolerations: <none> Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 21s 21s 1 {default-scheduler } Normal Scheduled Successfully assigned vote to kube-3 20s 20s 1 {kubelet kube-3} spec.containers{vote} Normal Pulling pulling image \"schoolofdevops/vote:latest\" 10s 10s 1 {kubelet kube-3} spec.containers{vote} Normal Pulled Successfully pulled image \"schoolofdevops/vote:latest\" 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Created Created container with docker id 48304b35b945; Security:[seccomp=unconfined] 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Started Started container with docker id 48304b35b945 Commands to operate the pod kubectl logs vote kubectl exec -it vote sh Inside the container in a pod ifconfig cat /etc/issue hostname cat /proc/cpuinfo ps aux","title":"Launching and operating a Pod"},{"location":"5-vote-deploying_pods/#attach-a-volume-to-the-pod","text":"Lets create a pod for database and attach a volume to it. To achieve this we will need to create a volumes definition attach volume to container using VolumeMounts property Local host volumes are of two types: * emptyDir * hostPath We will pick hostPath. Refer to this doc to read more about hostPath. File: db-pod.yaml apiVersion: v1 kind: Pod metadata: name: db labels: app: postgres role: database tier: back spec: containers: - name: db image: postgres:9.4 ports: - containerPort: 5432 volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data volumes: - name: db-data hostPath: path: /var/lib/pgdata type: DirectoryOrCreate To create this pod, kubectl apply -f db-pod.yaml kubectl describe pod db kubectl get events Exercise : Examine /var/lib/pgdata on the systems to check if the directory is been created and if the data is present.","title":"Attach a Volume to the Pod"},{"location":"5-vote-deploying_pods/#creating-multi-container-pods","text":"file: multi_container_pod.yml apiVersion: v1 kind: Pod metadata: name: web labels: tier: front app: nginx role: ui spec: containers: - name: nginx image: nginx:stable-alpine ports: - containerPort: 80 protocol: TCP volumeMounts: - name: data mountPath: /var/www/html-sample-app - name: sync image: schoolofdevops/sync:v2 volumeMounts: - name: data mountPath: /var/www/app volumes: - name: data emptyDir: {} To create this pod kubectl apply -f multi_container_pod.yml Check Status root@kube-01:~# kubectl get pods NAME READY STATUS RESTARTS AGE nginx 0/2 ContainerCreating 0 7s vote 1/1 Running 0 3m Checking logs, logging in kubectl logs web -c sync kubectl logs web -c nginx kubectl exec -it web sh -c nginx kubectl exec -it web sh -c sync Observe whats common and whats isolated in two containers running inside the same pod using the following commands, shared hostname ifconfig isolated cat /etc/issue ps aux df -h","title":"Creating Multi Container Pods"},{"location":"5-vote-deploying_pods/#exercise","text":"Create a pod definition for redis and deploy.","title":"Exercise"},{"location":"5-vote-deploying_pods/#reading-list","text":"PodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core Managing Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/ Node Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/","title":"Reading List"},{"location":"6-vote-kubernetes_deployment/","text":"Creating a Deployment A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate. Deployment has mainly two responsibilities, Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count. Update Strategy: Define a release strategy and update the pods accordingly. /k8s-code/projects/instavote/dev/ cp vote-rs.yaml vote-deploy.yaml Deployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows, File: vote-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 8 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v2 spec: containers: - name: app image: schoolofdevops/vote:v2 ports: - containerPort: 80 protocol: TCP This time, start monitoring with --show-labels options added. watch -n 1 kubectl get pod,deploy,rs,svc --show-labels Lets create the Deployment. Do monitor the labels of the pod while applying this. kubectl apply -f vote-deploy.yaml Observe the chances to pod labels, specifically the pod-template-hash . Now that the deployment is created. To validate, kubectl get deployment kubectl get rs --show-labels kubectl get deploy,pods,rs kubectl rollout status deployment/vote kubectl get pods --show-labels Sample Output kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE vote 3 3 3 1 3m Scaling a deployment To scale a deployment in Kubernetes: kubectl scale deployment/vote --replicas=12 kubectl rollout status deployment/vote Sample output: Waiting for rollout to finish: 5 of 12 updated replicas are available... Waiting for rollout to finish: 6 of 12 updated replicas are available... deployment \"vote\" successfully rolled out You could also update the deployment by editing it. kubectl edit deploy/vote [change replicas to 15 from the editor, save and observe] Rolling Updates in Action Now, update the deployment spec to apply file: vote-deploy.yaml spec: ... replicas: 15 ... labels: app: python role: vote version: v3 ... template: spec: containers: - name: app image: schoolofdevops/vote:v3 apply kubectl apply -f vote-deploy.yaml kubectl rollout status deployment/vote Observe rollout status and monitoring screen. kubectl rollout history deploy/vote kubectl rollout history deploy/vote --revision=1 Undo and Rollback file: vote-deploy.yaml spec: containers: - name: app image: schoolofdevops/vote:rgjerdf apply kubectl apply -f vote-deploy.yaml kubectl rollout status kubectl rollout history deploy/vote kubectl rollout history deploy/vote --revision=xx where replace xxx with revisions Find out the previous revision with sane configs. To undo to a sane version (for example revision 3) kubectl rollout undo deploy/vote --to-revision=3","title":"Defining Update Strategy"},{"location":"6-vote-kubernetes_deployment/#creating-a-deployment","text":"A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate. Deployment has mainly two responsibilities, Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count. Update Strategy: Define a release strategy and update the pods accordingly. /k8s-code/projects/instavote/dev/ cp vote-rs.yaml vote-deploy.yaml Deployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows, File: vote-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 8 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v2 spec: containers: - name: app image: schoolofdevops/vote:v2 ports: - containerPort: 80 protocol: TCP This time, start monitoring with --show-labels options added. watch -n 1 kubectl get pod,deploy,rs,svc --show-labels Lets create the Deployment. Do monitor the labels of the pod while applying this. kubectl apply -f vote-deploy.yaml Observe the chances to pod labels, specifically the pod-template-hash . Now that the deployment is created. To validate, kubectl get deployment kubectl get rs --show-labels kubectl get deploy,pods,rs kubectl rollout status deployment/vote kubectl get pods --show-labels Sample Output kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE vote 3 3 3 1 3m","title":"Creating a Deployment"},{"location":"6-vote-kubernetes_deployment/#scaling-a-deployment","text":"To scale a deployment in Kubernetes: kubectl scale deployment/vote --replicas=12 kubectl rollout status deployment/vote Sample output: Waiting for rollout to finish: 5 of 12 updated replicas are available... Waiting for rollout to finish: 6 of 12 updated replicas are available... deployment \"vote\" successfully rolled out You could also update the deployment by editing it. kubectl edit deploy/vote [change replicas to 15 from the editor, save and observe]","title":"Scaling a deployment"},{"location":"6-vote-kubernetes_deployment/#rolling-updates-in-action","text":"Now, update the deployment spec to apply file: vote-deploy.yaml spec: ... replicas: 15 ... labels: app: python role: vote version: v3 ... template: spec: containers: - name: app image: schoolofdevops/vote:v3 apply kubectl apply -f vote-deploy.yaml kubectl rollout status deployment/vote Observe rollout status and monitoring screen. kubectl rollout history deploy/vote kubectl rollout history deploy/vote --revision=1","title":"Rolling Updates in Action"},{"location":"6-vote-kubernetes_deployment/#undo-and-rollback","text":"file: vote-deploy.yaml spec: containers: - name: app image: schoolofdevops/vote:rgjerdf apply kubectl apply -f vote-deploy.yaml kubectl rollout status kubectl rollout history deploy/vote kubectl rollout history deploy/vote --revision=xx where replace xxx with revisions Find out the previous revision with sane configs. To undo to a sane version (for example revision 3) kubectl rollout undo deploy/vote --to-revision=3","title":"Undo and Rollback"},{"location":"7-vote-exposing_app_with_service/","text":"Exposing Application with a Service Types of Services: ClusterIP NodePort LoadBalancer ExternalName kubectl get pods kubectl get svc Sample Output: NAME READY STATUS RESTARTS AGE voting-appp-1j52x 1/1 Running 0 12m voting-appp-pr2xz 1/1 Running 0 9m voting-appp-qpxbm 1/1 Running 0 15m Setting up monitoring If you are not running a monitoring screen, start it in a new terminal with the following command. watch -n 1 kubectl get pod,deploy,rs,svc Writing Service Spec Lets start writing the meta information for service. Filename: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: And then add the spec to it. Refer to Service (v1 core) api at this page https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/ --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort Save the file. Now to create a service: kubectl apply -f vote-svc.yaml --dry-run kubectl apply -f vote-svc.yaml kubectl get svc Now to check which port the pod is connected kubectl describe service vote Check for the Nodeport here Sample Output Name: vote Namespace: instavote Labels: role=svc tier=front Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"svc\",\"tier\":\"front\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{... Selector: app=vote Type: NodePort IP: 10.108.108.157 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31429/TCP Endpoints: 10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more... Session Affinity: None External Traffic Policy: Cluster Events: <none> Go to browser and check hostip:NodePort Here the node port is 31429. Sample output will be: Exposing the app with ExternalIP spec: selector: role: vote ports: - port: 80 protocol: TCP targetPort: 80 type: NodePort externalIPs: - xx.xx.xx.xx - yy.yy.yy.yy Where replace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts. apply kubectl get svc kubectl apply -f vote-svc.yaml kubectl get svc kubectl describe svc vote [sample output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote NodePort 10.107.71.204 206.189.150.190,159.65.8.227 80:30000/TCP 11m where, EXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http:// : to access this application. e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on. Internal Service Discovery Visit the vote app from browser Attemp to vote by clicking on one of the options observe what happens. Does it go through? Debugging, kubectl get pod kubectl exec vote-xxxx ping redis [replace xxxx with the actual pod id of one of the vote pods ] keep the above command on a watch. You should create a new terminal to run the watch command. e.g. watch kubectl exec vote-kvc7j ping redis where, vote-kvc7j is one of the vote pods that I am running. Replace this with the actual pod id. Now create redis service kubectl apply -f redis-svc.yaml kubectl get svc kubectl describe svc redis Watch the ping and observe if its able to resolve redis by hostname and its pointing to an IP address. e.g. PING redis (10.102.77.6): 56 data bytes where 10.102.77.6 is the ClusterIP assigned to the service. What happened here? Service redis was created with a ClusterIP e.g. 10.102.77.6 A DNS entry was created for this service. The fqdn of the service is redis.instavote.svc.cluster.local and it takes the form of my-svc.my-namespace.svc.cluster.local Each pod points to internal DNS server running in the cluster. You could see the details of this by running the following commands kubectl exec vote-xxxx cat /etc/resolv.conf [replace vote-xxxx with actual pod id] [sample output] nameserver 10.96.0.10 search instavote.svc.cluster.local svc.cluster.local cluster.local options ndots:5 where 10.96.0.10 is the ClusterIP assigned to the DNS service. You could co relate that with, kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 1h kubernetes-dashboard NodePort 10.104.42.73 <none> 80:31000/TCP 23m where, 10.96.0.10 is the ClusterIP assigned to kube-dns and matches the configuration in /etc/resolv.conf above. Creating Endpoints for Redis Service is been created, but you still need to launch the actual pods running redis application. Create the endpoints now, kubectl apply -f redis-deploy.yaml kubectl describe svc redis [sample output] Name: redis Namespace: instavote Labels: role=redis tier=back Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"redis\",\"tier\":\"back\"},\"name\":\"redis\",\"namespace\":\"instavote\"},\"spec\"... Selector: app=redis Type: ClusterIP IP: 10.102.77.6 Port: <unset> 6379/TCP TargetPort: 6379/TCP Endpoints: 10.32.0.6:6379,10.46.0.6:6379 Session Affinity: None Events: <none> Again, visit the vote app from browser, attempt to register your vote and observe what happens now. Reading Debugging * Services * Kubernetes Services Documentation * Service API Specs for Kubernetes Version 1.10","title":"Publishing appliaction and Service Discovery"},{"location":"7-vote-exposing_app_with_service/#exposing-application-with-a-service","text":"Types of Services: ClusterIP NodePort LoadBalancer ExternalName kubectl get pods kubectl get svc Sample Output: NAME READY STATUS RESTARTS AGE voting-appp-1j52x 1/1 Running 0 12m voting-appp-pr2xz 1/1 Running 0 9m voting-appp-qpxbm 1/1 Running 0 15m","title":"Exposing Application with  a Service"},{"location":"7-vote-exposing_app_with_service/#setting-up-monitoring","text":"If you are not running a monitoring screen, start it in a new terminal with the following command. watch -n 1 kubectl get pod,deploy,rs,svc","title":"Setting up monitoring"},{"location":"7-vote-exposing_app_with_service/#writing-service-spec","text":"Lets start writing the meta information for service. Filename: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: And then add the spec to it. Refer to Service (v1 core) api at this page https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/ --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort Save the file. Now to create a service: kubectl apply -f vote-svc.yaml --dry-run kubectl apply -f vote-svc.yaml kubectl get svc Now to check which port the pod is connected kubectl describe service vote Check for the Nodeport here Sample Output Name: vote Namespace: instavote Labels: role=svc tier=front Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"svc\",\"tier\":\"front\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{... Selector: app=vote Type: NodePort IP: 10.108.108.157 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 31429/TCP Endpoints: 10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more... Session Affinity: None External Traffic Policy: Cluster Events: <none> Go to browser and check hostip:NodePort Here the node port is 31429. Sample output will be:","title":"Writing Service Spec"},{"location":"7-vote-exposing_app_with_service/#exposing-the-app-with-externalip","text":"spec: selector: role: vote ports: - port: 80 protocol: TCP targetPort: 80 type: NodePort externalIPs: - xx.xx.xx.xx - yy.yy.yy.yy Where replace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts. apply kubectl get svc kubectl apply -f vote-svc.yaml kubectl get svc kubectl describe svc vote [sample output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote NodePort 10.107.71.204 206.189.150.190,159.65.8.227 80:30000/TCP 11m where, EXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http:// : to access this application. e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on.","title":"Exposing the app with ExternalIP"},{"location":"7-vote-exposing_app_with_service/#internal-service-discovery","text":"Visit the vote app from browser Attemp to vote by clicking on one of the options observe what happens. Does it go through? Debugging, kubectl get pod kubectl exec vote-xxxx ping redis [replace xxxx with the actual pod id of one of the vote pods ] keep the above command on a watch. You should create a new terminal to run the watch command. e.g. watch kubectl exec vote-kvc7j ping redis where, vote-kvc7j is one of the vote pods that I am running. Replace this with the actual pod id. Now create redis service kubectl apply -f redis-svc.yaml kubectl get svc kubectl describe svc redis Watch the ping and observe if its able to resolve redis by hostname and its pointing to an IP address. e.g. PING redis (10.102.77.6): 56 data bytes where 10.102.77.6 is the ClusterIP assigned to the service. What happened here? Service redis was created with a ClusterIP e.g. 10.102.77.6 A DNS entry was created for this service. The fqdn of the service is redis.instavote.svc.cluster.local and it takes the form of my-svc.my-namespace.svc.cluster.local Each pod points to internal DNS server running in the cluster. You could see the details of this by running the following commands kubectl exec vote-xxxx cat /etc/resolv.conf [replace vote-xxxx with actual pod id] [sample output] nameserver 10.96.0.10 search instavote.svc.cluster.local svc.cluster.local cluster.local options ndots:5 where 10.96.0.10 is the ClusterIP assigned to the DNS service. You could co relate that with, kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP 1h kubernetes-dashboard NodePort 10.104.42.73 <none> 80:31000/TCP 23m where, 10.96.0.10 is the ClusterIP assigned to kube-dns and matches the configuration in /etc/resolv.conf above.","title":"Internal Service Discovery"},{"location":"7-vote-exposing_app_with_service/#creating-endpoints-for-redis","text":"Service is been created, but you still need to launch the actual pods running redis application. Create the endpoints now, kubectl apply -f redis-deploy.yaml kubectl describe svc redis [sample output] Name: redis Namespace: instavote Labels: role=redis tier=back Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"redis\",\"tier\":\"back\"},\"name\":\"redis\",\"namespace\":\"instavote\"},\"spec\"... Selector: app=redis Type: ClusterIP IP: 10.102.77.6 Port: <unset> 6379/TCP TargetPort: 6379/TCP Endpoints: 10.32.0.6:6379,10.46.0.6:6379 Session Affinity: None Events: <none> Again, visit the vote app from browser, attempt to register your vote and observe what happens now.","title":"Creating Endpoints for Redis"},{"location":"7-vote-exposing_app_with_service/#reading","text":"Debugging * Services * Kubernetes Services Documentation * Service API Specs for Kubernetes Version 1.10","title":"Reading"},{"location":"9-vote-configmaps_and_secrets/","text":"Configmap Configmap is one of the ways to provide configurations to your application. Injecting env variables with configmaps Create our configmap for vote app file: projects/instavote/dev/vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote namespace: instavote data: OPTION_A: Visa OPTION_B: Mastercard In the above given configmap, we define two environment variables, OPTION_A=EMACS OPTION_B=VI Lets create the configmap object kubectl get cm kubectl apply -f vote-cm.yaml kubectl get cm kubectl describe cm vote In order to use this configmap in the deployment, we need to reference it from the deployment file. Check the deployment file for vote add for the following block. file: vote-deploy.yaml ... spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote envFrom: - configMapRef: name: vote ports: - containerPort: 80 protocol: TCP restartPolicy: Always So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code. kubectl apply -f vote-deploy.yaml Watch the monitoring screen for deployment in progress. kubectl get deploy --show-labels kubectl get rs --show-labels kubectl rollout status deploy/vote Providing environment Specific Configs Copy the dev env to staging cd k8s-code/projects/instavote cp -r dev staging Edit the configurations for staging cd staging Edit vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote data: OPTION_A: Apple OPTION_B: Samsung Edit vote-svc.yaml remove namespace if set remove extIP remove hard coded nodePort config if any file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 type: NodePort Edit vote-deploy.yaml remove namespace if set change replicas to 2 Lets launch it all in another namespace. We could use default namespace for this purpose. kubectl config set-context $(kubectl config current-context) --namespace=default Verify the namespace has been switched by observing the monitoring screen. Deploy new objects in this nmespace kubectl apply -f vote-svc.yaml kubectl apply -f vote-cm.yaml kubectl apply -f vote-deploy.yaml Now connect to the vote service by finding out the nodePort configs kubectl get svc vote Troubleshooting Do you see the application when you browse to http://host:nodeport If not, why? Find the root cause and fix it. Clean up and Switch back the namespace Verify the environment specific options are in effect. Once verified, you could switch the namespace back to instavote. kubectl delete deploy/vote svc/vote kubectl config set-context $(kubectl config current-context) --namespace=instavote cd ../dev/ Configuration file as ConfigMap In the topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file. Syntax for consuming file as a configmap is as follows kubectl create configmap --from-file <CONF-FILE-PATH> <NAME-OF-CONFIGMAP> We have redis configuration as a file named projects/instavote/config/redis.conf . We are going to convert this file into a configmap cd k8s-code/projects/instavote/config/ kubectl create configmap --from-file redis.conf redis Now validate the configs kubectl get cm kubectl describe cm redis To use this confif file, update your redis-deploy.yaml file to use it by mounting it as a volume. File: dev/redis-deploy.yaml spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP volumeMounts: - name: redis subPath: redis.conf mountPath: /etc/redis.conf volumes: - name: redis configMap: name: redis restartPolicy: Always And apply it kubectl apply -f redis-deploy.yaml kubectl apply -f redis-svc.yaml kubectl get rs,deploy --show-labels Exercise: Connect to redis pod and verify configs. Secrets Secrets are for storing sensitive data like passwords and keychains . We will see how db deployment uses username and password in form of a secret. You would define two fields for db, username password To create secrets for db you need to generate base64 format as follows, echo \"admin\" | base64 echo \"password\" | base64 where admin and password are the actual values that you would want to inject into the pod environment. If you do not have a unix host, you can make use of online base64 utility to generate these strings. http://www.utilities-online.info/base64 Lets now add it to the secrets file, File: projects/instavote/dev/db-secrets.yaml apiVersion: v1 kind: Secret metadata: name: db namespace: instavote type: Opaque data: POSTGRES_USER: YWRtaW4= POSTGRES_PASSWD: cGFzc3dvcmQ= kubectl apply -f db-secrets.yaml kubectl get secrets kubectl describe secret db Secrets can be referred to inside a container spec with following syntax env: - name: VAR valueFrom: secretKeyRef: name: db key: SECRET_VAR To consume these secrets, update the deployment for db file: db-deploy.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: db namespace: instavote spec: replicas: 1 selector: matchLabels: tier: back app: postgres minReadySeconds: 10 template: metadata: labels: app: postgres role: db tier: back spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP # Secret definition env: - name: POSTGRES_USER valueFrom: secretKeyRef: name: db key: POSTGRES_USER - name: POSTGRES_PASSWD valueFrom: secretKeyRef: name: db key: POSTGRES_PASSWD restartPolicy: Always To apply this, kubectl apply -f db-deploy.yaml kubectl apply -f db-svc.yaml kubectl get pods,rs,deploy kubectl exec -it db-xxxx sh [replace db-xxxx with pod name for db ] env | grep -i postgres [here you should see the env vars defined for POSTGRES_USER and POSTGRES_PASS] Note: Automatic Updation of deployments on ConfigMap Updates Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes. There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368 Currently, this can be done by using immutable configMaps. Create a configMaps and apply it with deployment. To update, create a new configMaps and do not update the previous one. Treat it as immutable. Update deployment spec to use the new version of the configMaps. This will ensure immediate update.","title":"Managing Configurations and Secrets"},{"location":"9-vote-configmaps_and_secrets/#configmap","text":"Configmap is one of the ways to provide configurations to your application.","title":"Configmap"},{"location":"9-vote-configmaps_and_secrets/#injecting-env-variables-with-configmaps","text":"Create our configmap for vote app file: projects/instavote/dev/vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote namespace: instavote data: OPTION_A: Visa OPTION_B: Mastercard In the above given configmap, we define two environment variables, OPTION_A=EMACS OPTION_B=VI Lets create the configmap object kubectl get cm kubectl apply -f vote-cm.yaml kubectl get cm kubectl describe cm vote In order to use this configmap in the deployment, we need to reference it from the deployment file. Check the deployment file for vote add for the following block. file: vote-deploy.yaml ... spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote envFrom: - configMapRef: name: vote ports: - containerPort: 80 protocol: TCP restartPolicy: Always So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code. kubectl apply -f vote-deploy.yaml Watch the monitoring screen for deployment in progress. kubectl get deploy --show-labels kubectl get rs --show-labels kubectl rollout status deploy/vote","title":"Injecting env variables with configmaps"},{"location":"9-vote-configmaps_and_secrets/#providing-environment-specific-configs","text":"Copy the dev env to staging cd k8s-code/projects/instavote cp -r dev staging Edit the configurations for staging cd staging Edit vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote data: OPTION_A: Apple OPTION_B: Samsung Edit vote-svc.yaml remove namespace if set remove extIP remove hard coded nodePort config if any file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 type: NodePort Edit vote-deploy.yaml remove namespace if set change replicas to 2 Lets launch it all in another namespace. We could use default namespace for this purpose. kubectl config set-context $(kubectl config current-context) --namespace=default Verify the namespace has been switched by observing the monitoring screen. Deploy new objects in this nmespace kubectl apply -f vote-svc.yaml kubectl apply -f vote-cm.yaml kubectl apply -f vote-deploy.yaml Now connect to the vote service by finding out the nodePort configs kubectl get svc vote Troubleshooting Do you see the application when you browse to http://host:nodeport If not, why? Find the root cause and fix it.","title":"Providing environment Specific Configs"},{"location":"9-vote-configmaps_and_secrets/#clean-up-and-switch-back-the-namespace","text":"Verify the environment specific options are in effect. Once verified, you could switch the namespace back to instavote. kubectl delete deploy/vote svc/vote kubectl config set-context $(kubectl config current-context) --namespace=instavote cd ../dev/","title":"Clean up and Switch back the  namespace"},{"location":"9-vote-configmaps_and_secrets/#configuration-file-as-configmap","text":"In the topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file. Syntax for consuming file as a configmap is as follows kubectl create configmap --from-file <CONF-FILE-PATH> <NAME-OF-CONFIGMAP> We have redis configuration as a file named projects/instavote/config/redis.conf . We are going to convert this file into a configmap cd k8s-code/projects/instavote/config/ kubectl create configmap --from-file redis.conf redis Now validate the configs kubectl get cm kubectl describe cm redis To use this confif file, update your redis-deploy.yaml file to use it by mounting it as a volume. File: dev/redis-deploy.yaml spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP volumeMounts: - name: redis subPath: redis.conf mountPath: /etc/redis.conf volumes: - name: redis configMap: name: redis restartPolicy: Always And apply it kubectl apply -f redis-deploy.yaml kubectl apply -f redis-svc.yaml kubectl get rs,deploy --show-labels Exercise: Connect to redis pod and verify configs.","title":"Configuration file as ConfigMap"},{"location":"9-vote-configmaps_and_secrets/#secrets","text":"Secrets are for storing sensitive data like passwords and keychains . We will see how db deployment uses username and password in form of a secret. You would define two fields for db, username password To create secrets for db you need to generate base64 format as follows, echo \"admin\" | base64 echo \"password\" | base64 where admin and password are the actual values that you would want to inject into the pod environment. If you do not have a unix host, you can make use of online base64 utility to generate these strings. http://www.utilities-online.info/base64 Lets now add it to the secrets file, File: projects/instavote/dev/db-secrets.yaml apiVersion: v1 kind: Secret metadata: name: db namespace: instavote type: Opaque data: POSTGRES_USER: YWRtaW4= POSTGRES_PASSWD: cGFzc3dvcmQ= kubectl apply -f db-secrets.yaml kubectl get secrets kubectl describe secret db Secrets can be referred to inside a container spec with following syntax env: - name: VAR valueFrom: secretKeyRef: name: db key: SECRET_VAR To consume these secrets, update the deployment for db file: db-deploy.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: db namespace: instavote spec: replicas: 1 selector: matchLabels: tier: back app: postgres minReadySeconds: 10 template: metadata: labels: app: postgres role: db tier: back spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP # Secret definition env: - name: POSTGRES_USER valueFrom: secretKeyRef: name: db key: POSTGRES_USER - name: POSTGRES_PASSWD valueFrom: secretKeyRef: name: db key: POSTGRES_PASSWD restartPolicy: Always To apply this, kubectl apply -f db-deploy.yaml kubectl apply -f db-svc.yaml kubectl get pods,rs,deploy kubectl exec -it db-xxxx sh [replace db-xxxx with pod name for db ] env | grep -i postgres [here you should see the env vars defined for POSTGRES_USER and POSTGRES_PASS]","title":"Secrets"},{"location":"9-vote-configmaps_and_secrets/#note-automatic-updation-of-deployments-on-configmap-updates","text":"Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes. There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368 Currently, this can be done by using immutable configMaps. Create a configMaps and apply it with deployment. To update, create a new configMaps and do not update the previous one. Treat it as immutable. Update deployment spec to use the new version of the configMaps. This will ensure immediate update.","title":"Note: Automatic Updation of deployments on ConfigMap  Updates"},{"location":"advanced_deployment/","text":"Deployments Deployments provide reliability and scalability to our application stack. Deployment makes sure that desired number of pods, which is specified declaratively in the deployment file (ex. catalogue-deploy.yml), are always up and running. If a pod fails to run, deployment will remove that pod and replace it with a new one. Deployments vs Replication Controllers Replication Controllers are the older version of Replica Sets. In some ways RC are superior than RS and in some cases it is inferior. But, how does replication work when compared to deployments? Replication Controllers are, * Older method of deploying application * Updates to the application is done with kubectl rolling-update command * Does not support roll back feature like in Deployments. * It has no backends, interacts with pods directly. Deployments are, * The newer method of deploying application. * Provides both rolling update and rollback * Replica Sets are the backend of Deployments. Deployment API Specs A typical deployment file looks like the following, apiVersion: apps/v1beta1 kind: Deployment metadata: name: <name-of-deployment> label: <deployment-labels> spec: replicas: <number-of-replicas> template: metadata: label: <pod-labels> spec: containers: <pod-spec> Defining Deployment Strategies We have two types of deployment strategies in Kubernetes, which are, * Rolling Update * Recreate. Rolling Update Let us look at our catalogue deployment file, File: catalogue-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: catalogue namespace: instavote labels: app: catalogue env: dev spec: replicas: 1 template: metadata: labels: app: catalogue env: dev spec: tolerations: - key: \"dedicate\" operator: \"Equal\" value: \"catalogue\" effect: \"NoExecute\" containers: - name: catalogue image: schoolofdevops/catalogue imagePullPolicy: Always ports: - containerPort: 80 We have not defined any deployment strategies here. Let us apply this deployment file. kubectl apply -f catalogue-deploy.yml [output] deployment \"catalogue\" created kubectl get deployment catalogue --export -o yaml [...] strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate As we can see, deployment strategy of rolling update is applied by default . When we use rolling update, * We can avoid down time since old version of application is still receiving traffic. * Only a few pod is updated at a time. Along with the type RollingUpdate, we see two fields. * maxSurge * maxUnavailable MaxSurge The maximum batch size of the available pods will get updated at once. For example, if we have 4 pods running with max surge value of 25%, then one pod will be updated at once. MaxUnavailable As the title implies, how many pods can be unavailable during the update process. Rolling updates are the preferred way to update an application. It is slower than recreate type, but it provides us with zero downtime deployments. When you create a deployment, a replica set for that deployment is also created. kubectl get rs --selector app=catalogue [output] NAME DESIRED CURRENT READY AGE catalogue-6bc67654d5 1 1 1 17m When you do an update, a new replica set is created. But the old replica set is kept for roll back purposes. Let us change the deployment to use a new version. File: catalogue-deploy.yml [...] containers: - name: catalogue image: schoolofdevops/catalogue:v1 imagePullPolicy: Always ports: - containerPort: 80 Apply the changes kubectl apply -f catalogue-deploy.yml kubectl get pods --selector app=catalogue [output] NAME READY STATUS RESTARTS AGE catalogue-6bc67654d5-tbn9h 0/1 ContainerCreating 0 7s catalogue-6c4f7b49d8-bdtgh 1/1 Running 0 8m kubectl get rs --selector app=catalogue [output] NAME DESIRED CURRENT READY AGE catalogue-6bc67654d5 0 0 0 21m catalogue-6c4f7b49d8 1 1 1 1m kubectl rollout status deployment catalogue [output] deployment \"catalogue\" successfully rolled kubectl rollout history deployment catalogue [output] deployments \"catalogue\" REVISION CHANGE-CAUSE 1 <none> 2 <none> As we can see, we have two revisions of our deployment. This revisions are used for roll backs about which we will learn later in this chapter. Recreate When the Recreate deployment strategy is used, * The old pods will be deleted * Then the new pods will be created. This will create some downtime in our stack. Hence, this strategy is only recommended for development use cases. Let us change the deployment strategy to recreate and image tag to latest . File: catalogue-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: catalogue namespace: instavote labels: app: catalogue env: dev spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: catalogue env: dev spec: tolerations: - key: \"dedicate\" operator: \"Equal\" value: \"catalogue\" effect: \"NoExecute\" containers: - name: catalogue image: schoolofdevops/catalogue:latest imagePullPolicy: Always ports: - containerPort: 80 Rollbacks As we discussed earlier, the major advantage of using deployments over replication controller is its roll back feature. Let us see how it is done. First check the image being used by the deployment. kubectl describe deployment catalogue [...] Containers: catalogue: Image: schoolofdevops/catalogue:v1 Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> The current deployment has the image with the tag v1 . kubectl rollout history deployment catalogue [output] deployments \"catalogue\" REVISION CHANGE-CAUSE 1 <none> 2 <none> We have two revisions. Revision 1 is the first ever deployment we have done while revision 2 is the one in which we changed the image tag to v1. Let us rollback to revision 1. Let us rollback to revision 1 which has the image tag of none . kubectl rollout undo deployment catalogue --to-revision=1 [output] deployment \"catalogue\" kubectl describe deployment catalogue [...] Containers: catalogue: Image: schoolofdevops/catalogue Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> Pause/Unpause When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations, 1. we can pause the update, 2. fix the issue, 3. resume the update. Let us change the image tag to V2 in pod spec. File: catalogue-deploy.yml containers: - name: catalogue image: schoolofdevops/catalogue:V2 imagePullPolicy: Always ports: - containerPort: 80 Apply the changes. kubectl apply -f catalogue-deploy.yml kubectl get pods [Output] NAME READY STATUS RESTARTS AGE catalogue-6c4f7b49d8-g5dgc 1/1 Running 0 16m catalogue-765554cc7-xsbhs 0/1 ErrImagePull 0 9s Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag. Now pause the update kubectl rollout pause Set the deployment to use v2 version of the image. kubectl set image deployment catalogue catalogue=schoolofdevops/catalogue:v2 [output] deployment \"catalogue\" image updated Now resume the update kubectl rollout resume deployment catalogue kubectl rollout status deployment catalogue [Ouput] deployment \"catalogue\" successfully rolled out kubectl get pods [Output] NAME READY STATUS RESTARTS AGE catalogue-6875c8df8f-k4hls 1/1 Running 0 1m When we do this, we skip the need of creating a new rolling update altogether. Additional Release Strategies Along with rolling update and recreate strategies, we can also do, Canary releases Blue/Green deployments. Canary Releases Blue/Green Deployments Create two more catalogue images with different products. Recreate type gives an error. Need to be fixed. Add Use-cases for recreate strategy type. Add additional details of why we skip creating a replica set / rolling update in pause/unpause section.","title":"Advanced deployment"},{"location":"advanced_deployment/#deployments","text":"Deployments provide reliability and scalability to our application stack. Deployment makes sure that desired number of pods, which is specified declaratively in the deployment file (ex. catalogue-deploy.yml), are always up and running. If a pod fails to run, deployment will remove that pod and replace it with a new one.","title":"Deployments"},{"location":"advanced_deployment/#deployments-vs-replication-controllers","text":"Replication Controllers are the older version of Replica Sets. In some ways RC are superior than RS and in some cases it is inferior. But, how does replication work when compared to deployments? Replication Controllers are, * Older method of deploying application * Updates to the application is done with kubectl rolling-update command * Does not support roll back feature like in Deployments. * It has no backends, interacts with pods directly. Deployments are, * The newer method of deploying application. * Provides both rolling update and rollback * Replica Sets are the backend of Deployments.","title":"Deployments vs Replication Controllers"},{"location":"advanced_deployment/#deployment-api-specs","text":"A typical deployment file looks like the following, apiVersion: apps/v1beta1 kind: Deployment metadata: name: <name-of-deployment> label: <deployment-labels> spec: replicas: <number-of-replicas> template: metadata: label: <pod-labels> spec: containers: <pod-spec>","title":"Deployment API Specs"},{"location":"advanced_deployment/#defining-deployment-strategies","text":"We have two types of deployment strategies in Kubernetes, which are, * Rolling Update * Recreate.","title":"Defining Deployment Strategies"},{"location":"advanced_deployment/#rolling-update","text":"Let us look at our catalogue deployment file, File: catalogue-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: catalogue namespace: instavote labels: app: catalogue env: dev spec: replicas: 1 template: metadata: labels: app: catalogue env: dev spec: tolerations: - key: \"dedicate\" operator: \"Equal\" value: \"catalogue\" effect: \"NoExecute\" containers: - name: catalogue image: schoolofdevops/catalogue imagePullPolicy: Always ports: - containerPort: 80 We have not defined any deployment strategies here. Let us apply this deployment file. kubectl apply -f catalogue-deploy.yml [output] deployment \"catalogue\" created kubectl get deployment catalogue --export -o yaml [...] strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate As we can see, deployment strategy of rolling update is applied by default . When we use rolling update, * We can avoid down time since old version of application is still receiving traffic. * Only a few pod is updated at a time. Along with the type RollingUpdate, we see two fields. * maxSurge * maxUnavailable","title":"Rolling Update"},{"location":"advanced_deployment/#maxsurge","text":"The maximum batch size of the available pods will get updated at once. For example, if we have 4 pods running with max surge value of 25%, then one pod will be updated at once.","title":"MaxSurge"},{"location":"advanced_deployment/#maxunavailable","text":"As the title implies, how many pods can be unavailable during the update process. Rolling updates are the preferred way to update an application. It is slower than recreate type, but it provides us with zero downtime deployments. When you create a deployment, a replica set for that deployment is also created. kubectl get rs --selector app=catalogue [output] NAME DESIRED CURRENT READY AGE catalogue-6bc67654d5 1 1 1 17m When you do an update, a new replica set is created. But the old replica set is kept for roll back purposes. Let us change the deployment to use a new version. File: catalogue-deploy.yml [...] containers: - name: catalogue image: schoolofdevops/catalogue:v1 imagePullPolicy: Always ports: - containerPort: 80 Apply the changes kubectl apply -f catalogue-deploy.yml kubectl get pods --selector app=catalogue [output] NAME READY STATUS RESTARTS AGE catalogue-6bc67654d5-tbn9h 0/1 ContainerCreating 0 7s catalogue-6c4f7b49d8-bdtgh 1/1 Running 0 8m kubectl get rs --selector app=catalogue [output] NAME DESIRED CURRENT READY AGE catalogue-6bc67654d5 0 0 0 21m catalogue-6c4f7b49d8 1 1 1 1m kubectl rollout status deployment catalogue [output] deployment \"catalogue\" successfully rolled kubectl rollout history deployment catalogue [output] deployments \"catalogue\" REVISION CHANGE-CAUSE 1 <none> 2 <none> As we can see, we have two revisions of our deployment. This revisions are used for roll backs about which we will learn later in this chapter.","title":"MaxUnavailable"},{"location":"advanced_deployment/#recreate","text":"When the Recreate deployment strategy is used, * The old pods will be deleted * Then the new pods will be created. This will create some downtime in our stack. Hence, this strategy is only recommended for development use cases. Let us change the deployment strategy to recreate and image tag to latest . File: catalogue-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: catalogue namespace: instavote labels: app: catalogue env: dev spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: catalogue env: dev spec: tolerations: - key: \"dedicate\" operator: \"Equal\" value: \"catalogue\" effect: \"NoExecute\" containers: - name: catalogue image: schoolofdevops/catalogue:latest imagePullPolicy: Always ports: - containerPort: 80","title":"Recreate"},{"location":"advanced_deployment/#rollbacks","text":"As we discussed earlier, the major advantage of using deployments over replication controller is its roll back feature. Let us see how it is done. First check the image being used by the deployment. kubectl describe deployment catalogue [...] Containers: catalogue: Image: schoolofdevops/catalogue:v1 Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> The current deployment has the image with the tag v1 . kubectl rollout history deployment catalogue [output] deployments \"catalogue\" REVISION CHANGE-CAUSE 1 <none> 2 <none> We have two revisions. Revision 1 is the first ever deployment we have done while revision 2 is the one in which we changed the image tag to v1. Let us rollback to revision 1. Let us rollback to revision 1 which has the image tag of none . kubectl rollout undo deployment catalogue --to-revision=1 [output] deployment \"catalogue\" kubectl describe deployment catalogue [...] Containers: catalogue: Image: schoolofdevops/catalogue Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"Rollbacks"},{"location":"advanced_deployment/#pauseunpause","text":"When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations, 1. we can pause the update, 2. fix the issue, 3. resume the update. Let us change the image tag to V2 in pod spec. File: catalogue-deploy.yml containers: - name: catalogue image: schoolofdevops/catalogue:V2 imagePullPolicy: Always ports: - containerPort: 80 Apply the changes. kubectl apply -f catalogue-deploy.yml kubectl get pods [Output] NAME READY STATUS RESTARTS AGE catalogue-6c4f7b49d8-g5dgc 1/1 Running 0 16m catalogue-765554cc7-xsbhs 0/1 ErrImagePull 0 9s Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag. Now pause the update kubectl rollout pause Set the deployment to use v2 version of the image. kubectl set image deployment catalogue catalogue=schoolofdevops/catalogue:v2 [output] deployment \"catalogue\" image updated Now resume the update kubectl rollout resume deployment catalogue kubectl rollout status deployment catalogue [Ouput] deployment \"catalogue\" successfully rolled out kubectl get pods [Output] NAME READY STATUS RESTARTS AGE catalogue-6875c8df8f-k4hls 1/1 Running 0 1m When we do this, we skip the need of creating a new rolling update altogether.","title":"Pause/Unpause"},{"location":"advanced_deployment/#additional-release-strategies","text":"Along with rolling update and recreate strategies, we can also do, Canary releases Blue/Green deployments.","title":"Additional Release Strategies"},{"location":"advanced_deployment/#canary-releases","text":"","title":"Canary Releases"},{"location":"advanced_deployment/#bluegreen-deployments","text":"Create two more catalogue images with different products. Recreate type gives an error. Need to be fixed. Add Use-cases for recreate strategy type. Add additional details of why we skip creating a replica set / rolling update in pause/unpause section.","title":"Blue/Green Deployments"},{"location":"advanced_pod_scheduling/","text":"Advanced Pod Scheduling In the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling. From the api document for version 1.11 following are the pod specs which are relevant from scheduling perspective. nodeSelector nodeName affinity schedulerName tolerations Using Node Selectors kubectl get nodes --show-labels kubectl label nodes <node-name> zone=aaa kubectl get nodes --show-labels e.g. kubectl label nodes node1 zone=bbb kubectl label nodes node2 zone=bbb kubectl label nodes node3 zone=aaa kubectl label nodes node4 zone=aaa kubectl get nodes --show-labels [sample output] NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node2 Ready master,node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node3 Ready node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa node4 Ready node 21h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa Check how the pods are distributed on the nodes using the following command, kubectl get pods -o wide --selector=\"role=vote\" NAME READY STATUS RESTARTS AGE IP NODE vote-5d88d47fc8-6rflg 1/1 Running 0 1m 10.233.75.9 node2 vote-5d88d47fc8-gbzbq 1/1 Running 0 1h 10.233.74.76 node4 vote-5d88d47fc8-q4vj6 1/1 Running 0 1h 10.233.102.133 node1 vote-5d88d47fc8-znd2z 1/1 Running 0 1m 10.233.71.20 node3 From the above output, you could see that the pods running vote app are currently equally distributed. Now, update pod definition to make it schedule only on nodes in zone bbb file: k8s-code/pods/vote-pod.yml .... template: ... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP nodeSelector: zone: 'bbb' For this change, pod needs to be re created. kubectl apply -f vote-pod.yml You would notice that, the moment you make that change, a new rollout kicks off, which will start redistributing the pods, now following the nodeSelector constraint that you added. Watch the output of the following command watch kubectl get pods -o wide --selector=\"role=vote\" You will see the following while it transitions NAME READY STATUS RESTARTS AGE IP NODE pod/vote-5d88d47fc8-6rflg 0/1 Terminating 0 5m 10.233.75.9 node2 pod/vote-5d88d47fc8-gbzbq 0/1 Terminating 0 1h 10.233.74.76 node4 pod/vote-5d88d47fc8-q4vj6 0/1 Terminating 0 1h 10.233.102.133 node1 pod/vote-67d7dd8f89-2w5wl 1/1 Running 0 44s 10.233.75.10 node2 pod/vote-67d7dd8f89-gm6bq 0/1 ContainerCreating 0 2s <none> node2 pod/vote-67d7dd8f89-w87n9 1/1 Running 0 44s 10.233.102.134 node1 pod/vote-67d7dd8f89-xccl8 1/1 Running 0 44s 10.233.102.135 node1 and after the rollout completes, NAME READY STATUS RESTARTS AGE IP NODE vote-67d7dd8f89-2w5wl 1/1 Running 0 2m 10.233.75.10 node2 vote-67d7dd8f89-gm6bq 1/1 Running 0 1m 10.233.75.11 node2 vote-67d7dd8f89-w87n9 1/1 Running 0 2m 10.233.102.134 node1 vote-67d7dd8f89-xccl8 1/1 Running 0 2m 10.233.102.135 node1 Exercise Just like nodeSelector above, you could enforce a pod to run on a specific node using nodeName . Try using that property to run all pods for results application on node3 Defining affinity and anti-affinity We have discussed about scheduling a pod on a particular node using NodeSelector , but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions. required preferred DuringScheduling DuringExecution Operators In NotIn Exists DoesNotExist Gt Lt Node Affinity Examine the current pod distribution kubectl get pods -o wide --selector=\"role=vote\" NAME READY STATUS RESTARTS AGE IP NODE vote-8546bbd84d-22d6x 1/1 Running 0 35s 10.233.102.137 node1 vote-8546bbd84d-8f9bc 1/1 Running 0 1m 10.233.102.136 node1 vote-8546bbd84d-bpg8f 1/1 Running 0 1m 10.233.75.12 node2 vote-8546bbd84d-d8j9g 1/1 Running 0 1m 10.233.75.13 node2 and node labels kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node2 Ready master,node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node3 Ready node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa node4 Ready node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa Lets create node affinity criteria as Pods for vote app must not run on the master nodes Pods for vote app preferably run on a node in zone bbb First is a hard affinity versus second being soft affinity. file: vote-deploy.yaml .... template: .... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: DoesNotExist preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: zone Pod Affinity Lets define pod affinity criteria as, Pods for vote and redis should be co located as much as possible (preferred) No two pods with redis app should be running on the same node (required) kubectl get pods -o wide --selector=\"role in (vote,redis)\" [sample output] NAME READY STATUS RESTARTS AGE IP NODE redis-6555998885-4k5cr 1/1 Running 0 4h 10.233.71.19 node3 redis-6555998885-fb8rk 1/1 Running 0 4h 10.233.102.132 node1 vote-74c894d6f5-bql8z 1/1 Running 0 22m 10.233.74.78 node4 vote-74c894d6f5-nnzmc 1/1 Running 0 21m 10.233.71.22 node3 vote-74c894d6f5-ss929 1/1 Running 0 22m 10.233.74.77 node4 vote-74c894d6f5-tpzgm 1/1 Running 0 22m 10.233.71.21 node3 file: vote-deploy.yaml ... template: ... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP affinity: ... podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: role operator: In values: - redis topologyKey: kubernetes.io/hostname file: redis-deploy.yaml .... template: ... spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP restartPolicy: Always affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: role operator: In values: - redis topologyKey: \"kubernetes.io/hostname\" apply kubectl apply -f redis-deploy.yaml kubectl apply -f vote-deploy.yaml check the pods distribution kubectl get pods -o wide --selector=\"role in (vote,redis)\" [sample output ] NAME READY STATUS RESTARTS AGE IP NODE redis-5bf748dbcf-gr8zg 1/1 Running 0 13m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 13m 10.233.74.79 node4 vote-56bf599b9c-22lpw 1/1 Running 0 12m 10.233.74.80 node4 vote-56bf599b9c-nvvfd 1/1 Running 0 13m 10.233.71.25 node3 vote-56bf599b9c-w6jc9 1/1 Running 0 13m 10.233.71.23 node3 vote-56bf599b9c-ztdgm 1/1 Running 0 13m 10.233.71.24 node3 Observations from the above output, Since redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes (node2 and node4) since vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3 If you kill the pods on node3, at the time of scheduling new ones, scheduler meets all affinity rules $ kubectl delete pods vote-56bf599b9c-nvvfd vote-56bf599b9c-w6jc9 vote-56bf599b9c-ztdgm pod \"vote-56bf599b9c-nvvfd\" deleted pod \"vote-56bf599b9c-w6jc9\" deleted pod \"vote-56bf599b9c-ztdgm\" deleted $ kubectl get pods -o wide --selector=\"role in (vote,redis)\" NAME READY STATUS RESTARTS AGE IP NODE redis-5bf748dbcf-gr8zg 1/1 Running 0 19m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 19m 10.233.74.79 node4 vote-56bf599b9c-22lpw 1/1 Running 0 19m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 20s 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 20s 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 19s 10.233.74.81 node4 Taints and tolerations Affinity is defined for pods Taints are defined for nodes You could add the taints with criteria and effects. Effetcs can be Taint Specs : effect NoSchedule PreferNoSchedule NoExecute key value timeAdded (only written for NoExecute taints) Observe the pods distribution $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-gr8zg 1/1 Running 0 27m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 27m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 26m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 8m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 8m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 8m 10.233.74.81 node4 worker-7c98c96fb4-7tzzw 1/1 Running 1 4h 10.233.75.8 node2 Lets taint a node. kubectl taint node node2 dedicated=worker:NoExecute after taining the node $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 2m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 30m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 29m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 11m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 11m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 11m 10.233.74.81 node4 worker-7c98c96fb4-46ltl 1/1 Running 0 2m 10.233.102.140 node1 All pods running on node2 just got evicted. Add toleration in the Deployment for worker. File: worker-deploy.yml apiVersion: apps/v1 ..... template: .... spec: containers: - name: app image: schoolofdevops/vote-worker:latest tolerations: - key: \"dedicated\" operator: \"Equal\" value: \"worker\" effect: \"NoExecute\" apply kubectl apply -f worker-deploy.yml Observe the pod distribution now. $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 3m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 31m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 30m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 12m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 12m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 12m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 1m 10.233.75.15 node2 You should see worker being scheduled on node2 To remove the taint created above kubectl taint node node2 dedicate:NoExecute-","title":"Advanced Pod Scheduling"},{"location":"advanced_pod_scheduling/#advanced-pod-scheduling","text":"In the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling. From the api document for version 1.11 following are the pod specs which are relevant from scheduling perspective. nodeSelector nodeName affinity schedulerName tolerations","title":"Advanced Pod Scheduling"},{"location":"advanced_pod_scheduling/#using-node-selectors","text":"kubectl get nodes --show-labels kubectl label nodes <node-name> zone=aaa kubectl get nodes --show-labels e.g. kubectl label nodes node1 zone=bbb kubectl label nodes node2 zone=bbb kubectl label nodes node3 zone=aaa kubectl label nodes node4 zone=aaa kubectl get nodes --show-labels [sample output] NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node2 Ready master,node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node3 Ready node 22h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa node4 Ready node 21h v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa Check how the pods are distributed on the nodes using the following command, kubectl get pods -o wide --selector=\"role=vote\" NAME READY STATUS RESTARTS AGE IP NODE vote-5d88d47fc8-6rflg 1/1 Running 0 1m 10.233.75.9 node2 vote-5d88d47fc8-gbzbq 1/1 Running 0 1h 10.233.74.76 node4 vote-5d88d47fc8-q4vj6 1/1 Running 0 1h 10.233.102.133 node1 vote-5d88d47fc8-znd2z 1/1 Running 0 1m 10.233.71.20 node3 From the above output, you could see that the pods running vote app are currently equally distributed. Now, update pod definition to make it schedule only on nodes in zone bbb file: k8s-code/pods/vote-pod.yml .... template: ... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP nodeSelector: zone: 'bbb' For this change, pod needs to be re created. kubectl apply -f vote-pod.yml You would notice that, the moment you make that change, a new rollout kicks off, which will start redistributing the pods, now following the nodeSelector constraint that you added. Watch the output of the following command watch kubectl get pods -o wide --selector=\"role=vote\" You will see the following while it transitions NAME READY STATUS RESTARTS AGE IP NODE pod/vote-5d88d47fc8-6rflg 0/1 Terminating 0 5m 10.233.75.9 node2 pod/vote-5d88d47fc8-gbzbq 0/1 Terminating 0 1h 10.233.74.76 node4 pod/vote-5d88d47fc8-q4vj6 0/1 Terminating 0 1h 10.233.102.133 node1 pod/vote-67d7dd8f89-2w5wl 1/1 Running 0 44s 10.233.75.10 node2 pod/vote-67d7dd8f89-gm6bq 0/1 ContainerCreating 0 2s <none> node2 pod/vote-67d7dd8f89-w87n9 1/1 Running 0 44s 10.233.102.134 node1 pod/vote-67d7dd8f89-xccl8 1/1 Running 0 44s 10.233.102.135 node1 and after the rollout completes, NAME READY STATUS RESTARTS AGE IP NODE vote-67d7dd8f89-2w5wl 1/1 Running 0 2m 10.233.75.10 node2 vote-67d7dd8f89-gm6bq 1/1 Running 0 1m 10.233.75.11 node2 vote-67d7dd8f89-w87n9 1/1 Running 0 2m 10.233.102.134 node1 vote-67d7dd8f89-xccl8 1/1 Running 0 2m 10.233.102.135 node1","title":"Using Node Selectors"},{"location":"advanced_pod_scheduling/#exercise","text":"Just like nodeSelector above, you could enforce a pod to run on a specific node using nodeName . Try using that property to run all pods for results application on node3","title":"Exercise"},{"location":"advanced_pod_scheduling/#defining-affinity-and-anti-affinity","text":"We have discussed about scheduling a pod on a particular node using NodeSelector , but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions. required preferred DuringScheduling DuringExecution Operators In NotIn Exists DoesNotExist Gt Lt","title":"Defining  affinity and anti-affinity"},{"location":"advanced_pod_scheduling/#node-affinity","text":"Examine the current pod distribution kubectl get pods -o wide --selector=\"role=vote\" NAME READY STATUS RESTARTS AGE IP NODE vote-8546bbd84d-22d6x 1/1 Running 0 35s 10.233.102.137 node1 vote-8546bbd84d-8f9bc 1/1 Running 0 1m 10.233.102.136 node1 vote-8546bbd84d-bpg8f 1/1 Running 0 1m 10.233.75.12 node2 vote-8546bbd84d-d8j9g 1/1 Running 0 1m 10.233.75.13 node2 and node labels kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node2 Ready master,node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb node3 Ready node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa node4 Ready node 1d v1.10.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa Lets create node affinity criteria as Pods for vote app must not run on the master nodes Pods for vote app preferably run on a node in zone bbb First is a hard affinity versus second being soft affinity. file: vote-deploy.yaml .... template: .... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: DoesNotExist preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: zone","title":"Node Affinity"},{"location":"advanced_pod_scheduling/#pod-affinity","text":"Lets define pod affinity criteria as, Pods for vote and redis should be co located as much as possible (preferred) No two pods with redis app should be running on the same node (required) kubectl get pods -o wide --selector=\"role in (vote,redis)\" [sample output] NAME READY STATUS RESTARTS AGE IP NODE redis-6555998885-4k5cr 1/1 Running 0 4h 10.233.71.19 node3 redis-6555998885-fb8rk 1/1 Running 0 4h 10.233.102.132 node1 vote-74c894d6f5-bql8z 1/1 Running 0 22m 10.233.74.78 node4 vote-74c894d6f5-nnzmc 1/1 Running 0 21m 10.233.71.22 node3 vote-74c894d6f5-ss929 1/1 Running 0 22m 10.233.74.77 node4 vote-74c894d6f5-tpzgm 1/1 Running 0 22m 10.233.71.21 node3 file: vote-deploy.yaml ... template: ... spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP affinity: ... podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: role operator: In values: - redis topologyKey: kubernetes.io/hostname file: redis-deploy.yaml .... template: ... spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP restartPolicy: Always affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: role operator: In values: - redis topologyKey: \"kubernetes.io/hostname\" apply kubectl apply -f redis-deploy.yaml kubectl apply -f vote-deploy.yaml check the pods distribution kubectl get pods -o wide --selector=\"role in (vote,redis)\" [sample output ] NAME READY STATUS RESTARTS AGE IP NODE redis-5bf748dbcf-gr8zg 1/1 Running 0 13m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 13m 10.233.74.79 node4 vote-56bf599b9c-22lpw 1/1 Running 0 12m 10.233.74.80 node4 vote-56bf599b9c-nvvfd 1/1 Running 0 13m 10.233.71.25 node3 vote-56bf599b9c-w6jc9 1/1 Running 0 13m 10.233.71.23 node3 vote-56bf599b9c-ztdgm 1/1 Running 0 13m 10.233.71.24 node3 Observations from the above output, Since redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes (node2 and node4) since vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3 If you kill the pods on node3, at the time of scheduling new ones, scheduler meets all affinity rules $ kubectl delete pods vote-56bf599b9c-nvvfd vote-56bf599b9c-w6jc9 vote-56bf599b9c-ztdgm pod \"vote-56bf599b9c-nvvfd\" deleted pod \"vote-56bf599b9c-w6jc9\" deleted pod \"vote-56bf599b9c-ztdgm\" deleted $ kubectl get pods -o wide --selector=\"role in (vote,redis)\" NAME READY STATUS RESTARTS AGE IP NODE redis-5bf748dbcf-gr8zg 1/1 Running 0 19m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 19m 10.233.74.79 node4 vote-56bf599b9c-22lpw 1/1 Running 0 19m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 20s 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 20s 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 19s 10.233.74.81 node4","title":"Pod Affinity"},{"location":"advanced_pod_scheduling/#taints-and-tolerations","text":"Affinity is defined for pods Taints are defined for nodes You could add the taints with criteria and effects. Effetcs can be Taint Specs : effect NoSchedule PreferNoSchedule NoExecute key value timeAdded (only written for NoExecute taints) Observe the pods distribution $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-gr8zg 1/1 Running 0 27m 10.233.75.14 node2 redis-5bf748dbcf-vxppx 1/1 Running 0 27m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 26m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 8m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 8m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 8m 10.233.74.81 node4 worker-7c98c96fb4-7tzzw 1/1 Running 1 4h 10.233.75.8 node2 Lets taint a node. kubectl taint node node2 dedicated=worker:NoExecute after taining the node $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 2m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 30m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 29m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 11m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 11m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 11m 10.233.74.81 node4 worker-7c98c96fb4-46ltl 1/1 Running 0 2m 10.233.102.140 node1 All pods running on node2 just got evicted. Add toleration in the Deployment for worker. File: worker-deploy.yml apiVersion: apps/v1 ..... template: .... spec: containers: - name: app image: schoolofdevops/vote-worker:latest tolerations: - key: \"dedicated\" operator: \"Equal\" value: \"worker\" effect: \"NoExecute\" apply kubectl apply -f worker-deploy.yml Observe the pod distribution now. $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 4h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 3m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 31m 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 4h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 4h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 30m 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 12m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 12m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 12m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 1m 10.233.75.15 node2 You should see worker being scheduled on node2 To remove the taint created above kubectl taint node node2 dedicate:NoExecute-","title":"Taints and tolerations"},{"location":"advanced_services/","text":"Services Services are a way to expose your backend pods to outside world. Services can also be used for internal networks. Service Discovery Whenever we create a service, the kubernetes api server will create a route for that service within the cluster. So you will be able to access the service using the dns entry for that service. Usually the service dns will look like Let us create a service for frontend File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 kubectl apply -f frontend-svc.yml kubectl exec -it front-end-5cbc986f44-dqws2 sh nslookup front-end.instavote Name: front-end.instavote Address 1: 10.233.6.236 front-end.instavote.svc.cluster.local This is achieved by the cluster dns add-on . This is how backend of service A consumes service B. Labels and Selectors When we create a service, it automatically adds appropriate pods to its backend. How does this happen? This is achieved by a mechanism called labels and selectors . We label our pods with keys and values. Our service template has selectors with these keys and values. So when a service is created, it will check for pods running with the same label. If it finds a pod with that label, the pod will be added to endpoint of the service. kubectl describe svc front-end Name: front-end Namespace: instavote Labels: app=front-end env=dev Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"front-end\",\"env\":\"dev\"},\"name\":\"front-end\",\"namespace\":\"instavote\"},\"... Selector: app=front-end Type: ClusterIP IP: 10.233.6.236 Port: <unset> 8079/TCP TargetPort: 8079/TCP Endpoints: 10.233.71.13:8079 Session Affinity: None Events: <none> Since we have only one pod running with the label app=front-end , it is added to the service endpoint list. Picking up the right type of a service We have four type of services to choose from. Each has a unique use case for which it can be used. Let us see about the types of services below. Types: ClusterIp Internal network. Can not accessed from outside world. Best suited for backend services which should not be exposed (ex: DB). NodePort Can be accessed from outside world. NodePort is best suited for development environment(debugging). Not suited for production use. LoadBalancer Normal cloud load balancer. Exposes service to outside world. ExternalName Used for entities which are outside the cluster. CNAME record for an external service. No ports are defined. We have one more service configuration, which is not exactly a service type. That is ., * ExternalIP Cluster IP and DNS ClusterIP is internal to the cluster. ClusterIP is provided by default. If you do not define the type of the service, then the service is configured with a clusterIP. A typical service type of clusterIP looks like, File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: ClusterIP NodePort Exposes a port on node within 30000 to 32767 range. If a service is defined as nodePort, we can access the service on . Let us change the frontend service type to NodePort and see what happens. File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: NodePort Lets get the node port created for the front-end service. kubectl get svc front-end Visit this node port on any of node's IP. In my case, ExternalIP ExternalIP is not exactly a service type. It can be coupled with any other service type. For example a service with NodePort can be used with ExternalIP as well. Only cache here is, the external IP has to one of the node/master's IP. It is used to route the traffic to one or more cluster nodes. File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: NodePort externalIPs: - 10.40.1.26 - 10.40.1.11 Lets check how externalIPs works, kubectl describe svc front-end curl 10.40.1.26 curl 10.40.1.11 LoadBalancer Service type LoadBalancer is only supported on AWS,GCE,Azure and Openstack. If you have you cluster running on any of these clouds, give it a try. It will create a load balancer for us with a ephemeral IP. We can also specify a loadBalancerIP. Mostly not recommended to use. Using service type LoadBalancer will rise your cloud provider spendings. Think about launching 10 load balancers for 10 different services. Thats where ingress come into picture (explained in the later part). apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: LoadBalancer Headless services with Endpoints/External Names Sometimes you may to decouple the services from backend pods. Sometimes you may want to use some external services outside the cluster and want to integrate those external services with the cluster. For these kind of use cases we can use Headless services . Let us see an example. Let us consider a scenario where you have your redis cluster hosted outside the cluster. Your backend in cluster need to use this redis cluster. In this case, you will create a service with ExternalName, which is nothing but a CNAME record of your redis cluster. This type of services will neither have any ports defined nor have any selectors. But, how do you make sure the backend pods uses this service? For that, you will create a custom endpoint, in which map your service to specific endpoints. Ex: External Name service kind: Service apiVersion: v1 metadata: name: redis-aws namespace: instavote spec: type: ExternalName externalName: redis.aws.schoolofdevops.com Ex: Endpoints kind: Endpoints apiVersion: v1 metadata: name: redis-aws subsets: - addresses: - ip: 10.40.30.123 - ip: 10.40.30.324 - ip: 10.40.30.478 ports: - port: 9376 These IPs have to be manually managed by the cluster administrator. Ingress Ingress controller is an Kubernetes object that manages external access to the backend services. This is the preferred way of exposing your services. Ingress Controller To use an ingress resource, an ingress controller (ex. nginx, traefik) must be running. This controller has to be deployed manually. To create an ingress controller in our cluster, run the following commands, kubectl apply -f traefik-rbac.yaml kubectl apply -f traefik-deployment.yaml Check whether the ingress controller is running or not. kubectl get pods -n kube-system [...] kubernetes-dashboard-75c5ff6844-r9sk8 1/1 Running 6 3d nginx-proxy-node3 1/1 Running 8 27d nginx-proxy-node4 1/1 Running 7 27d traefik-ingress-controller-568dd77566-vk72b 1/1 Running 0 5m Purpose of an Ingress Ingress can be used for, * Load balancing, * SSL Termination, * Name based virtual hosting. Types of Ingress Let us see about different implementation of ingress. Keep or delete the following(Need to come up with use cases). ### Single Service Ingress ### Simple fanout Ingress ### Name based virtual hosting ### TLS ### Load Balancer Services Vs Ingress Service type LoadBalancer Service type Nodeport Creating service resources for applications and backing services Need a demo set up for types: load balancer and external name Need input on this: Creating service resources for applications and backing services, Service API specs, Traffic Policy Can add some points about scaling of ingress controller (running it as a daemonset on all the nodes)","title":"Services"},{"location":"advanced_services/#services","text":"Services are a way to expose your backend pods to outside world. Services can also be used for internal networks.","title":"Services"},{"location":"advanced_services/#service-discovery","text":"Whenever we create a service, the kubernetes api server will create a route for that service within the cluster. So you will be able to access the service using the dns entry for that service. Usually the service dns will look like Let us create a service for frontend File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 kubectl apply -f frontend-svc.yml kubectl exec -it front-end-5cbc986f44-dqws2 sh nslookup front-end.instavote Name: front-end.instavote Address 1: 10.233.6.236 front-end.instavote.svc.cluster.local This is achieved by the cluster dns add-on . This is how backend of service A consumes service B.","title":"Service Discovery"},{"location":"advanced_services/#labels-and-selectors","text":"When we create a service, it automatically adds appropriate pods to its backend. How does this happen? This is achieved by a mechanism called labels and selectors . We label our pods with keys and values. Our service template has selectors with these keys and values. So when a service is created, it will check for pods running with the same label. If it finds a pod with that label, the pod will be added to endpoint of the service. kubectl describe svc front-end Name: front-end Namespace: instavote Labels: app=front-end env=dev Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"front-end\",\"env\":\"dev\"},\"name\":\"front-end\",\"namespace\":\"instavote\"},\"... Selector: app=front-end Type: ClusterIP IP: 10.233.6.236 Port: <unset> 8079/TCP TargetPort: 8079/TCP Endpoints: 10.233.71.13:8079 Session Affinity: None Events: <none> Since we have only one pod running with the label app=front-end , it is added to the service endpoint list.","title":"Labels and Selectors"},{"location":"advanced_services/#picking-up-the-right-type-of-a-service","text":"We have four type of services to choose from. Each has a unique use case for which it can be used. Let us see about the types of services below.","title":"Picking up the right type of a service"},{"location":"advanced_services/#types","text":"ClusterIp Internal network. Can not accessed from outside world. Best suited for backend services which should not be exposed (ex: DB). NodePort Can be accessed from outside world. NodePort is best suited for development environment(debugging). Not suited for production use. LoadBalancer Normal cloud load balancer. Exposes service to outside world. ExternalName Used for entities which are outside the cluster. CNAME record for an external service. No ports are defined. We have one more service configuration, which is not exactly a service type. That is ., * ExternalIP","title":"Types:"},{"location":"advanced_services/#cluster-ip-and-dns","text":"ClusterIP is internal to the cluster. ClusterIP is provided by default. If you do not define the type of the service, then the service is configured with a clusterIP. A typical service type of clusterIP looks like, File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: ClusterIP","title":"Cluster IP and DNS"},{"location":"advanced_services/#nodeport","text":"Exposes a port on node within 30000 to 32767 range. If a service is defined as nodePort, we can access the service on . Let us change the frontend service type to NodePort and see what happens. File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: NodePort Lets get the node port created for the front-end service. kubectl get svc front-end Visit this node port on any of node's IP. In my case,","title":"NodePort"},{"location":"advanced_services/#externalip","text":"ExternalIP is not exactly a service type. It can be coupled with any other service type. For example a service with NodePort can be used with ExternalIP as well. Only cache here is, the external IP has to one of the node/master's IP. It is used to route the traffic to one or more cluster nodes. File: frontend-svc.yml apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: NodePort externalIPs: - 10.40.1.26 - 10.40.1.11 Lets check how externalIPs works, kubectl describe svc front-end curl 10.40.1.26 curl 10.40.1.11","title":"ExternalIP"},{"location":"advanced_services/#loadbalancer","text":"Service type LoadBalancer is only supported on AWS,GCE,Azure and Openstack. If you have you cluster running on any of these clouds, give it a try. It will create a load balancer for us with a ephemeral IP. We can also specify a loadBalancerIP. Mostly not recommended to use. Using service type LoadBalancer will rise your cloud provider spendings. Think about launching 10 load balancers for 10 different services. Thats where ingress come into picture (explained in the later part). apiVersion: v1 kind: Service metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: selector: app: front-end ports: - protocol: TCP port: 8079 type: LoadBalancer","title":"LoadBalancer"},{"location":"advanced_services/#headless-services-with-endpointsexternal-names","text":"Sometimes you may to decouple the services from backend pods. Sometimes you may want to use some external services outside the cluster and want to integrate those external services with the cluster. For these kind of use cases we can use Headless services . Let us see an example. Let us consider a scenario where you have your redis cluster hosted outside the cluster. Your backend in cluster need to use this redis cluster. In this case, you will create a service with ExternalName, which is nothing but a CNAME record of your redis cluster. This type of services will neither have any ports defined nor have any selectors. But, how do you make sure the backend pods uses this service? For that, you will create a custom endpoint, in which map your service to specific endpoints. Ex: External Name service kind: Service apiVersion: v1 metadata: name: redis-aws namespace: instavote spec: type: ExternalName externalName: redis.aws.schoolofdevops.com Ex: Endpoints kind: Endpoints apiVersion: v1 metadata: name: redis-aws subsets: - addresses: - ip: 10.40.30.123 - ip: 10.40.30.324 - ip: 10.40.30.478 ports: - port: 9376 These IPs have to be manually managed by the cluster administrator.","title":"Headless services with Endpoints/External Names"},{"location":"advanced_services/#ingress","text":"Ingress controller is an Kubernetes object that manages external access to the backend services. This is the preferred way of exposing your services.","title":"Ingress"},{"location":"advanced_services/#ingress-controller","text":"To use an ingress resource, an ingress controller (ex. nginx, traefik) must be running. This controller has to be deployed manually. To create an ingress controller in our cluster, run the following commands, kubectl apply -f traefik-rbac.yaml kubectl apply -f traefik-deployment.yaml Check whether the ingress controller is running or not. kubectl get pods -n kube-system [...] kubernetes-dashboard-75c5ff6844-r9sk8 1/1 Running 6 3d nginx-proxy-node3 1/1 Running 8 27d nginx-proxy-node4 1/1 Running 7 27d traefik-ingress-controller-568dd77566-vk72b 1/1 Running 0 5m","title":"Ingress Controller"},{"location":"advanced_services/#purpose-of-an-ingress","text":"Ingress can be used for, * Load balancing, * SSL Termination, * Name based virtual hosting.","title":"Purpose of an Ingress"},{"location":"advanced_services/#types-of-ingress","text":"Let us see about different implementation of ingress. Keep or delete the following(Need to come up with use cases). ### Single Service Ingress ### Simple fanout Ingress ### Name based virtual hosting ### TLS ### Load Balancer","title":"Types of Ingress"},{"location":"advanced_services/#services-vs-ingress","text":"Service type LoadBalancer Service type Nodeport","title":"Services Vs Ingress"},{"location":"advanced_services/#creating-service-resources-for-applications-and-backing-services","text":"Need a demo set up for types: load balancer and external name Need input on this: Creating service resources for applications and backing services, Service API specs, Traffic Policy Can add some points about scaling of ingress controller (running it as a daemonset on all the nodes)","title":"Creating service resources for applications and backing services"},{"location":"cluster-administration/","text":"Kubernetes Cluster Administration Defining Quotas kubectl create namespace staging file: staging-quota.yaml apiVersion: v1 kind: ResourceQuota metadata: name: staging namespace: staging spec: hard: requests.cpu: \"0.5\" requests.memory: 500Mi limits.cpu: \"2\" limits.memory: 2Gi count/deployments.apps: 1 kubectl apply -f quota.yaml kubectl get quota -n staging kubectl describe quota -n staging file: nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: staging spec: replicas: 2 selector: matchLabels: app: web template: metadata: name: nginx labels: app: web spec: containers: - name: nginx image: nginx resources: limits: memory: \"500Mi\" cpu: \"500m\" requests: memory: \"200Mi\" cpu: \"200m\" kubectl apply -f nginx-deploy.yaml kubectl describe quota -n staging kubectl run dep2 --image=nginx -n staging Nodes Maintenance Cordon $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 5h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 42m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 1h 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 5h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 1h 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 50m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 50m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 50m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 39m 10.233.75.15 node2 $ kubectl cordon node4 node/node4 cordoned $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 5h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 43m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 1h 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 5h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 1h 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 51m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 51m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 51m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 40m 10.233.75.15 node2 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node4 node/node4 uncordoned $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 Drain a Node $ kubectl drain node3 node/node3 cordoned error: unable to drain node \"node3\", aborting command... There are pending nodes to be drained: node3 error: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node3 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 Drain with options kubectl drain node4 --ignore-daemonsets=true node/node4 cordoned WARNING: Ignoring DaemonSet-managed pods: calico-node-hnw87 pod/nginx-65899c769f-lphtq evicted pod/vote-56bf599b9c-22lpw evicted pod/vote-56bf599b9c-bqsrq evicted pod/vote-56bf599b9c-xw7zc evicted pod/nginx-65899c769f-kq9qb evicted pod/nginx-65899c769f-b59jq evicted pod/vote-56bf599b9c-4l6bc evicted pod/redis-5bf748dbcf-vxppx evicted pod/db-66496667c9-qggzd evicted pod/result-5c7569bcb7-s4rdx evicted $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 1m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 52m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 1m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 1m 10.233.102.142 node1 vote-56bf599b9c-fvcqt 1/1 Running 0 1m 10.233.71.31 node3 vote-56bf599b9c-k6s7q 1/1 Running 0 1m 10.233.71.30 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 1m 10.233.71.29 node3 vote-56bf599b9c-zz746 1/1 Running 0 1m 10.233.71.32 node3 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 49m 10.233.75.15 node2 $ kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-node-4f8xc 1/1 Running 2 1d 128.199.249.122 node3 calico-node-gbgxs 1/1 Running 2 1d 128.199.224.141 node1 calico-node-hnw87 1/1 Running 4 1d 128.199.248.156 node4 calico-node-tq46l 1/1 Running 0 39m 128.199.248.240 node2 kube-apiserver-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-apiserver-node2 1/1 Running 2 1d 128.199.248.240 node2 kube-controller-manager-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-controller-manager-node2 1/1 Running 1 1d 128.199.248.240 node2 kube-dns-6d6674c7c6-2gqhv 3/3 Running 0 22h 10.233.71.15 node3 kube-dns-6d6674c7c6-9d2zg 3/3 Running 0 22h 10.233.102.131 node1 kube-proxy-node1 1/1 Running 2 23h 128.199.224.141 node1 kube-proxy-node2 1/1 Running 2 1d 128.199.248.240 node2 kube-proxy-node3 1/1 Running 3 1d 128.199.249.122 node3 kube-proxy-node4 1/1 Running 2 1d 128.199.248.156 node4 kube-scheduler-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-scheduler-node2 1/1 Running 1 1d 128.199.248.240 node2 kubedns-autoscaler-679b8b455-tkntk 1/1 Running 2 1d 10.233.71.14 node3 kubernetes-dashboard-55fdfd74b4-jdgch 1/1 Running 4 1d 10.233.71.12 node3 nginx-proxy-node3 1/1 Running 3 1d 128.199.249.122 node3 nginx-proxy-node4 1/1 Running 2 1d 128.199.248.156 node4 tiller-deploy-5c688d5f9b-8hbpv 1/1 Running 0 22h 10.233.71.16 node3 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node4 node/node4 uncordoned $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 2m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 53m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 2m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 2m 10.233.102.142 node1 vote-56bf599b9c-fvcqt 1/1 Running 0 2m 10.233.71.31 node3 vote-56bf599b9c-k6s7q 1/1 Running 0 2m 10.233.71.30 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 2m 10.233.71.29 node3 vote-56bf599b9c-zz746 1/1 Running 0 2m 10.233.71.32 node3 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 50m 10.233.75.15 node2 $ kubectl delete pods vote-56bf599b9c-k6s7q vote-56bf599b9c-k6s7q vote-56bf599b9c-zz746 pod \"vote-56bf599b9c-k6s7q\" deleted pod \"vote-56bf599b9c-k6s7q\" deleted pod \"vote-56bf599b9c-zz746\" deleted $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 3m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 54m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 3m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 3m 10.233.102.142 node1 vote-56bf599b9c-dzgsf 1/1 Running 0 17s 10.233.74.86 node4 vote-56bf599b9c-fvcqt 1/1 Running 0 3m 10.233.71.31 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 3m 10.233.71.29 node3 vote-56bf599b9c-ptd29 1/1 Running 0 17s 10.233.74.85 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 51m 10.233.75.15 node2","title":"Cluster Administration"},{"location":"cluster-administration/#kubernetes-cluster-administration","text":"","title":"Kubernetes Cluster Administration"},{"location":"cluster-administration/#defining-quotas","text":"kubectl create namespace staging file: staging-quota.yaml apiVersion: v1 kind: ResourceQuota metadata: name: staging namespace: staging spec: hard: requests.cpu: \"0.5\" requests.memory: 500Mi limits.cpu: \"2\" limits.memory: 2Gi count/deployments.apps: 1 kubectl apply -f quota.yaml kubectl get quota -n staging kubectl describe quota -n staging file: nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: staging spec: replicas: 2 selector: matchLabels: app: web template: metadata: name: nginx labels: app: web spec: containers: - name: nginx image: nginx resources: limits: memory: \"500Mi\" cpu: \"500m\" requests: memory: \"200Mi\" cpu: \"200m\" kubectl apply -f nginx-deploy.yaml kubectl describe quota -n staging kubectl run dep2 --image=nginx -n staging","title":"Defining Quotas"},{"location":"cluster-administration/#nodes-maintenance","text":"Cordon $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 5h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 42m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 1h 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 5h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 1h 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 50m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 50m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 50m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 39m 10.233.75.15 node2 $ kubectl cordon node4 node/node4 cordoned $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-qggzd 1/1 Running 0 5h 10.233.74.74 node4 redis-5bf748dbcf-ckn65 1/1 Running 0 43m 10.233.71.26 node3 redis-5bf748dbcf-vxppx 1/1 Running 0 1h 10.233.74.79 node4 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-s4rdx 1/1 Running 0 5h 10.233.74.75 node4 vote-56bf599b9c-22lpw 1/1 Running 0 1h 10.233.74.80 node4 vote-56bf599b9c-4l6bc 1/1 Running 0 51m 10.233.74.83 node4 vote-56bf599b9c-bqsrq 1/1 Running 0 51m 10.233.74.82 node4 vote-56bf599b9c-xw7zc 1/1 Running 0 51m 10.233.74.81 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 0 40m 10.233.75.15 node2 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node4 node/node4 uncordoned $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2","title":"Nodes  Maintenance"},{"location":"cluster-administration/#drain-a-node","text":"$ kubectl drain node3 node/node3 cordoned error: unable to drain node \"node3\", aborting command... There are pending nodes to be drained: node3 error: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node3 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 Drain with options kubectl drain node4 --ignore-daemonsets=true node/node4 cordoned WARNING: Ignoring DaemonSet-managed pods: calico-node-hnw87 pod/nginx-65899c769f-lphtq evicted pod/vote-56bf599b9c-22lpw evicted pod/vote-56bf599b9c-bqsrq evicted pod/vote-56bf599b9c-xw7zc evicted pod/nginx-65899c769f-kq9qb evicted pod/nginx-65899c769f-b59jq evicted pod/vote-56bf599b9c-4l6bc evicted pod/redis-5bf748dbcf-vxppx evicted pod/db-66496667c9-qggzd evicted pod/result-5c7569bcb7-s4rdx evicted $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 1m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 52m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 1m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 1m 10.233.102.142 node1 vote-56bf599b9c-fvcqt 1/1 Running 0 1m 10.233.71.31 node3 vote-56bf599b9c-k6s7q 1/1 Running 0 1m 10.233.71.30 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 1m 10.233.71.29 node3 vote-56bf599b9c-zz746 1/1 Running 0 1m 10.233.71.32 node3 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 49m 10.233.75.15 node2 $ kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-node-4f8xc 1/1 Running 2 1d 128.199.249.122 node3 calico-node-gbgxs 1/1 Running 2 1d 128.199.224.141 node1 calico-node-hnw87 1/1 Running 4 1d 128.199.248.156 node4 calico-node-tq46l 1/1 Running 0 39m 128.199.248.240 node2 kube-apiserver-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-apiserver-node2 1/1 Running 2 1d 128.199.248.240 node2 kube-controller-manager-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-controller-manager-node2 1/1 Running 1 1d 128.199.248.240 node2 kube-dns-6d6674c7c6-2gqhv 3/3 Running 0 22h 10.233.71.15 node3 kube-dns-6d6674c7c6-9d2zg 3/3 Running 0 22h 10.233.102.131 node1 kube-proxy-node1 1/1 Running 2 23h 128.199.224.141 node1 kube-proxy-node2 1/1 Running 2 1d 128.199.248.240 node2 kube-proxy-node3 1/1 Running 3 1d 128.199.249.122 node3 kube-proxy-node4 1/1 Running 2 1d 128.199.248.156 node4 kube-scheduler-node1 1/1 Running 3 23h 128.199.224.141 node1 kube-scheduler-node2 1/1 Running 1 1d 128.199.248.240 node2 kubedns-autoscaler-679b8b455-tkntk 1/1 Running 2 1d 10.233.71.14 node3 kubernetes-dashboard-55fdfd74b4-jdgch 1/1 Running 4 1d 10.233.71.12 node3 nginx-proxy-node3 1/1 Running 3 1d 128.199.249.122 node3 nginx-proxy-node4 1/1 Running 2 1d 128.199.248.156 node4 tiller-deploy-5c688d5f9b-8hbpv 1/1 Running 0 22h 10.233.71.16 node3 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node2 Ready master,node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 node3 Ready node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-130-generic docker://17.3.2 node4 Ready,SchedulingDisabled node 1d v1.10.4 <none> Ubuntu 16.04.4 LTS 4.4.0-124-generic docker://17.3.2 $ kubectl uncordon node4 node/node4 uncordoned $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 2m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 53m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 2m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 2m 10.233.102.142 node1 vote-56bf599b9c-fvcqt 1/1 Running 0 2m 10.233.71.31 node3 vote-56bf599b9c-k6s7q 1/1 Running 0 2m 10.233.71.30 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 2m 10.233.71.29 node3 vote-56bf599b9c-zz746 1/1 Running 0 2m 10.233.71.32 node3 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 50m 10.233.75.15 node2 $ kubectl delete pods vote-56bf599b9c-k6s7q vote-56bf599b9c-k6s7q vote-56bf599b9c-zz746 pod \"vote-56bf599b9c-k6s7q\" deleted pod \"vote-56bf599b9c-k6s7q\" deleted pod \"vote-56bf599b9c-zz746\" deleted $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE db-66496667c9-wvrrg 1/1 Running 0 3m 10.233.75.18 node2 redis-5bf748dbcf-ckn65 1/1 Running 0 54m 10.233.71.26 node3 redis-5bf748dbcf-qbx2t 1/1 Running 0 3m 10.233.75.17 node2 result-5c7569bcb7-4fptr 1/1 Running 0 5h 10.233.71.18 node3 result-5c7569bcb7-h5222 1/1 Running 0 3m 10.233.102.142 node1 vote-56bf599b9c-dzgsf 1/1 Running 0 17s 10.233.74.86 node4 vote-56bf599b9c-fvcqt 1/1 Running 0 3m 10.233.71.31 node3 vote-56bf599b9c-kv9qp 1/1 Running 0 3m 10.233.71.29 node3 vote-56bf599b9c-ptd29 1/1 Running 0 17s 10.233.74.85 node4 worker-6cc8dbd4f8-6bkfg 1/1 Running 1 51m 10.233.75.15 node2","title":"Drain a Node"},{"location":"cluster_setup_kubespray/","text":"High Available Kubernetes Cluster Setup using Kubespray Kubespray is an Ansible based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster. Prerequisites Hardware Pre requisites 4 Nodes: Virtual/Physical Machines Memory: 2GB CPU: 1 Core Hard disk: 20GB available Software Pre Requisites On All Nodes Ubuntu 16.04 OS Python SSH Server Privileged user On Ansible Control Node Ansible version 2.4 or greater Jinja Networking Pre Requisites Internet access to download docker images and install softwares IPv4 Forwarding should be enabled Firewall should allow ssh access as well as ports required by Kubernetes. Internally open all the ports between node. Architecture of a high available kubernetes cluster Preparing the nodes Run instructions in the section On all nodes in the cluster. This includes Ansible controller too. Install Python Ansible needs python to be installed on all the machines. sudo apt update sudo apt install python Enable IPv4 Forwarding On all nodes Enalbe IPv4 forwarding by uncommenting the following line echo \"net.ipv4.ip_forward=1\" >> /etc/sysctl.conf Disable Swap swapoff -a Setup passwordless SSH between ansible controller and kubernetes nodes On control node Ansible uses passwordless ssh 1 to create the cluster. Let us see how to set it up from your control node . Generate ssh keypair if not present already using the following command. ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ubuntu/.ssh/id_rsa. Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub. The key fingerprint is: SHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1 The key's randomart image is: +---[RSA 2048]----+ | E .. | | . o +.. | | . +o*+o | |. .o+Bo+ | |. .++.X S | |+ +ooX . | |.=.OB.+ . | | .=o*= . . | | .o. . | +----[SHA256]-----+ Just leave the fields to defaults. This command will generate a public key and private key for you. Copy over the public key to all nodes. Example, assuming ubuntu as the user which has a privileged access on the node with ip address 10.10.1.101 , ssh-copy-id ubuntu@10.10.1.101 This will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace 10.40.1.26 with your respective machine's IP. e.g. ssh ubuntu@10.10.1.101 Make sure to copy the public key to all kubernetes nodes. Replace username with the actual user on your system . If the above mentioned command fails, then copy your public key and paste it in the remote machine's ~/.ssh/authorized_keys file. e.g. (Only if ssh-copy-id fails) cat ~/.ssh/id_rsa.pub ssh ubunut@10.10.1.101 vim ~/.ssh/authorized_keys # Paste the public key Setup Ansible Control node and Kubespray On control node Set Locale export LC_ALL=\"en_US.UTF-8\" export LC_CTYPE=\"en_US.UTF-8\" sudo dpkg-reconfigure locales Do no select any other locale in the menu. Just press ( OK ) in the next two screens. Setup kubespray Kubespray is hosted on GitHub. Let us the clone the official repository . git clone https://github.com/kubernetes-incubator/kubespray.git cd kubespray Install Prerequisites Install the python dependencies. This step installs Ansible as well. You do not need to install Ansible separately . sudo apt install python-pip -y sudo pip install -r requirements.txt Set Remote User for Ansible Add the following section in ansible.cfg file remote_user=ubuntu If the user you are going to connect is differnt, use that instead. Your ansible.cfg file should look like this. [ssh_connection] pipelining=True ssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null #control_path = ~/.ssh/ansible-%%r@%%h:%%p [defaults] host_key_checking=False gathering = smart fact_caching = jsonfile fact_caching_connection = /tmp stdout_callback = skippy library = ./library callback_whitelist = profile_tasks roles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles deprecation_warnings=False remote_user=ubuntu Create Inventory cp -rfp inventory/sample inventory/prod where prod is the custom configuration name. Replace is with whatever name you would like to assign to the current cluster. To build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments CONFIG_FILE=inventory/prod/hosts.ini python3 contrib/inventory_builder/inventory.py 10.10.1.101 10.10.1.102 10.10.1.103 10.10.1.104 Where replace the IP addresses (e.g. 10.10.1.101) with the actual IPs of your nodes Once its run, you should see an inventory file generated which may look similar to below file: inventory/prod/hosts.ini [all] node1 ansible_host=10.10.1.101 ip=10.10.1.101 node2 ansible_host=10.10.1.102 ip=10.10.1.102 node3 ansible_host=10.10.1.103 ip=10.10.1.103 node4 ansible_host=10.10.1.104 ip=10.10.1.104 [kube-master] node1 node2 [kube-node] node1 node2 node3 node4 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] [vault] node1 node2 node3 Customise Kubernetes Cluster Configs There are two configs files in your inventroy directory's group_vars (e.g. inventory/prod/group_vars) viz. all.yml k8s-cluster.yml Ansible is data driven, and most of the configurations of the cluster can be tweaked by changing the variable values from the above files. Few of the configurations you may want to modify file: inventory/prod/group_vars/k8s-cluster.yml kubelet_max_pods: 100 cluster_name: prod helm_enabled: true Provisioning kubernetes cluster with kubespray On control node We are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster. ansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml Option -i = Inventory file path Option -b = Become as root user Option -v = Give verbose output If you face this following error, while running ansible-playbook command, you can fix it by running following instructions ERROR : ERROR! Unexpected Exception, this is probably a bug: (cryptography 1.2.3 (/usr/lib/python3/dist-packages), Requirement.parse('cryptography>=1.5'), {'paramiko'}) FIX : sudo pip install --upgrade pip sudo pip uninstall cryptography sudo pip install cryptography ansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml This Ansible run will take around 30 mins to complete. Kubectl Configs On kube master node Once the cluster setup is done, copy the configuration and setup the permissions. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Check the State of the Cluster On the node where kubectl is setup Let us check the state of the cluster by running, kubectl cluster-info Kubernetes master is running at https://10.10.1.101:6443 KubeDNS is running at https://10.10.1.101:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready master,node 21h v1.9.0+coreos.0 node2 Ready master,node 21h v1.9.0+coreos.0 node3 Ready node 21h v1.9.0+coreos.0 node4 Ready node 21h v1.9.0+coreos.0 If you are able to see this, your cluster has been set up successfully. 1 You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible. Access Kubernetes Cluster Remotely (Optional) On your local machine You could also install kubectl on your laptop/workstation. To learn how to install it for your OS, refer to the procedure here . e.g. To install kubectl on Ubuntu, sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo touch /etc/apt/sources.list.d/kubernetes.list echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl Copy kubernetes config to your local machine Copy kubeconfig file to your local machine mkdir ~/.kube scp -r ubuntu@MASTER_HOST_IP:/etc/kubernetes/admin.conf ~/.kube/config kubectl get nodes Deploy Kubernetes Objects Since its a new cluster, which is differnt than what you have created with kubeadm earlier, or if this is the first time you are creating a kubernetes cluster with kubespray as part of Advanced Workshop , you need to deploy services which have been covered as part of the previous topics. In order to do that, use the following commands on the node where you have configured kubectl git clone https://github.com/schoolofdevops/k8s-code.git cd k8s-code/projects/instavote kubectl apply -f instavote-ns.yaml kubectl apply -f prod/ Switch to instavote namespace and validate, kubectl config set-context $(kubectl config current-context) --namespace=instavote kubectl get pods,deploy,svc where, --cluster=prod : prod is the cluter name you created earlier. If not, use the correct name of the cluster ( kubectl config view) --user=admin-prod: is the admin user created by default while installing with kubespray --namespace=instavote : the namespace you just created to deploy instavote app stack [sample output] $ kubectl get pods,deploy,svc NAME READY STATUS RESTARTS AGE pod/db-66496667c9-qggzd 1/1 Running 0 7m pod/redis-6555998885-4k5cr 1/1 Running 0 7m pod/redis-6555998885-fb8rk 1/1 Running 0 7m pod/result-5c7569bcb7-4fptr 1/1 Running 0 7m pod/result-5c7569bcb7-s4rdx 1/1 Running 0 7m pod/vote-5d88d47fc8-gbzbq 1/1 Running 0 7m pod/vote-5d88d47fc8-q4vj6 1/1 Running 0 7m pod/worker-7c98c96fb4-7tzzw 1/1 Running 0 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/db 1 1 1 1 7m deployment.extensions/redis 2 2 2 2 7m deployment.extensions/result 2 2 2 2 7m deployment.extensions/vote 2 2 2 2 7m deployment.extensions/worker 1 1 1 1 7m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/db ClusterIP 10.233.16.207 <none> 5432/TCP 7m service/redis ClusterIP 10.233.14.61 <none> 6379/TCP 7m service/result NodePort 10.233.22.10 <none> 80:30100/TCP 7m service/vote NodePort 10.233.19.111 <none> 80:30000/TCP 7m References Installing Kubernetes On Premises/On Cloud with Kubespray Kubespray on Github","title":"HA setup with Kubespray"},{"location":"cluster_setup_kubespray/#high-available-kubernetes-cluster-setup-using-kubespray","text":"Kubespray is an Ansible based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.","title":"High Available Kubernetes Cluster Setup using Kubespray"},{"location":"cluster_setup_kubespray/#prerequisites","text":"","title":"Prerequisites"},{"location":"cluster_setup_kubespray/#hardware-pre-requisites","text":"4 Nodes: Virtual/Physical Machines Memory: 2GB CPU: 1 Core Hard disk: 20GB available","title":"Hardware Pre requisites"},{"location":"cluster_setup_kubespray/#software-pre-requisites","text":"On All Nodes Ubuntu 16.04 OS Python SSH Server Privileged user On Ansible Control Node Ansible version 2.4 or greater Jinja","title":"Software Pre Requisites"},{"location":"cluster_setup_kubespray/#networking-pre-requisites","text":"Internet access to download docker images and install softwares IPv4 Forwarding should be enabled Firewall should allow ssh access as well as ports required by Kubernetes. Internally open all the ports between node.","title":"Networking Pre Requisites"},{"location":"cluster_setup_kubespray/#architecture-of-a-high-available-kubernetes-cluster","text":"","title":"Architecture of a high available kubernetes cluster"},{"location":"cluster_setup_kubespray/#preparing-the-nodes","text":"Run instructions in the section On all nodes in the cluster. This includes Ansible controller too.","title":"Preparing the nodes"},{"location":"cluster_setup_kubespray/#install-python","text":"Ansible needs python to be installed on all the machines. sudo apt update sudo apt install python","title":"Install Python"},{"location":"cluster_setup_kubespray/#enable-ipv4-forwarding","text":"On all nodes Enalbe IPv4 forwarding by uncommenting the following line echo \"net.ipv4.ip_forward=1\" >> /etc/sysctl.conf Disable Swap swapoff -a","title":"Enable IPv4 Forwarding"},{"location":"cluster_setup_kubespray/#setup-passwordless-ssh-between-ansible-controller-and-kubernetes-nodes","text":"On control node Ansible uses passwordless ssh 1 to create the cluster. Let us see how to set it up from your control node . Generate ssh keypair if not present already using the following command. ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ubuntu/.ssh/id_rsa. Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub. The key fingerprint is: SHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1 The key's randomart image is: +---[RSA 2048]----+ | E .. | | . o +.. | | . +o*+o | |. .o+Bo+ | |. .++.X S | |+ +ooX . | |.=.OB.+ . | | .=o*= . . | | .o. . | +----[SHA256]-----+ Just leave the fields to defaults. This command will generate a public key and private key for you. Copy over the public key to all nodes. Example, assuming ubuntu as the user which has a privileged access on the node with ip address 10.10.1.101 , ssh-copy-id ubuntu@10.10.1.101 This will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace 10.40.1.26 with your respective machine's IP. e.g. ssh ubuntu@10.10.1.101 Make sure to copy the public key to all kubernetes nodes. Replace username with the actual user on your system . If the above mentioned command fails, then copy your public key and paste it in the remote machine's ~/.ssh/authorized_keys file. e.g. (Only if ssh-copy-id fails) cat ~/.ssh/id_rsa.pub ssh ubunut@10.10.1.101 vim ~/.ssh/authorized_keys # Paste the public key","title":"Setup passwordless SSH between ansible controller and kubernetes nodes"},{"location":"cluster_setup_kubespray/#setup-ansible-control-node-and-kubespray","text":"On control node","title":"Setup Ansible Control node and Kubespray"},{"location":"cluster_setup_kubespray/#set-locale","text":"export LC_ALL=\"en_US.UTF-8\" export LC_CTYPE=\"en_US.UTF-8\" sudo dpkg-reconfigure locales Do no select any other locale in the menu. Just press ( OK ) in the next two screens.","title":"Set Locale"},{"location":"cluster_setup_kubespray/#setup-kubespray","text":"Kubespray is hosted on GitHub. Let us the clone the official repository . git clone https://github.com/kubernetes-incubator/kubespray.git cd kubespray","title":"Setup kubespray"},{"location":"cluster_setup_kubespray/#install-prerequisites","text":"Install the python dependencies. This step installs Ansible as well. You do not need to install Ansible separately . sudo apt install python-pip -y sudo pip install -r requirements.txt","title":"Install Prerequisites"},{"location":"cluster_setup_kubespray/#set-remote-user-for-ansible","text":"Add the following section in ansible.cfg file remote_user=ubuntu If the user you are going to connect is differnt, use that instead. Your ansible.cfg file should look like this. [ssh_connection] pipelining=True ssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null #control_path = ~/.ssh/ansible-%%r@%%h:%%p [defaults] host_key_checking=False gathering = smart fact_caching = jsonfile fact_caching_connection = /tmp stdout_callback = skippy library = ./library callback_whitelist = profile_tasks roles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles deprecation_warnings=False remote_user=ubuntu","title":"Set Remote User for Ansible"},{"location":"cluster_setup_kubespray/#create-inventory","text":"cp -rfp inventory/sample inventory/prod where prod is the custom configuration name. Replace is with whatever name you would like to assign to the current cluster. To build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments CONFIG_FILE=inventory/prod/hosts.ini python3 contrib/inventory_builder/inventory.py 10.10.1.101 10.10.1.102 10.10.1.103 10.10.1.104 Where replace the IP addresses (e.g. 10.10.1.101) with the actual IPs of your nodes Once its run, you should see an inventory file generated which may look similar to below file: inventory/prod/hosts.ini [all] node1 ansible_host=10.10.1.101 ip=10.10.1.101 node2 ansible_host=10.10.1.102 ip=10.10.1.102 node3 ansible_host=10.10.1.103 ip=10.10.1.103 node4 ansible_host=10.10.1.104 ip=10.10.1.104 [kube-master] node1 node2 [kube-node] node1 node2 node3 node4 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] [vault] node1 node2 node3","title":"Create Inventory"},{"location":"cluster_setup_kubespray/#customise-kubernetes-cluster-configs","text":"There are two configs files in your inventroy directory's group_vars (e.g. inventory/prod/group_vars) viz. all.yml k8s-cluster.yml Ansible is data driven, and most of the configurations of the cluster can be tweaked by changing the variable values from the above files. Few of the configurations you may want to modify file: inventory/prod/group_vars/k8s-cluster.yml kubelet_max_pods: 100 cluster_name: prod helm_enabled: true","title":"Customise Kubernetes Cluster Configs"},{"location":"cluster_setup_kubespray/#provisioning-kubernetes-cluster-with-kubespray","text":"On control node We are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster. ansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml Option -i = Inventory file path Option -b = Become as root user Option -v = Give verbose output If you face this following error, while running ansible-playbook command, you can fix it by running following instructions ERROR : ERROR! Unexpected Exception, this is probably a bug: (cryptography 1.2.3 (/usr/lib/python3/dist-packages), Requirement.parse('cryptography>=1.5'), {'paramiko'}) FIX : sudo pip install --upgrade pip sudo pip uninstall cryptography sudo pip install cryptography ansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml This Ansible run will take around 30 mins to complete.","title":"Provisioning  kubernetes cluster with kubespray"},{"location":"cluster_setup_kubespray/#kubectl-configs","text":"On kube master node Once the cluster setup is done, copy the configuration and setup the permissions. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Kubectl Configs"},{"location":"cluster_setup_kubespray/#check-the-state-of-the-cluster","text":"On the node where kubectl is setup Let us check the state of the cluster by running, kubectl cluster-info Kubernetes master is running at https://10.10.1.101:6443 KubeDNS is running at https://10.10.1.101:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready master,node 21h v1.9.0+coreos.0 node2 Ready master,node 21h v1.9.0+coreos.0 node3 Ready node 21h v1.9.0+coreos.0 node4 Ready node 21h v1.9.0+coreos.0 If you are able to see this, your cluster has been set up successfully. 1 You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.","title":"Check the State of the Cluster"},{"location":"cluster_setup_kubespray/#access-kubernetes-cluster-remotely-optional","text":"On your local machine You could also install kubectl on your laptop/workstation. To learn how to install it for your OS, refer to the procedure here . e.g. To install kubectl on Ubuntu, sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo touch /etc/apt/sources.list.d/kubernetes.list echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl","title":"Access Kubernetes Cluster Remotely (Optional)"},{"location":"cluster_setup_kubespray/#copy-kubernetes-config-to-your-local-machine","text":"Copy kubeconfig file to your local machine mkdir ~/.kube scp -r ubuntu@MASTER_HOST_IP:/etc/kubernetes/admin.conf ~/.kube/config kubectl get nodes","title":"Copy kubernetes config to your local machine"},{"location":"cluster_setup_kubespray/#deploy-kubernetes-objects","text":"Since its a new cluster, which is differnt than what you have created with kubeadm earlier, or if this is the first time you are creating a kubernetes cluster with kubespray as part of Advanced Workshop , you need to deploy services which have been covered as part of the previous topics. In order to do that, use the following commands on the node where you have configured kubectl git clone https://github.com/schoolofdevops/k8s-code.git cd k8s-code/projects/instavote kubectl apply -f instavote-ns.yaml kubectl apply -f prod/ Switch to instavote namespace and validate, kubectl config set-context $(kubectl config current-context) --namespace=instavote kubectl get pods,deploy,svc where, --cluster=prod : prod is the cluter name you created earlier. If not, use the correct name of the cluster ( kubectl config view) --user=admin-prod: is the admin user created by default while installing with kubespray --namespace=instavote : the namespace you just created to deploy instavote app stack [sample output] $ kubectl get pods,deploy,svc NAME READY STATUS RESTARTS AGE pod/db-66496667c9-qggzd 1/1 Running 0 7m pod/redis-6555998885-4k5cr 1/1 Running 0 7m pod/redis-6555998885-fb8rk 1/1 Running 0 7m pod/result-5c7569bcb7-4fptr 1/1 Running 0 7m pod/result-5c7569bcb7-s4rdx 1/1 Running 0 7m pod/vote-5d88d47fc8-gbzbq 1/1 Running 0 7m pod/vote-5d88d47fc8-q4vj6 1/1 Running 0 7m pod/worker-7c98c96fb4-7tzzw 1/1 Running 0 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/db 1 1 1 1 7m deployment.extensions/redis 2 2 2 2 7m deployment.extensions/result 2 2 2 2 7m deployment.extensions/vote 2 2 2 2 7m deployment.extensions/worker 1 1 1 1 7m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/db ClusterIP 10.233.16.207 <none> 5432/TCP 7m service/redis ClusterIP 10.233.14.61 <none> 6379/TCP 7m service/result NodePort 10.233.22.10 <none> 80:30100/TCP 7m service/vote NodePort 10.233.19.111 <none> 80:30000/TCP 7m","title":"Deploy Kubernetes Objects"},{"location":"cluster_setup_kubespray/#references","text":"Installing Kubernetes On Premises/On Cloud with Kubespray Kubespray on Github","title":"References"},{"location":"configuring_authentication_and_authorization/","text":"Kubernetes Access Control: Authentication and Authorization In this lab you are going to, Create users and groups and setup certs based authentication Create service accounts for applications Create Roles and ClusterRoles to define authorizations Map Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings. How one can access the Kubernetes API? The Kubernetes API can be accessed by three ways. Kubectl - A command line utility of Kubernetes Client libraries - Go, Python, etc., REST requests Who can access the Kubernetes API? Kubernetes API can be accessed by, Human Users Service Accounts Each of these topics will be discussed in detail in the later part of this chapter. Stages of a Request When a request tries to contact the API , it goes through various stages as illustrated in the image given below. source: official kubernetes site api groups and resources apiGroup Resources apps daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale core configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy autoscaling horizontalpodautoscalers batch cronjobs, jobs policy poddisruptionbudgets networking.k8s.io networkpolicies authorization.k8s.io localsubjectaccessreviews rbac.authorization.k8s.io rolebindings,roles extensions deprecated (read notes) Notes In addition to the above apiGroups, you may see extensions being used in some example code snippets. Please note that extensions was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above. You could read this comment and the thread to get clarity on this. Role Based Access Control (RBAC) Group User Namespaces Resources Access Type (verbs) ops maya all all get, list, watch, update, patch, create, delete, deletecollection dev kim instavote deployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims get, list , watch, update, patch, create interns yono instavote readonly get, list, watch Service Accounts Namespace Resources Access Type (verbs) monitoring all all readonly Creating Kubernetes Users and Groups Generate the user's private key mkdir -p ~/.kube/users cd ~/.kube/users openssl genrsa -out maya.key 2048 openssl genrsa -out kim.key 2048 openssl genrsa -out yono.key 2048 [sample Output] openssl genrsa -out maya.key 2048 Generating RSA private key, 2048 bit long modulus .............................................................+++ .........................+++ e is 65537 (0x10001) Lets now create a Certification Signing Request (CSR) for each of the users. When you generate the csr make sure you also provide CN: This will be set as username O: Org name. This is actually used as a group by kubernetes while authenticating/authorizing users. You could add as many as you need e.g. openssl req -new -key maya.key -out maya.csr -subj \"/CN=maya/O=ops/O=example.org\" openssl req -new -key kim.key -out kim.csr -subj \"/CN=kim/O=dev/O=example.org\" openssl req -new -key yono.key -out yono.csr -subj \"/CN=yono/O=interns/O=example.org\" In order to be deemed authentic, these CSRs need to be signed by the Certification Authority (CA) which in this case is Kubernetes Master. You need access to the folllwing files on kubernetes master. Certificate : ca.crt (kubeadm) or ca.key (kubespray) Pricate Key : ca.key (kubeadm) or ca-key.pem (kubespray) You would typically find it at one of the following paths /etc/kubernetes/pki (kubeadm) /etc/kubernetes/ssl (kubespray) To verify which one is your cert and which one is key, use the following command, $ file ca.pem ca.pem: PEM certificate $ file ca-key.pem ca-key.pem: PEM RSA private key Once signed, .csr files with added signatures become the certificates that could be used to authenticate. You could either move the crt files to k8s master, sign and download copy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure. In the example here, I have already downloaded ca.pem and ca-key.pem to my management workstation, which are used to sign the CSRs. Assuming all the files are in the same directory, sign the CSR as, openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in maya.csr -out maya.crt openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in kim.csr -out kim.crt openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in yono.csr -out yono.crt Setting up User configs with kubectl In order to configure the users that you created above, following steps need to be performed with kubectl Add credentials in the configurations Set context to login as a user to a cluster Switch context in order to assume the user's identity while working with the cluster to add credentials, kubectl config set-credentials maya --client-certificate=/absolute/path/to/maya.crt --client-key=/absolute/path/to/maya.key kubectl config set-credentials kim --client-certificate=/absolute/path/to/kim.crt --client-key=~/.kube/users/kim.key kubectl config set-credentials yono --client-certificate=/absolute/path/to/yono.crt --client-key=~/.kube/users/yono.key where, Replace /absolute/path/to/ with the path to these files. invalid : ~/.kube/users/yono.crt valid : /home/xyz/.kube/users/yono.crt And proceed to set/create contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find, kubectl config get-contexts [sample output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE admin-prod prod admin-cluster.local instavote admin-cluster4 cluster4 admin-cluster4 instavote * kubernetes-admin@kubernetes kubernetes kubernetes-admin instavote where, prod , cluster4 and kubernetes are cluster names. To set context for prod cluster, kubectl config set-context maya-prod --cluster=prod --user=maya --namespace=instavote kubectl config set-context kim-prod --cluster=prod --user=kim --namespace=instavote kubectl config set-context yono-prod --cluster=prod --user=yono --namespace=instavote Where, maya-prod : name of the context prod : name of the kubernetes cluster you set while creating it maya : user you created and configured above to connect to the cluster You could verify the configs with kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * admin-prod prod admin-prod kim-prod prod kim maya-prod prod maya yono-prod prod yono and kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://128.199.248.240:6443 name: prod contexts: - context: cluster: prod user: admin-prod name: admin-prod - context: cluster: prod user: kim name: kim-prod - context: cluster: prod user: maya name: maya-prod - context: cluster: prod user: yono name: yono-prod current-context: admin-prod kind: Config preferences: {} users: - name: admin-prod user: client-certificate-data: REDACTED client-key-data: REDACTED - name: maya user: client-certificate: users/~/.kube/users/maya.crt client-key: users/~/.kube/users/maya.key You could assume the identity of user yono and connect to the prod cluster as, kubectl config use-context yono-prod kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE admin-prod prod admin-prod kim-prod prod kim maya-prod prod maya * yono-prod prod yono And then try running any command as, kubectl get pods Alternately, if you are a admin user, you could impersonate a user and run a command with that literally using --as option kubectl config use-context admin-prod kubectl get pods --as yono [Sample Output] No resources found. Error from server (Forbidden): pods is forbidden: User \"yono\" cannot list pods in the namespace \"instavote\" Either ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section. Define authorisation rules with Roles and ClusterRoles Whats the difference between Roles and ClusterRoles ?? Role is limited to a namespace (Projects/Orgs/Env) ClusterRole is Global Lets say you want to provide read only access to instavote , a project specific namespace to all users in the example.org file: interns-role.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: namespace: instavote name: interns rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"get\", \"list\", \"watch\"] In order to map it to all users in example.org , create a RoleBinding as interns-rolebinding.yml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: interns namespace: instavote subjects: - kind: Group name: interns apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: interns apiGroup: rbac.authorization.k8s.io kubectl create -f interns-role.yml kubectl create -f interns-rolebinding.yml To gt information about the objects created above, kubectl get roles -n instavote kubectl get roles,rolebindings -n instavote kubectl describe role interns kubectl describe rolebinding interns To validate the access, kubectl config use-context yono-prod kubectl get pods To switch back to admin, kubectl config use-context admin-prod Exercise Create a Role and Rolebinding for dev group with the authorizations defined in the table above. Once applied, test it","title":"Authentication and Authorization (RBAC)"},{"location":"configuring_authentication_and_authorization/#kubernetes-access-control-authentication-and-authorization","text":"In this lab you are going to, Create users and groups and setup certs based authentication Create service accounts for applications Create Roles and ClusterRoles to define authorizations Map Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings.","title":"Kubernetes Access Control:  Authentication and Authorization"},{"location":"configuring_authentication_and_authorization/#how-one-can-access-the-kubernetes-api","text":"The Kubernetes API can be accessed by three ways. Kubectl - A command line utility of Kubernetes Client libraries - Go, Python, etc., REST requests","title":"How one can access the Kubernetes API?"},{"location":"configuring_authentication_and_authorization/#who-can-access-the-kubernetes-api","text":"Kubernetes API can be accessed by, Human Users Service Accounts Each of these topics will be discussed in detail in the later part of this chapter.","title":"Who can access the Kubernetes API?"},{"location":"configuring_authentication_and_authorization/#stages-of-a-request","text":"When a request tries to contact the API , it goes through various stages as illustrated in the image given below. source: official kubernetes site","title":"Stages of a Request"},{"location":"configuring_authentication_and_authorization/#api-groups-and-resources","text":"apiGroup Resources apps daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale core configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy autoscaling horizontalpodautoscalers batch cronjobs, jobs policy poddisruptionbudgets networking.k8s.io networkpolicies authorization.k8s.io localsubjectaccessreviews rbac.authorization.k8s.io rolebindings,roles extensions deprecated (read notes)","title":"api groups and resources"},{"location":"configuring_authentication_and_authorization/#notes","text":"In addition to the above apiGroups, you may see extensions being used in some example code snippets. Please note that extensions was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above. You could read this comment and the thread to get clarity on this.","title":"Notes"},{"location":"configuring_authentication_and_authorization/#role-based-access-control-rbac","text":"Group User Namespaces Resources Access Type (verbs) ops maya all all get, list, watch, update, patch, create, delete, deletecollection dev kim instavote deployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims get, list , watch, update, patch, create interns yono instavote readonly get, list, watch Service Accounts Namespace Resources Access Type (verbs) monitoring all all readonly","title":"Role Based Access Control (RBAC)"},{"location":"configuring_authentication_and_authorization/#creating-kubernetes-users-and-groups","text":"Generate the user's private key mkdir -p ~/.kube/users cd ~/.kube/users openssl genrsa -out maya.key 2048 openssl genrsa -out kim.key 2048 openssl genrsa -out yono.key 2048 [sample Output] openssl genrsa -out maya.key 2048 Generating RSA private key, 2048 bit long modulus .............................................................+++ .........................+++ e is 65537 (0x10001) Lets now create a Certification Signing Request (CSR) for each of the users. When you generate the csr make sure you also provide CN: This will be set as username O: Org name. This is actually used as a group by kubernetes while authenticating/authorizing users. You could add as many as you need e.g. openssl req -new -key maya.key -out maya.csr -subj \"/CN=maya/O=ops/O=example.org\" openssl req -new -key kim.key -out kim.csr -subj \"/CN=kim/O=dev/O=example.org\" openssl req -new -key yono.key -out yono.csr -subj \"/CN=yono/O=interns/O=example.org\" In order to be deemed authentic, these CSRs need to be signed by the Certification Authority (CA) which in this case is Kubernetes Master. You need access to the folllwing files on kubernetes master. Certificate : ca.crt (kubeadm) or ca.key (kubespray) Pricate Key : ca.key (kubeadm) or ca-key.pem (kubespray) You would typically find it at one of the following paths /etc/kubernetes/pki (kubeadm) /etc/kubernetes/ssl (kubespray) To verify which one is your cert and which one is key, use the following command, $ file ca.pem ca.pem: PEM certificate $ file ca-key.pem ca-key.pem: PEM RSA private key Once signed, .csr files with added signatures become the certificates that could be used to authenticate. You could either move the crt files to k8s master, sign and download copy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure. In the example here, I have already downloaded ca.pem and ca-key.pem to my management workstation, which are used to sign the CSRs. Assuming all the files are in the same directory, sign the CSR as, openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in maya.csr -out maya.crt openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in kim.csr -out kim.crt openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in yono.csr -out yono.crt","title":"Creating Kubernetes Users and Groups"},{"location":"configuring_authentication_and_authorization/#setting-up-user-configs-with-kubectl","text":"In order to configure the users that you created above, following steps need to be performed with kubectl Add credentials in the configurations Set context to login as a user to a cluster Switch context in order to assume the user's identity while working with the cluster to add credentials, kubectl config set-credentials maya --client-certificate=/absolute/path/to/maya.crt --client-key=/absolute/path/to/maya.key kubectl config set-credentials kim --client-certificate=/absolute/path/to/kim.crt --client-key=~/.kube/users/kim.key kubectl config set-credentials yono --client-certificate=/absolute/path/to/yono.crt --client-key=~/.kube/users/yono.key where, Replace /absolute/path/to/ with the path to these files. invalid : ~/.kube/users/yono.crt valid : /home/xyz/.kube/users/yono.crt And proceed to set/create contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find, kubectl config get-contexts [sample output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE admin-prod prod admin-cluster.local instavote admin-cluster4 cluster4 admin-cluster4 instavote * kubernetes-admin@kubernetes kubernetes kubernetes-admin instavote where, prod , cluster4 and kubernetes are cluster names. To set context for prod cluster, kubectl config set-context maya-prod --cluster=prod --user=maya --namespace=instavote kubectl config set-context kim-prod --cluster=prod --user=kim --namespace=instavote kubectl config set-context yono-prod --cluster=prod --user=yono --namespace=instavote Where, maya-prod : name of the context prod : name of the kubernetes cluster you set while creating it maya : user you created and configured above to connect to the cluster You could verify the configs with kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * admin-prod prod admin-prod kim-prod prod kim maya-prod prod maya yono-prod prod yono and kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://128.199.248.240:6443 name: prod contexts: - context: cluster: prod user: admin-prod name: admin-prod - context: cluster: prod user: kim name: kim-prod - context: cluster: prod user: maya name: maya-prod - context: cluster: prod user: yono name: yono-prod current-context: admin-prod kind: Config preferences: {} users: - name: admin-prod user: client-certificate-data: REDACTED client-key-data: REDACTED - name: maya user: client-certificate: users/~/.kube/users/maya.crt client-key: users/~/.kube/users/maya.key You could assume the identity of user yono and connect to the prod cluster as, kubectl config use-context yono-prod kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE admin-prod prod admin-prod kim-prod prod kim maya-prod prod maya * yono-prod prod yono And then try running any command as, kubectl get pods Alternately, if you are a admin user, you could impersonate a user and run a command with that literally using --as option kubectl config use-context admin-prod kubectl get pods --as yono [Sample Output] No resources found. Error from server (Forbidden): pods is forbidden: User \"yono\" cannot list pods in the namespace \"instavote\" Either ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section.","title":"Setting up User configs with kubectl"},{"location":"configuring_authentication_and_authorization/#define-authorisation-rules-with-roles-and-clusterroles","text":"Whats the difference between Roles and ClusterRoles ?? Role is limited to a namespace (Projects/Orgs/Env) ClusterRole is Global Lets say you want to provide read only access to instavote , a project specific namespace to all users in the example.org file: interns-role.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: namespace: instavote name: interns rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"get\", \"list\", \"watch\"] In order to map it to all users in example.org , create a RoleBinding as interns-rolebinding.yml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: interns namespace: instavote subjects: - kind: Group name: interns apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: interns apiGroup: rbac.authorization.k8s.io kubectl create -f interns-role.yml kubectl create -f interns-rolebinding.yml To gt information about the objects created above, kubectl get roles -n instavote kubectl get roles,rolebindings -n instavote kubectl describe role interns kubectl describe rolebinding interns To validate the access, kubectl config use-context yono-prod kubectl get pods To switch back to admin, kubectl config use-context admin-prod","title":"Define authorisation rules with Roles and ClusterRoles"},{"location":"configuring_authentication_and_authorization/#exercise","text":"Create a Role and Rolebinding for dev group with the authorizations defined in the table above. Once applied, test it","title":"Exercise"},{"location":"elk_monitoring/","text":"","title":"Elk monitoring"},{"location":"helm/","text":"Helm Package Manager Install Helm To install helm you can follow following instructions. curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh chmod 700 get_helm.sh ./get_helm.sh Verify the installtion is successful, helm --help Set RBAC for Tiller file: tiller-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system Apply the ClusterRole and ClusterRoleBinding. kubectl apply -f tiller-rbac.yaml Initialize This is where we actually initialize Tiller in our Kubernetes cluster. helm init --service-account tiller [sample output] Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! Install Wordpress with Helm Search Helm repository for Wordpress chart helm search wordpress Fetch the chart to your local environment and change directory. helm fetch --untar stable/wordpress cd wordpress Create a copy of default vaules file and edit it. cp values.yaml my-values.yaml vim my-values.yaml Run it as a dry run to check for errors. helm install --name blog --values my-values.yaml . --dry-run Deploy the Wordpress stack. helm install --name blog --values my-values.yaml . Install Prometheus with Helm Official Prometheus Helm Chart repository. https://github.com/helm/charts/tree/master/stable/prometheus Official Grafana Helm Chart repository. https://github.com/helm/charts/tree/master/stable/grafana Grafana Deployment Download Grafana charts to your local machine and change directory. helm fetch --untar stable/grafana cd grafana Create a copy of default vaules file and edit it. cp values.yaml myvalues.yaml vim myvalues.yaml Make sure your charts doesn't have any error. helm install --name grafana --values myvalues.yaml --namespace instavote . --dry-run Deploy Grafana on your K8s Cluster. helm install --name grafana --values myvalues.yaml --namespace instavote . Prometheus Deployment Download Prometheus charts to your local machine and change directory. helm fetch --untar stable/prometheus cd prometheus Create a copy of default vaules file and edit it. cp values.yaml myvalues.yaml vim myvalues.yaml Make sure your charts doesn't have any error. helm install --name prometheus --values myvalues.yaml --namespace instavote . --dry-run Deploy Prometheus on your K8s Cluster. helm install --name prometheus --values myvalues.yaml --namespace instavote . Install heapster with helm helm install stable/heapster --namespace kube-system --name heapster --set image.tag=v1.5.1 --set rbac.create=true [output] NAME: heapster LAST DEPLOYED: Tue May 22 11:46:44 2018 NAMESPACE: kube-system STATUS: DEPLOYED RESOURCES: ==> v1beta1/Role NAME AGE heapster-heapster-pod-nanny 2s ==> v1beta1/RoleBinding NAME AGE heapster-heapster-pod-nanny 2s ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE heapster ClusterIP 10.96.63.82 <none> 8082/TCP 2s ==> v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE heapster-heapster 1 0 0 0 2s ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE heapster-heapster-696df57b44-zjf78 0/2 Pending 0 1s ==> v1/ServiceAccount NAME SECRETS AGE heapster-heapster 1 2s ==> v1beta1/ClusterRoleBinding NAME AGE heapster-heapster 2s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace kube-system -l \"app=heapster-heapster\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl --namespace kube-system port-forward $POD_NAME 8082","title":"Helm Charts"},{"location":"helm/#helm-package-manager","text":"","title":"Helm Package Manager"},{"location":"helm/#install-helm","text":"To install helm you can follow following instructions. curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh chmod 700 get_helm.sh ./get_helm.sh Verify the installtion is successful, helm --help","title":"Install Helm"},{"location":"helm/#set-rbac-for-tiller","text":"file: tiller-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system Apply the ClusterRole and ClusterRoleBinding. kubectl apply -f tiller-rbac.yaml","title":"Set RBAC for Tiller"},{"location":"helm/#initialize","text":"This is where we actually initialize Tiller in our Kubernetes cluster. helm init --service-account tiller [sample output] Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming!","title":"Initialize"},{"location":"helm/#install-wordpress-with-helm","text":"Search Helm repository for Wordpress chart helm search wordpress Fetch the chart to your local environment and change directory. helm fetch --untar stable/wordpress cd wordpress Create a copy of default vaules file and edit it. cp values.yaml my-values.yaml vim my-values.yaml Run it as a dry run to check for errors. helm install --name blog --values my-values.yaml . --dry-run Deploy the Wordpress stack. helm install --name blog --values my-values.yaml .","title":"Install Wordpress with Helm"},{"location":"helm/#install-prometheus-with-helm","text":"Official Prometheus Helm Chart repository. https://github.com/helm/charts/tree/master/stable/prometheus Official Grafana Helm Chart repository. https://github.com/helm/charts/tree/master/stable/grafana","title":"Install Prometheus with Helm"},{"location":"helm/#grafana-deployment","text":"Download Grafana charts to your local machine and change directory. helm fetch --untar stable/grafana cd grafana Create a copy of default vaules file and edit it. cp values.yaml myvalues.yaml vim myvalues.yaml Make sure your charts doesn't have any error. helm install --name grafana --values myvalues.yaml --namespace instavote . --dry-run Deploy Grafana on your K8s Cluster. helm install --name grafana --values myvalues.yaml --namespace instavote .","title":"Grafana Deployment"},{"location":"helm/#prometheus-deployment","text":"Download Prometheus charts to your local machine and change directory. helm fetch --untar stable/prometheus cd prometheus Create a copy of default vaules file and edit it. cp values.yaml myvalues.yaml vim myvalues.yaml Make sure your charts doesn't have any error. helm install --name prometheus --values myvalues.yaml --namespace instavote . --dry-run Deploy Prometheus on your K8s Cluster. helm install --name prometheus --values myvalues.yaml --namespace instavote .","title":"Prometheus Deployment"},{"location":"helm/#install-heapster-with-helm","text":"helm install stable/heapster --namespace kube-system --name heapster --set image.tag=v1.5.1 --set rbac.create=true [output] NAME: heapster LAST DEPLOYED: Tue May 22 11:46:44 2018 NAMESPACE: kube-system STATUS: DEPLOYED RESOURCES: ==> v1beta1/Role NAME AGE heapster-heapster-pod-nanny 2s ==> v1beta1/RoleBinding NAME AGE heapster-heapster-pod-nanny 2s ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE heapster ClusterIP 10.96.63.82 <none> 8082/TCP 2s ==> v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE heapster-heapster 1 0 0 0 2s ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE heapster-heapster-696df57b44-zjf78 0/2 Pending 0 1s ==> v1/ServiceAccount NAME SECRETS AGE heapster-heapster 1 2s ==> v1beta1/ClusterRoleBinding NAME AGE heapster-heapster 2s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace kube-system -l \"app=heapster-heapster\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl --namespace kube-system port-forward $POD_NAME 8082","title":"Install heapster with helm"},{"location":"ingress/","text":"Ingress Pre Requisites Ingress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources. On GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress. Daemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service. Create a Ingress Controller An ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for GCE and nginx ingress controllers, however additional softwares are commonly used too. As part of this implementation you are going to use Traefik as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support. +----+----+--+ | ingress | | controller | +----+-------+ There are commonly two ways you could deploy an ingress Using Deployments with HA setup Using DaemonSets which run on every node We pick DaemonSet , which will ensure that one instance of traefik is run on every node. Also, we use a specific configuration hostNetwork so that the pod running traefik attaches to the network of underlying host, and not go through kube-proxy . This would avoid extra network hop and increase performance a bit. Deploy ingress controller with daemonset as cd k8s-code/ingress/traefik kubectl get ds -n kube-system kubectl apply -f traefik-rbac.yaml kubectl apply -f traefik-ds.yaml Validate kubectl get svc,ds,pods -n kube-system --selector='k8s-app=traefik-ingress-lb' [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik-ingress-service ClusterIP 10.109.182.203 <none> 80/TCP,8080/TCP 11h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.extensions/traefik-ingress-controller 2 2 2 2 2 <none> 11h NAME READY STATUS RESTARTS AGE pod/traefik-ingress-controller-bmwn7 1/1 Running 0 11h pod/traefik-ingress-controller-vl296 1/1 Running 0 11h You would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see traefik's management UI. Setting up Named Based Routing for Vote App We will direct all our request to the ingress controller now, but with differnt hostname e.g. vote.example.org or results.example.org . And it should direct to the correct service based on the host name. In order to achieve this you, as a user would create a ingress object with a set of rules, +----+----+--+ | ingress | | controller | +----+-------+ | +-----+----+ +---watch----> | ingress | <------- user +----------+ file: vote-ing.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: vote annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: \"basic\" ingress.kubernetes.io/auth-secret: \"mysecret\" spec: rules: - host: vote.example.org http: paths: - path: / backend: serviceName: vote servicePort: 82 - host: results.example.org http: paths: - path: / backend: serviceName: results servicePort: 81 And apply kubectl get ing kubectl apply -f vote-ing.yaml --dry-run kubectl apply -f vote-ing.yaml Since the ingress controller is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows. +----------+ +--create----> | traefik | | | rules | | +----------+ +----+----+--+ ^ | ingress | : | controller | : +----+-------+ : | +-----+----+ +---watch----> | ingress | <------- user +----------+ where, A user creates a ingress object with the rules. This could be a named based or a path based routing. An ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs. You could now see the rule added to ingress controller, Where, vote.example.org and results.example.org are added as frontends. These frontends point to respective services vote and results . respective backends also appear on the right hand side of the screen, mapping to each of the service. Adding Local DNS You have created the ingress rules based on hostnames e.g. vote.example.org and results.example.org . In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik. vote.example.org -------+ +----- vote:81 | +-------------+ | | | ingress | | +===> | node:80 | ===+ | +-------------+ | | | results.example.org -------+ +----- results:82 To achieve this you need to either, Create a DNS entry, provided you own the domain and have access to the dns management console. Create a local hosts file entry. On unix systems its in /etc/hosts file. On windows its at C:\\Windows\\System32\\drivers\\etc\\hosts . You need admin access to edit this file. For example, on a linux or osx, you could edit it as, sudo vim /etc/hosts And add an entry such as , xxx.xxx.xxx.xxx vote.example.org results.example.org where, xxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik. And then access the app urls using http://vote.example.org or http://results.example.org Adding HTTP Authentication with Annotations Creating htpasswd spec as Secret htpasswd -c auth devops Or use Online htpasswd generator to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name auth in the current directory. Then generate the secret as, kubectl create secret generic mysecret --from-file auth kubectl get secret kubectl describe secret mysecret And then add annotations to the ingress object so that it is read by the ingress controller to update configurations. file: vote-ing.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: vote annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: \"basic\" ingress.kubernetes.io/auth-secret: \"mysecret\" spec: rules: - host: vote.example.org http: paths: - path: / backend: serviceName: vote servicePort: 82 - host: results.example.org http: paths: - path: / backend: serviceName: results servicePort: 81 where, ingress.kubernetes.io/auth-type: \"basic\" defines authentication type that needs to be added. ingress.kubernetes.io/auth-secret: \"mysecret\" refers to the secret created earlier. apply kubectl apply -f vote-ing.yaml kubectl get ing/vote -o yaml Observe the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added. +----------+ +--update----> | traefik | | | configs | | +----------+ +----+----+--+ ^ | ingress | : | controller | : +----+-------+ : | +-----+-------+ +---watch----> | ingress | <------- user | annotations | +-------------+ And if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below. Reading Trafeik's Guide to Kubernetes Ingress Controller Annotations DaemonSets References Online htpasswd generator Keywords trafeik on kubernetes kubernetes ingress kubernetes annotations daemonsets","title":"Application Routing with Ingress Controllers"},{"location":"ingress/#ingress","text":"","title":"Ingress"},{"location":"ingress/#pre-requisites","text":"Ingress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources. On GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress. Daemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service.","title":"Pre Requisites"},{"location":"ingress/#create-a-ingress-controller","text":"An ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for GCE and nginx ingress controllers, however additional softwares are commonly used too. As part of this implementation you are going to use Traefik as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support. +----+----+--+ | ingress | | controller | +----+-------+ There are commonly two ways you could deploy an ingress Using Deployments with HA setup Using DaemonSets which run on every node We pick DaemonSet , which will ensure that one instance of traefik is run on every node. Also, we use a specific configuration hostNetwork so that the pod running traefik attaches to the network of underlying host, and not go through kube-proxy . This would avoid extra network hop and increase performance a bit. Deploy ingress controller with daemonset as cd k8s-code/ingress/traefik kubectl get ds -n kube-system kubectl apply -f traefik-rbac.yaml kubectl apply -f traefik-ds.yaml Validate kubectl get svc,ds,pods -n kube-system --selector='k8s-app=traefik-ingress-lb' [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik-ingress-service ClusterIP 10.109.182.203 <none> 80/TCP,8080/TCP 11h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.extensions/traefik-ingress-controller 2 2 2 2 2 <none> 11h NAME READY STATUS RESTARTS AGE pod/traefik-ingress-controller-bmwn7 1/1 Running 0 11h pod/traefik-ingress-controller-vl296 1/1 Running 0 11h You would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see traefik's management UI.","title":"Create a Ingress Controller"},{"location":"ingress/#setting-up-named-based-routing-for-vote-app","text":"We will direct all our request to the ingress controller now, but with differnt hostname e.g. vote.example.org or results.example.org . And it should direct to the correct service based on the host name. In order to achieve this you, as a user would create a ingress object with a set of rules, +----+----+--+ | ingress | | controller | +----+-------+ | +-----+----+ +---watch----> | ingress | <------- user +----------+ file: vote-ing.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: vote annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: \"basic\" ingress.kubernetes.io/auth-secret: \"mysecret\" spec: rules: - host: vote.example.org http: paths: - path: / backend: serviceName: vote servicePort: 82 - host: results.example.org http: paths: - path: / backend: serviceName: results servicePort: 81 And apply kubectl get ing kubectl apply -f vote-ing.yaml --dry-run kubectl apply -f vote-ing.yaml Since the ingress controller is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows. +----------+ +--create----> | traefik | | | rules | | +----------+ +----+----+--+ ^ | ingress | : | controller | : +----+-------+ : | +-----+----+ +---watch----> | ingress | <------- user +----------+ where, A user creates a ingress object with the rules. This could be a named based or a path based routing. An ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs. You could now see the rule added to ingress controller, Where, vote.example.org and results.example.org are added as frontends. These frontends point to respective services vote and results . respective backends also appear on the right hand side of the screen, mapping to each of the service.","title":"Setting up Named Based Routing for Vote App"},{"location":"ingress/#adding-local-dns","text":"You have created the ingress rules based on hostnames e.g. vote.example.org and results.example.org . In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik. vote.example.org -------+ +----- vote:81 | +-------------+ | | | ingress | | +===> | node:80 | ===+ | +-------------+ | | | results.example.org -------+ +----- results:82 To achieve this you need to either, Create a DNS entry, provided you own the domain and have access to the dns management console. Create a local hosts file entry. On unix systems its in /etc/hosts file. On windows its at C:\\Windows\\System32\\drivers\\etc\\hosts . You need admin access to edit this file. For example, on a linux or osx, you could edit it as, sudo vim /etc/hosts And add an entry such as , xxx.xxx.xxx.xxx vote.example.org results.example.org where, xxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik. And then access the app urls using http://vote.example.org or http://results.example.org","title":"Adding Local DNS"},{"location":"ingress/#adding-http-authentication-with-annotations","text":"","title":"Adding HTTP Authentication with Annotations"},{"location":"ingress/#creating-htpasswd-spec-as-secret","text":"htpasswd -c auth devops Or use Online htpasswd generator to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name auth in the current directory. Then generate the secret as, kubectl create secret generic mysecret --from-file auth kubectl get secret kubectl describe secret mysecret And then add annotations to the ingress object so that it is read by the ingress controller to update configurations. file: vote-ing.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: vote annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: \"basic\" ingress.kubernetes.io/auth-secret: \"mysecret\" spec: rules: - host: vote.example.org http: paths: - path: / backend: serviceName: vote servicePort: 82 - host: results.example.org http: paths: - path: / backend: serviceName: results servicePort: 81 where, ingress.kubernetes.io/auth-type: \"basic\" defines authentication type that needs to be added. ingress.kubernetes.io/auth-secret: \"mysecret\" refers to the secret created earlier. apply kubectl apply -f vote-ing.yaml kubectl get ing/vote -o yaml Observe the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added. +----------+ +--update----> | traefik | | | configs | | +----------+ +----+----+--+ ^ | ingress | : | controller | : +----+-------+ : | +-----+-------+ +---watch----> | ingress | <------- user | annotations | +-------------+ And if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below.","title":"Creating htpasswd spec as Secret"},{"location":"ingress/#reading","text":"Trafeik's Guide to Kubernetes Ingress Controller Annotations DaemonSets References Online htpasswd generator Keywords trafeik on kubernetes kubernetes ingress kubernetes annotations daemonsets","title":"Reading"},{"location":"kube_visualizer/","text":"Kubernetes Visualizer In this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time. Set up Fork the repository and deploy the visualizer on kubernetes git clone https://github.com/schoolofdevops/kube-ops-view kubectl apply -f kube-ops-view/deploy/ [Sample Output] serviceaccount \"kube-ops-view\" created clusterrole \"kube-ops-view\" created clusterrolebinding \"kube-ops-view\" created deployment \"kube-ops-view\" created ingress \"kube-ops-view\" created deployment \"kube-ops-view-redis\" created service \"kube-ops-view-redis\" created service \"kube-ops-view\" created Get the nodeport for the service. kubectl get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-ops-view NodePort 10.107.204.74 <none> 80:**30073**/TCP 1m kube-ops-view-redis ClusterIP 10.104.50.176 <none> 6379/TCP 1m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 8m In my case, port 30073 is the nodeport. Visit the port from the browser. You could add /#scale=2.0 or similar option where 2.0 = 200% the scale. http://<NODE_IP:NODE_PORT>/#scale=2.0","title":"Kuberentes Visualizer"},{"location":"kube_visualizer/#kubernetes-visualizer","text":"In this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time.","title":"Kubernetes Visualizer"},{"location":"kube_visualizer/#set-up","text":"Fork the repository and deploy the visualizer on kubernetes git clone https://github.com/schoolofdevops/kube-ops-view kubectl apply -f kube-ops-view/deploy/ [Sample Output] serviceaccount \"kube-ops-view\" created clusterrole \"kube-ops-view\" created clusterrolebinding \"kube-ops-view\" created deployment \"kube-ops-view\" created ingress \"kube-ops-view\" created deployment \"kube-ops-view-redis\" created service \"kube-ops-view-redis\" created service \"kube-ops-view\" created Get the nodeport for the service. kubectl get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-ops-view NodePort 10.107.204.74 <none> 80:**30073**/TCP 1m kube-ops-view-redis ClusterIP 10.104.50.176 <none> 6379/TCP 1m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 8m In my case, port 30073 is the nodeport. Visit the port from the browser. You could add /#scale=2.0 or similar option where 2.0 = 200% the scale. http://<NODE_IP:NODE_PORT>/#scale=2.0","title":"Set up"},{"location":"kubespray-prereqs/","text":"Provisioning HA Lab Cluster with Vagrant Vagrant Setup: This tutorial assumes you have Vagrant+VirtualBox setup. While Vagrant is used for basic infrastructure requirements, the lessons learned in this tutorial can be applied to other platforms. Start from Set up Kubernetes Using Kubespray (or) Refer to this Document , if you have VMs running elsewhere Software Requirements on Host Machine: Virtual Box (latest) Vagrant (latest) Git Bash (Only for Windows) Conemu (Only for Windows) Set up Learning Environment: Setup the repo git clone https://github.com/schoolofdevops/kubespray-1 Bring up the VMs cd kubespray-1 vagrant up vagrant status Login to nodes Open four different terminals to login to 4 nodes created with above command Terminal 1 vagrant ssh k8s-01 sudo su Terminal 2 vagrant ssh k8s-02 sudo su Terminal 3 vagrant ssh k8s-03 sudo su Terminal 4 vagrant ssh k8s-04 sudo su Once the environment is setup, follow Production Grade Setup with Kubespray","title":"Kubespray HA lab setup with Vagrant"},{"location":"kubespray-prereqs/#provisioning-ha-lab-cluster-with-vagrant","text":"","title":"Provisioning HA Lab Cluster  with Vagrant"},{"location":"kubespray-prereqs/#vagrant-setup","text":"This tutorial assumes you have Vagrant+VirtualBox setup. While Vagrant is used for basic infrastructure requirements, the lessons learned in this tutorial can be applied to other platforms. Start from Set up Kubernetes Using Kubespray (or) Refer to this Document , if you have VMs running elsewhere","title":"Vagrant Setup:"},{"location":"kubespray-prereqs/#software-requirements-on-host-machine","text":"Virtual Box (latest) Vagrant (latest) Git Bash (Only for Windows) Conemu (Only for Windows)","title":"Software Requirements on Host Machine:"},{"location":"kubespray-prereqs/#set-up-learning-environment","text":"Setup the repo git clone https://github.com/schoolofdevops/kubespray-1 Bring up the VMs cd kubespray-1 vagrant up vagrant status Login to nodes Open four different terminals to login to 4 nodes created with above command Terminal 1 vagrant ssh k8s-01 sudo su Terminal 2 vagrant ssh k8s-02 sudo su Terminal 3 vagrant ssh k8s-03 sudo su Terminal 4 vagrant ssh k8s-04 sudo su Once the environment is setup, follow Production Grade Setup with Kubespray","title":"Set up Learning Environment:"},{"location":"logging/","text":"Logging References Logging Arcchitecture","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#references","text":"Logging Arcchitecture","title":"References"},{"location":"minikube/","text":"Single node k8s cluster with Minikube Minikube offers one of the easiest zero to dev experience to setup a single node kubernetes cluster. Its also the ideal way to create a local dev environment to test kubernetes code on. This document explains how to setup and work with single node kubernetes cluster with minikube. Install Minikube Instructions to install minikube may vary based on the operating system and choice of the hypervisor. This is the official document which explains how to install minikube. Start all in one single node cluster with minikube minikube status [output] minikube: cluster: kubectl: minikube start [output] Starting local Kubernetes v1.8.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. minikube status [output] minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 Launch a kubernetes dashboard minikube dashboard Setting up docker environment minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH=\"/Users/gouravshah/.minikube/certs\" export DOCKER_API_VERSION=\"1.23\" # Run this command to configure your shell: # eval $(minikube docker-env) Run the command given above, e.g. eval $(minikube docker-env) Now your docker client should be able to connect with the minikube cluster docker ps Additional Commands minikube ip minikube get-k8s-versions minikube logs","title":"Using minikube to setup single node environment"},{"location":"minikube/#single-node-k8s-cluster-with-minikube","text":"Minikube offers one of the easiest zero to dev experience to setup a single node kubernetes cluster. Its also the ideal way to create a local dev environment to test kubernetes code on. This document explains how to setup and work with single node kubernetes cluster with minikube.","title":"Single node k8s cluster with Minikube"},{"location":"minikube/#install-minikube","text":"Instructions to install minikube may vary based on the operating system and choice of the hypervisor. This is the official document which explains how to install minikube.","title":"Install Minikube"},{"location":"minikube/#start-all-in-one-single-node-cluster-with-minikube","text":"minikube status [output] minikube: cluster: kubectl: minikube start [output] Starting local Kubernetes v1.8.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. minikube status [output] minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100","title":"Start all in one single node cluster with minikube"},{"location":"minikube/#launch-a-kubernetes-dashboard","text":"minikube dashboard","title":"Launch a kubernetes dashboard"},{"location":"minikube/#setting-up-docker-environment","text":"minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH=\"/Users/gouravshah/.minikube/certs\" export DOCKER_API_VERSION=\"1.23\" # Run this command to configure your shell: # eval $(minikube docker-env) Run the command given above, e.g. eval $(minikube docker-env) Now your docker client should be able to connect with the minikube cluster docker ps","title":"Setting up docker environment"},{"location":"minikube/#additional-commands","text":"minikube ip minikube get-k8s-versions minikube logs","title":"Additional Commands"},{"location":"network_policies/","text":"Setting up a firewall with Network Policies While setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets update the namespace with a label. file: instavote-ns.yaml kind: Namespace apiVersion: v1 metadata: name: instavote labels: project: instavote apply kubectl get namespace --show-labels kubectl apply -f instavote-ns.yaml kubectl get namespace --show-labels Now, define a restrictive network policy which would, Block all incoming connections from any source except for pods from the same namespace Block all outgoing connections +-----------------------------------------------------------+ | | | +----------+ +-----------+ | x | | results | | db | | | | | | | | | +----------+ +-----------+ | | | | | | +----+----+--+ | | | worker | | | | | | | +----+-------+ | | | | | | +----------+ +-----------+ | | | vote | | redis | | x | | | | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress apply kubectl get netpol kubectl apply -f instavote-netpol.yaml kubectl get netpol kubectl describe netpol/default-deny Try accessing the vote and results ui. Can you access it ? Setting up ingress rules for outward facing applications +-----------------------------------------------------------+ | | | +----------+ +-----------+ | =====> | results | | db | | | | | | | | | +----------+ +-----------+ | | | | | | +----+----+--+ | | | worker | | | | | | | +----+-------+ | | | | | | +----------+ +-----------+ | | | vote | | redis | | =====> | | | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ To the same file, add a new network policy object. file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: public-ingress namespace: instavote spec: podSelector: matchExpressions: - {key: role, operator: In, values: [vote, results]} policyTypes: - Ingress ingress: - {} where, instavote-ingress is a new network policy which, defines policy for pods with vote and results role and allows them incoming access from anywhere apply kubectl apply -f instavote-netpol.yaml Exercise Try accessing the ui now and check if you are able to. Try to vote, see if that works? Why ? Setting up egress rules to allow communication between services from same project When you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the environment, however you still need to provide inter connection between services from the same project. Specifically vote , worker and results apps need outgoing connection to redis and db . Lets allow that with a egress policy. +-----------------------------------------------------------+ | | | +------------+ +-----------+ | =====> | results | ------>| db | | | | | | | <-------+ | | +------------+ +-----------+ | | | | | | | | | +----+----+---+ | | | worker | | | | | | | +----+--------+ | | | | | | | | +----------+ +-----------+ | | | | vote | | redis | <-------+ | =====> | | ------> | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ Edit the same policy file and add the following snippet, file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: project: instavote egress: - to: - namespaceSelector: matchLabels: project: instavote --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: public-ingress namespace: instavote spec: podSelector: matchExpressions: - {key: role, operator: In, values: [vote, results]} policyTypes: - Ingress ingress: - {} where, instavote-egress is a new network policy which, defines policy for pods with vote , worker and results role and allows them outgoing access to any pods in the same namespace, and that includes redis and db Project The above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application. Create network policies with following specs, vote allow incoming connections from anywhere, only on port 80 allow outgoing connections to redis block everything else, incoming and outgoing redis allow incoming connections from vote and worker , only on port 6379 block everything else, incoming and outgoing worker allow outgoing connections to redis and db block everything else, incoming and outgoing db allow incoming connections from worker and results , only on port 5342 block everything else, incoming and outgoing results allow incoming connections from anywhere, only on port 80 allow outgoing connections to db block everything else, incoming and outgoing","title":"Access Control with Network Policies"},{"location":"network_policies/#setting-up-a-firewall-with-network-policies","text":"While setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets update the namespace with a label. file: instavote-ns.yaml kind: Namespace apiVersion: v1 metadata: name: instavote labels: project: instavote apply kubectl get namespace --show-labels kubectl apply -f instavote-ns.yaml kubectl get namespace --show-labels Now, define a restrictive network policy which would, Block all incoming connections from any source except for pods from the same namespace Block all outgoing connections +-----------------------------------------------------------+ | | | +----------+ +-----------+ | x | | results | | db | | | | | | | | | +----------+ +-----------+ | | | | | | +----+----+--+ | | | worker | | | | | | | +----+-------+ | | | | | | +----------+ +-----------+ | | | vote | | redis | | x | | | | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress apply kubectl get netpol kubectl apply -f instavote-netpol.yaml kubectl get netpol kubectl describe netpol/default-deny Try accessing the vote and results ui. Can you access it ?","title":"Setting up a firewall with Network Policies"},{"location":"network_policies/#setting-up-ingress-rules-for-outward-facing-applications","text":"+-----------------------------------------------------------+ | | | +----------+ +-----------+ | =====> | results | | db | | | | | | | | | +----------+ +-----------+ | | | | | | +----+----+--+ | | | worker | | | | | | | +----+-------+ | | | | | | +----------+ +-----------+ | | | vote | | redis | | =====> | | | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ To the same file, add a new network policy object. file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: public-ingress namespace: instavote spec: podSelector: matchExpressions: - {key: role, operator: In, values: [vote, results]} policyTypes: - Ingress ingress: - {} where, instavote-ingress is a new network policy which, defines policy for pods with vote and results role and allows them incoming access from anywhere apply kubectl apply -f instavote-netpol.yaml Exercise Try accessing the ui now and check if you are able to. Try to vote, see if that works? Why ?","title":"Setting up ingress rules for outward facing applications"},{"location":"network_policies/#setting-up-egress-rules-to-allow-communication-between-services-from-same-project","text":"When you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the environment, however you still need to provide inter connection between services from the same project. Specifically vote , worker and results apps need outgoing connection to redis and db . Lets allow that with a egress policy. +-----------------------------------------------------------+ | | | +------------+ +-----------+ | =====> | results | ------>| db | | | | | | | <-------+ | | +------------+ +-----------+ | | | | | | | | | +----+----+---+ | | | worker | | | | | | | +----+--------+ | | | | | | | | +----------+ +-----------+ | | | | vote | | redis | <-------+ | =====> | | ------> | | | | +----------+ +-----------+ | | | +-----------------------------------------------------------+ Edit the same policy file and add the following snippet, file: instavote-netpol.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: project: instavote egress: - to: - namespaceSelector: matchLabels: project: instavote --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: public-ingress namespace: instavote spec: podSelector: matchExpressions: - {key: role, operator: In, values: [vote, results]} policyTypes: - Ingress ingress: - {} where, instavote-egress is a new network policy which, defines policy for pods with vote , worker and results role and allows them outgoing access to any pods in the same namespace, and that includes redis and db","title":"Setting up egress rules to allow communication between services from same project"},{"location":"network_policies/#project","text":"The above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application. Create network policies with following specs, vote allow incoming connections from anywhere, only on port 80 allow outgoing connections to redis block everything else, incoming and outgoing redis allow incoming connections from vote and worker , only on port 6379 block everything else, incoming and outgoing worker allow outgoing connections to redis and db block everything else, incoming and outgoing db allow incoming connections from worker and results , only on port 5342 block everything else, incoming and outgoing results allow incoming connections from anywhere, only on port 80 allow outgoing connections to db block everything else, incoming and outgoing","title":"Project"},{"location":"pod-adv-specs/","text":"Additional Pod Specs - Resources, Security Specs Resource requests and limits We can control the amount of resource requested and used by all the pods. This can be done by adding following data the deployment template. Resource Request File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 resources: requests: memory: \"128Mi\" cpu: \"250m\" This ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use resource limit to limit the resource usage by a pod. Expected output: kubectl describe pod front-end-5c64b7c5cc-cwgr5 [...] Containers: front-end: Container ID: Image: schoolofdevops/frontend Image ID: Port: 8079/TCP State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Requests: cpu: 250m memory: 128Mi Resource limit File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 resources: requests: memory: \"128Mi\" cpu: \"250m\" limits: memory: \"256Mi\" cpu: \"500m\" Expected output: kubectl describe pod front-end-5b877b4dff-5twdd [...] Containers: front-end: Container ID: docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554 Image: schoolofdevops/frontend Image ID: docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50 Port: 8079/TCP State: Running Started: Thu, 08 Feb 2018 17:14:54 +0530 Ready: True Restart Count: 0 Limits: cpu: 500m memory: 256Mi Requests: cpu: 250m memory: 128Mi","title":"Pod Resource and Security Specs"},{"location":"pod-adv-specs/#additional-pod-specs-resources-security-specs","text":"","title":"Additional  Pod Specs  - Resources, Security Specs"},{"location":"pod-adv-specs/#resource-requests-and-limits","text":"We can control the amount of resource requested and used by all the pods. This can be done by adding following data the deployment template.","title":"Resource requests and limits"},{"location":"pod-adv-specs/#resource-request","text":"File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 resources: requests: memory: \"128Mi\" cpu: \"250m\" This ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use resource limit to limit the resource usage by a pod. Expected output: kubectl describe pod front-end-5c64b7c5cc-cwgr5 [...] Containers: front-end: Container ID: Image: schoolofdevops/frontend Image ID: Port: 8079/TCP State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Requests: cpu: 250m memory: 128Mi","title":"Resource Request"},{"location":"pod-adv-specs/#resource-limit","text":"File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 resources: requests: memory: \"128Mi\" cpu: \"250m\" limits: memory: \"256Mi\" cpu: \"500m\" Expected output: kubectl describe pod front-end-5b877b4dff-5twdd [...] Containers: front-end: Container ID: docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554 Image: schoolofdevops/frontend Image ID: docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50 Port: 8079/TCP State: Running Started: Thu, 08 Feb 2018 17:14:54 +0530 Ready: True Restart Count: 0 Limits: cpu: 500m memory: 256Mi Requests: cpu: 250m memory: 128Mi","title":"Resource limit"},{"location":"pod_security/","text":"Content to this chapter will be added in future. For a tutorial on this chapter, refer to the following page https://github.com/kubernetes/examples/tree/master/staging/podsecuritypolicy/rbac","title":"Pod security"},{"location":"pods-health-probes/","text":"Checking health with Probes Adding health checks Health checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks, * Liveness Probe * Readiness Probe Probes are simply a diagnostic action performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely, * ExecAction : Executes a command inside the pod. Assumed successful when the command returns 0 as exit code. * TCPSocketAction : Checks for a state of a particular port on the pod. Considered successful when the state of the port is open . * HTTPGetAction : Performs a GET request on pod's IP. Assumed successful when the status code is greater than 200 and less than 400 In cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice. Liveness Probe Liveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is Success . Let us add liveness probe to our frontend deployment. The following probe will check whether it is able to access the port or not . File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 Expected output: kubectl apply -f front-end/frontend-deploy.yml kubectl get pods kubectl describe pod front-end-757db58546-fkgdw [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 22s default-scheduler Successfully assigned front-end-757db58546-fkgdw to node4 Normal SuccessfulMountVolume 22s kubelet, node4 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 20s kubelet, node4 pulling image \"schoolofdevops/frontend\" Normal Pulled 17s kubelet, node4 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 17s kubelet, node4 Created container Normal Started 17s kubelet, node4 Started container Let us change the livenessProbe check port to 8080. livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 5 Apply this deployment file and check the description of the pod Expected output: kubectl apply -f frontend-deploy.yml kubectl get pods kubectl describe pod front-end-bf86ffd8b-bjb7p [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned front-end-bf86ffd8b-bjb7p to node3 Normal SuccessfulMountVolume 1m kubelet, node3 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 38s (x2 over 1m) kubelet, node3 pulling image \"schoolofdevops/frontend\" Normal Killing 38s kubelet, node3 Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated. Normal Pulled 35s (x2 over 1m) kubelet, node3 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 35s (x2 over 1m) kubelet, node3 Created container Normal Started 35s (x2 over 1m) kubelet, node3 Started container Warning Unhealthy 27s (x5 over 1m) kubelet, node3 Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused Readiness Probe Readiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is Success . Readiness probe is configured just like liveness probe. But this time we will use httpGet request . File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 Expected output: kubectl apply -f front-end/frontend-deploy.yml kubectl get pods kubectl describe pod front-end-c5bc89b57-g42nc [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 11s default-scheduler Successfully assigned front-end-c5bc89b57-g42nc to node4 Normal SuccessfulMountVolume 10s kubelet, node4 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 8s kubelet, node4 pulling image \"schoolofdevops/frontend\" Normal Pulled 6s kubelet, node4 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 5s kubelet, node4 Created container Normal Started 5s kubelet, node4 Started container Task : Change the readinessProbe port to 8080 and check what happens to the pod.","title":"Adding health checks with Probes"},{"location":"pods-health-probes/#checking-health-with-probes","text":"","title":"Checking health with Probes"},{"location":"pods-health-probes/#adding-health-checks","text":"Health checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks, * Liveness Probe * Readiness Probe Probes are simply a diagnostic action performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely, * ExecAction : Executes a command inside the pod. Assumed successful when the command returns 0 as exit code. * TCPSocketAction : Checks for a state of a particular port on the pod. Considered successful when the state of the port is open . * HTTPGetAction : Performs a GET request on pod's IP. Assumed successful when the status code is greater than 200 and less than 400 In cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice.","title":"Adding health checks"},{"location":"pods-health-probes/#liveness-probe","text":"Liveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is Success . Let us add liveness probe to our frontend deployment. The following probe will check whether it is able to access the port or not . File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 Expected output: kubectl apply -f front-end/frontend-deploy.yml kubectl get pods kubectl describe pod front-end-757db58546-fkgdw [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 22s default-scheduler Successfully assigned front-end-757db58546-fkgdw to node4 Normal SuccessfulMountVolume 22s kubelet, node4 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 20s kubelet, node4 pulling image \"schoolofdevops/frontend\" Normal Pulled 17s kubelet, node4 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 17s kubelet, node4 Created container Normal Started 17s kubelet, node4 Started container Let us change the livenessProbe check port to 8080. livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 5 Apply this deployment file and check the description of the pod Expected output: kubectl apply -f frontend-deploy.yml kubectl get pods kubectl describe pod front-end-bf86ffd8b-bjb7p [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned front-end-bf86ffd8b-bjb7p to node3 Normal SuccessfulMountVolume 1m kubelet, node3 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 38s (x2 over 1m) kubelet, node3 pulling image \"schoolofdevops/frontend\" Normal Killing 38s kubelet, node3 Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated. Normal Pulled 35s (x2 over 1m) kubelet, node3 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 35s (x2 over 1m) kubelet, node3 Created container Normal Started 35s (x2 over 1m) kubelet, node3 Started container Warning Unhealthy 27s (x5 over 1m) kubelet, node3 Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused","title":"Liveness Probe"},{"location":"pods-health-probes/#readiness-probe","text":"Readiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is Success . Readiness probe is configured just like liveness probe. But this time we will use httpGet request . File: code/frontend-deploy.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: front-end namespace: instavote labels: app: front-end env: dev spec: replicas: 1 template: metadata: labels: app: front-end env: dev spec: containers: - name: front-end image: schoolofdevops/frontend imagePullPolicy: Always ports: - containerPort: 8079 livenessProbe: tcpSocket: port: 8079 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: / port: 8079 initialDelaySeconds: 5 periodSeconds: 3 Expected output: kubectl apply -f front-end/frontend-deploy.yml kubectl get pods kubectl describe pod front-end-c5bc89b57-g42nc [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 11s default-scheduler Successfully assigned front-end-c5bc89b57-g42nc to node4 Normal SuccessfulMountVolume 10s kubelet, node4 MountVolume.SetUp succeeded for volume \"default-token-w4279\" Normal Pulling 8s kubelet, node4 pulling image \"schoolofdevops/frontend\" Normal Pulled 6s kubelet, node4 Successfully pulled image \"schoolofdevops/frontend\" Normal Created 5s kubelet, node4 Created container Normal Started 5s kubelet, node4 Started container Task : Change the readinessProbe port to 8080 and check what happens to the pod.","title":"Readiness Probe"},{"location":"quickdive/","text":"Kubernetes Quick Dive Launch vote application with kubernetes. (simiar to docker run command) kubectl run vote --image=schoolofdevops/vote kubectl get pods Publish the application (similar to using -P for port mapping) kubectl expose deployment vote --type=NodePort --port 80 kubectl get svc Scale the vote app to run 4 instances. kubectl scale --replicas=4 deployment/vote kubectl get pods Connect to the app, refresh the page to see it load balancing. Also try to vote and observe what happens. Setup additional apps Now lets launch rest of the apps. kubectl run redis --image=redis:alpine kubectl expose deployment redis --port 6379 kubectl run worker --image=schoolofdevops/worker kubectl run db --image=postgres:9.4 kubectl expose deployment db --port 5432 kubectl run result --image=schoolofdevops/vote-result kubectl expose deployment result --type=NodePort --port 80 Cleaing up Once you are done observing, you could delete it with the following commands, kubectl delete deploy db redis vote worker result kubectl delete svc db redis results vote","title":"Kubernetes Quickdive"},{"location":"quickdive/#kubernetes-quick-dive","text":"Launch vote application with kubernetes. (simiar to docker run command) kubectl run vote --image=schoolofdevops/vote kubectl get pods Publish the application (similar to using -P for port mapping) kubectl expose deployment vote --type=NodePort --port 80 kubectl get svc Scale the vote app to run 4 instances. kubectl scale --replicas=4 deployment/vote kubectl get pods Connect to the app, refresh the page to see it load balancing. Also try to vote and observe what happens.","title":"Kubernetes Quick Dive"},{"location":"quickdive/#setup-additional-apps","text":"Now lets launch rest of the apps. kubectl run redis --image=redis:alpine kubectl expose deployment redis --port 6379 kubectl run worker --image=schoolofdevops/worker kubectl run db --image=postgres:9.4 kubectl expose deployment db --port 5432 kubectl run result --image=schoolofdevops/vote-result kubectl expose deployment result --type=NodePort --port 80","title":"Setup additional apps"},{"location":"quickdive/#cleaing-up","text":"Once you are done observing, you could delete it with the following commands, kubectl delete deploy db redis vote worker result kubectl delete svc db redis results vote","title":"Cleaing up"},{"location":"rbac-resource-group-mapping/","text":"RBAC Reference kubernetes Instances Configuration GCP NUMBER OF NODE-SIZE INSTANCE TYPE CPU MEMORY 1-5 n1-standard-1 1 6-10 n1-standard-2 2 11-100 n1-standard-4 4 101-250 n1-standard-8 8 251-500 n1-standard-16 16 more than 500 n1-standard-32 32 AWS NUMBER OF NODE_SIZE INSTANCE TYPE CPU MEMORY 1-5 m3.medium 1 3.75 6-10 m3.large 2 7.50 11-100 m3.xlarge 4 15 101-250 m3.2xlarge 8 30 251-500 c4.4xlarge 8 30 more than 500 c4.8xlarge 16 60 api groups and resources apiGroup Resources apps daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale core configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy autoscaling horizontalpodautoscalers batch cronjobs, jobs policy poddisruptionbudgets networking.k8s.io networkpolicies authorization.k8s.io localsubjectaccessreviews rbac.authorization.k8s.io rolebindings,roles extensions deprecated (read notes) Notes In addition to the above apiGroups, you may see extensions being used in some example code snippets. Please note that extensions was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above. You could read this comment and the thread to get clarity on this.","title":"RBAC apiGroups to Resource Mapping"},{"location":"rbac-resource-group-mapping/#rbac-reference","text":"","title":"RBAC Reference"},{"location":"rbac-resource-group-mapping/#kubernetes-instances-configuration","text":"","title":"kubernetes Instances Configuration"},{"location":"rbac-resource-group-mapping/#gcp","text":"NUMBER OF NODE-SIZE INSTANCE TYPE CPU MEMORY 1-5 n1-standard-1 1 6-10 n1-standard-2 2 11-100 n1-standard-4 4 101-250 n1-standard-8 8 251-500 n1-standard-16 16 more than 500 n1-standard-32 32","title":"GCP"},{"location":"rbac-resource-group-mapping/#aws","text":"NUMBER OF NODE_SIZE INSTANCE TYPE CPU MEMORY 1-5 m3.medium 1 3.75 6-10 m3.large 2 7.50 11-100 m3.xlarge 4 15 101-250 m3.2xlarge 8 30 251-500 c4.4xlarge 8 30 more than 500 c4.8xlarge 16 60","title":"AWS"},{"location":"rbac-resource-group-mapping/#api-groups-and-resources","text":"apiGroup Resources apps daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale core configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy autoscaling horizontalpodautoscalers batch cronjobs, jobs policy poddisruptionbudgets networking.k8s.io networkpolicies authorization.k8s.io localsubjectaccessreviews rbac.authorization.k8s.io rolebindings,roles extensions deprecated (read notes)","title":"api groups and resources"},{"location":"rbac-resource-group-mapping/#notes","text":"In addition to the above apiGroups, you may see extensions being used in some example code snippets. Please note that extensions was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above. You could read this comment and the thread to get clarity on this.","title":"Notes"},{"location":"replication/","text":"Making application high available with Replication Controllers If you are not running a monitoring screen, start it in a new terminal with the following command. watch -n 1 kubectl get pod,deploy,rs,svc kubectl delete pod vote Setting up a Namespace Check current config kubectl config view You could also examine the current configs in file cat ~/.kube/config Creating a namespace Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod. Lets create a namespace called instavote cd projects/instavote cat instavote-ns.yaml [output] kind: Namespace apiVersion: v1 metadata: name: instavote Lets create a namespace kubectl get ns kubectl apply -f instavote-ns.yaml kubectl get ns And switch to it kubectl config --help kubectl config get-contexts kubectl config current-context kubectl config set-context $(kubectl config current-context) --namespace=instavote kubectl config view kubectl config get-contexts Exercise : Go back to the monitoring screen and observe what happens after switching the namespace. To understand how ReplicaSets works with the selectors lets launch a pod in the new namespace with existing specs. cd k8s-code/pods kubectl apply -f vote-pod.yaml kubectl get pods cd ../projects/instavote/dev/ Lets now write the spec for the Rplica Set. This is going to mainly contain, replicas selector template (pod spec ) minReadySeconds file: vote-rs.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: vote spec: replicas: 5 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: Lets now add the metadata and spec from pod spec defined in vote-pod.yaml. And with that, the Replica Set Spec changes to file: vote-rs.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: vote spec: replicas: 5 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP Replica Sets in Action kubectl apply -f vote-rs.yaml --dry-run kubectl apply -f vote-rs.yaml kubectl get rs kubectl describe rs vote kubectl get pods Exercise : Switch to monitoring screen, observe how many replicas were created and why Compare selectors and labels of the pods created with and without replica sets kubectl get pods kubectl get pods --show-labels Exercise: Deploying new version of the application kubectl edit rs/vote Update the version of the image from schoolofdevops/vote:v1 to schoolofdevops/vote:v2 Save the file. Observe if application got updated. Note what do you observe. Do you see the new version deployed ?? Exercise: Self Healing Replica Sets List the pods and kill some of those, see what replica set does. kubectl get pods kubectl delete pods vote-xxxx vote-yyyy where replace xxxx and yyyy with actual pod ids. Questions: Did replica set replaced the pods ? Which version of the application is running now ? Lets now delete the pod created independent of replica set. kubectl get pods kubectl delete pods vote Observe what happens. * Does replica set take any action after deleting the pod created outside of its spec ? Why?","title":"Making application highly available"},{"location":"replication/#making-application-high-available-with-replication-controllers","text":"If you are not running a monitoring screen, start it in a new terminal with the following command. watch -n 1 kubectl get pod,deploy,rs,svc kubectl delete pod vote","title":"Making application high available with Replication Controllers"},{"location":"replication/#setting-up-a-namespace","text":"Check current config kubectl config view You could also examine the current configs in file cat ~/.kube/config","title":"Setting up a Namespace"},{"location":"replication/#creating-a-namespace","text":"Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod. Lets create a namespace called instavote cd projects/instavote cat instavote-ns.yaml [output] kind: Namespace apiVersion: v1 metadata: name: instavote Lets create a namespace kubectl get ns kubectl apply -f instavote-ns.yaml kubectl get ns And switch to it kubectl config --help kubectl config get-contexts kubectl config current-context kubectl config set-context $(kubectl config current-context) --namespace=instavote kubectl config view kubectl config get-contexts Exercise : Go back to the monitoring screen and observe what happens after switching the namespace. To understand how ReplicaSets works with the selectors lets launch a pod in the new namespace with existing specs. cd k8s-code/pods kubectl apply -f vote-pod.yaml kubectl get pods cd ../projects/instavote/dev/ Lets now write the spec for the Rplica Set. This is going to mainly contain, replicas selector template (pod spec ) minReadySeconds file: vote-rs.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: vote spec: replicas: 5 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: Lets now add the metadata and spec from pod spec defined in vote-pod.yaml. And with that, the Replica Set Spec changes to file: vote-rs.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: vote spec: replicas: 5 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP","title":"Creating a namespace"},{"location":"replication/#replica-sets-in-action","text":"kubectl apply -f vote-rs.yaml --dry-run kubectl apply -f vote-rs.yaml kubectl get rs kubectl describe rs vote kubectl get pods Exercise : Switch to monitoring screen, observe how many replicas were created and why Compare selectors and labels of the pods created with and without replica sets kubectl get pods kubectl get pods --show-labels","title":"Replica Sets in Action"},{"location":"replication/#exercise-deploying-new-version-of-the-application","text":"kubectl edit rs/vote Update the version of the image from schoolofdevops/vote:v1 to schoolofdevops/vote:v2 Save the file. Observe if application got updated. Note what do you observe. Do you see the new version deployed ??","title":"Exercise: Deploying new version of the application"},{"location":"replication/#exercise-self-healing-replica-sets","text":"List the pods and kill some of those, see what replica set does. kubectl get pods kubectl delete pods vote-xxxx vote-yyyy where replace xxxx and yyyy with actual pod ids. Questions: Did replica set replaced the pods ? Which version of the application is running now ? Lets now delete the pod created independent of replica set. kubectl get pods kubectl delete pods vote Observe what happens. * Does replica set take any action after deleting the pod created outside of its spec ? Why?","title":"Exercise: Self Healing Replica Sets"},{"location":"run_and_publish_image/","text":"Quck Deploy a Docker Image with Kubernetes kubectl run vote --image=schoolofdevops/vote --port 80 kubectl scale --replicas=10 deploy/vote kubectl expose deploy vote --port 80 --target-port 80 --type NodePort","title":"Quck Deploy a Docker Image with Kubernetes"},{"location":"run_and_publish_image/#quck-deploy-a-docker-image-with-kubernetes","text":"kubectl run vote --image=schoolofdevops/vote --port 80 kubectl scale --replicas=10 deploy/vote kubectl expose deploy vote --port 80 --target-port 80 --type NodePort","title":"Quck Deploy a Docker Image with Kubernetes"},{"location":"statefulset/","text":"Running Replicated Redis Setup with statefulsets What will you learn * Statefulsets * initContainers Reading List https://redis.io/topics/replication https://discuss.pivotal.io/hc/en-us/articles/205309278-How-to-setup-Redis-master-and-slave-replication https://www.digitalocean.com/community/tutorials/how-to-configure-redis-replication-on-ubuntu-16-04 Run Replicated Statefulsets Applications Keywords init containers kubernetes statefulsets redis replication","title":"Running Replicated Redis Setup with statefulsets"},{"location":"statefulset/#running-replicated-redis-setup-with-statefulsets","text":"What will you learn * Statefulsets * initContainers","title":"Running Replicated Redis Setup with statefulsets"},{"location":"statefulset/#reading-list","text":"https://redis.io/topics/replication https://discuss.pivotal.io/hc/en-us/articles/205309278-How-to-setup-Redis-master-and-slave-replication https://www.digitalocean.com/community/tutorials/how-to-configure-redis-replication-on-ubuntu-16-04 Run Replicated Statefulsets Applications Keywords init containers kubernetes statefulsets redis replication","title":"Reading List"},{"location":"vote-deployement_strategies/","text":"Release Strategies Releases with downtime using Recreate Strategy When the Recreate deployment strategy is used, * The old pods will be deleted * Then the new pods will be created. This will create some downtime in our stack. Let us change the deployment strategy to recreate and image tag to v4 . cp vote-deploy.yaml vote-deploy-recreate.yaml And edit the specs with following changes Update strategy to Recreate Remove rolling update specs file: vote-deploy-recreate.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote spec: strategy: type: Recreate revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template: metadata: name: vote labels: app: python role: vote version: v4 spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP and apply kubectl get pods,rs,deploy,svc kubectl apply -f vote-deploy-recreate.yaml kubectl rollout status deplloyment/vote While the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated. You would observe that All pods wit the current version are deleted first Only after all the existing pods are deleted, pods with new version are launched Canary Releases cd k8s-code/projets/instavote/dev mkdir canary cp vote-deploy.yaml canary/vote-canary-deploy.yaml change the following fields in vote-canary-deploy.yaml metadata.name: vote-canary spec.replicas: 3 spec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template.metadata.labels.version: v4 template.spec.containers.image: schoolofdevops/vote:v4 File: canary/frontend-canary-deploy.yml apiVersion: apps/v1 kind: Deployment metadata: name: vote-canary spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 3 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4, v5]} minReadySeconds: 40 template: metadata: name: vote labels: app: python role: vote version: v4 spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP Before creating this deployment, find out how many endpoints the service has, kubectl describe service/vote [sample output ] Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more... In this example current endpoints are 15 Now create the deployment for canary release kubectl apply -f canary/frontend-canary-deploy.yml And validate, kubectl get rs,deploy,svc kubectl describe service/vote When you describe vote service, observe the number of endpoints [sample output] Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more... Now its 18 , which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions. Delete Canary Once validated, you could clean up canary release using kubectl delete -f canary/vote-canary-deploy.yaml Blue/Green Releases Before proceeding, lets clean up the existing deployment. kubectl delete deploy/vote kubectl delete svc/vote kubectl get pods,deploy,rs,svc And create the work directory for blue-green release definitions. cd k8s-code/projets/instavote/dev mkdir blue-green file: blue-green/vote-blue-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote-blue spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v3 release: bluegreen code: blue spec: containers: - name: app image: schoolofdevops/vote:v3 ports: - containerPort: 80 protocol: TCP file: blue-green/vote-green-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote-green spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template: metadata: name: vote labels: app: python role: vote version: v3 release: bluegreen code: green spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP file: blue-green/vote-bg-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote-bg labels: role: vote release: bluegreen spec: selector: role: vote release: bluegreen code: green ports: - port: 80 targetPort: 80 nodePort: 30001 type: NodePort file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote release: bluegreen code: blue ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort Creating blue deployment Now create vote service and observe the endpoints kubectl apply -f vote-svc.yaml kubectl get svc kubectl describe svc/vote [sample output] Name: vote Namespace: instavote Labels: role=vote Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":... Selector: code=blue,release=bluegreen,role=vote Type: NodePort IP: 10.111.93.227 External IPs: 206.189.150.190,159.65.8.227 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 30000/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> where, * endpoints are None * its selecting pods with code=blue Now lets create the deployment for blue release kubectl get pods,rs,deploy kubectl apply -f blue-green/vote-blue-deploy.yaml kubectl get pods,rs,deploy kubectl rollout status deploy/vote-blue [sample output] Waiting for rollout to finish: 2 of 15 updated replicas are available... deployment \"vote-blue\" successfully rolled out Now if you check the service, it should have the pods launched with blue set as endpoints kubectl describe svc/vote Name: vote Namespace: instavote Labels: role=vote Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":... Selector: code=blue,release=bluegreen,role=vote Type: NodePort IP: 10.111.93.227 External IPs: 206.189.150.190,159.65.8.227 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 30000/TCP Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more... Session Affinity: None External Traffic Policy: Cluster Events: <none> You could observe the Endpoints created and added to the service. Browse to http://IPADDRESS:NODEPORT to see the application deployed. Deploying new version with green release While deploying a new version with blue-green strategy, we would Create a new deployment in parallel Test it by creating another service Cut over to new release by updating selector in the main service Lets create the deployment with new version and a service to test it. Lets call it the green deployment kubectl apply -f blue-green/vote-bg-svc.yaml kubectl apply -f blue-green/vote-bg-svc.yaml kubectl apply -f blue-green/vote-green-deploy.yaml kubectl rollout status deploy/vote-green [sample output] Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 7 of 15 updated replicas are available... deployment \"vote-green\" successfully rolled out Validate kubectl get pods,rs,deploy,svc You could also test it by going to the http://host:nodeport for service vote-bg Switching to new version Now that you have the new version running in parallel, you could quickly switch to it by updating selector for main vote service which is live. Please note, while switching there may be a momentory downtime. Steps visit http://HOST:NODEPORT for vote service update vote service to select green release apply service definition visit http://HOST:NODEPORT for vote service again to validate file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote release: bluegreen code: green ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort Apply it with kubectl apply -f vote-svc.yaml kubectl describe svc/vote If you visit http://HOST:NODEPORT for vote service, you should see the application version updated Clean up the previous version kubectl delete deploy/vote-blue Clean up blue-green configs Now that you are done testing blue green release, lets revert to our previous configurations. kubectl delete deploy/vote-green kubectl apply -f vote-deploy.yaml Also update the service definition and remove following selectors added for blue green release release: bluegreen code: blue file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort And apply kubectl apply -f vote-svc.yaml Pause/Unpause When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations, 1. we can pause the update, 2. fix the issue, 3. resume the update. Let us change the image tag to V4 in pod spec. File: vote-deploy.yaml spec: containers: - name: app image: schoolofdevops/vote:V4 ports: - containerPort: 80 protocol: TCP Apply the changes. kubectl apply -f vote-deploy.yaml kubectl get pods [Output] NAME READY STATUS RESTARTS AGE vote-6c4f7b49d8-g5dgc 1/1 Running 0 16m vote-765554cc7-xsbhs 0/1 ErrImagePull 0 9s Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag. Now pause the update kubectl rollout pause deploy/vote Set the deployment to use v4 version of the image. Now resume the update kubectl rollout resume deployment vote kubectl rollout status deployment vote [Ouput] deployment \"vote\" successfully rolled out and validate kubectl get pods,rs,deploy [Output] NAME READY STATUS RESTARTS AGE vote-6875c8df8f-k4hls 1/1 Running 0 1m When you do this, you skip the need of creating a new rolling update altogether.","title":"Building Deployment Strategies"},{"location":"vote-deployement_strategies/#release-strategies","text":"","title":"Release Strategies"},{"location":"vote-deployement_strategies/#releases-with-downtime-using-recreate-strategy","text":"When the Recreate deployment strategy is used, * The old pods will be deleted * Then the new pods will be created. This will create some downtime in our stack. Let us change the deployment strategy to recreate and image tag to v4 . cp vote-deploy.yaml vote-deploy-recreate.yaml And edit the specs with following changes Update strategy to Recreate Remove rolling update specs file: vote-deploy-recreate.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote spec: strategy: type: Recreate revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template: metadata: name: vote labels: app: python role: vote version: v4 spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP and apply kubectl get pods,rs,deploy,svc kubectl apply -f vote-deploy-recreate.yaml kubectl rollout status deplloyment/vote While the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated. You would observe that All pods wit the current version are deleted first Only after all the existing pods are deleted, pods with new version are launched","title":"Releases with downtime using Recreate Strategy"},{"location":"vote-deployement_strategies/#canary-releases","text":"cd k8s-code/projets/instavote/dev mkdir canary cp vote-deploy.yaml canary/vote-canary-deploy.yaml change the following fields in vote-canary-deploy.yaml metadata.name: vote-canary spec.replicas: 3 spec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template.metadata.labels.version: v4 template.spec.containers.image: schoolofdevops/vote:v4 File: canary/frontend-canary-deploy.yml apiVersion: apps/v1 kind: Deployment metadata: name: vote-canary spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 3 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4, v5]} minReadySeconds: 40 template: metadata: name: vote labels: app: python role: vote version: v4 spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP Before creating this deployment, find out how many endpoints the service has, kubectl describe service/vote [sample output ] Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more... In this example current endpoints are 15 Now create the deployment for canary release kubectl apply -f canary/frontend-canary-deploy.yml And validate, kubectl get rs,deploy,svc kubectl describe service/vote When you describe vote service, observe the number of endpoints [sample output] Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more... Now its 18 , which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions.","title":"Canary  Releases"},{"location":"vote-deployement_strategies/#delete-canary","text":"Once validated, you could clean up canary release using kubectl delete -f canary/vote-canary-deploy.yaml","title":"Delete Canary"},{"location":"vote-deployement_strategies/#bluegreen-releases","text":"Before proceeding, lets clean up the existing deployment. kubectl delete deploy/vote kubectl delete svc/vote kubectl get pods,deploy,rs,svc And create the work directory for blue-green release definitions. cd k8s-code/projets/instavote/dev mkdir blue-green file: blue-green/vote-blue-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote-blue spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3]} template: metadata: name: vote labels: app: python role: vote version: v3 release: bluegreen code: blue spec: containers: - name: app image: schoolofdevops/vote:v3 ports: - containerPort: 80 protocol: TCP file: blue-green/vote-green-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: vote-green spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 revisionHistoryLimit: 4 paused: false replicas: 15 minReadySeconds: 20 selector: matchLabels: role: vote matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]} template: metadata: name: vote labels: app: python role: vote version: v3 release: bluegreen code: green spec: containers: - name: app image: schoolofdevops/vote:v4 ports: - containerPort: 80 protocol: TCP file: blue-green/vote-bg-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote-bg labels: role: vote release: bluegreen spec: selector: role: vote release: bluegreen code: green ports: - port: 80 targetPort: 80 nodePort: 30001 type: NodePort file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote release: bluegreen code: blue ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort","title":"Blue/Green  Releases"},{"location":"vote-deployement_strategies/#creating-blue-deployment","text":"Now create vote service and observe the endpoints kubectl apply -f vote-svc.yaml kubectl get svc kubectl describe svc/vote [sample output] Name: vote Namespace: instavote Labels: role=vote Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":... Selector: code=blue,release=bluegreen,role=vote Type: NodePort IP: 10.111.93.227 External IPs: 206.189.150.190,159.65.8.227 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 30000/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> where, * endpoints are None * its selecting pods with code=blue Now lets create the deployment for blue release kubectl get pods,rs,deploy kubectl apply -f blue-green/vote-blue-deploy.yaml kubectl get pods,rs,deploy kubectl rollout status deploy/vote-blue [sample output] Waiting for rollout to finish: 2 of 15 updated replicas are available... deployment \"vote-blue\" successfully rolled out Now if you check the service, it should have the pods launched with blue set as endpoints kubectl describe svc/vote Name: vote Namespace: instavote Labels: role=vote Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":... Selector: code=blue,release=bluegreen,role=vote Type: NodePort IP: 10.111.93.227 External IPs: 206.189.150.190,159.65.8.227 Port: <unset> 80/TCP TargetPort: 80/TCP NodePort: <unset> 30000/TCP Endpoints: 10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more... Session Affinity: None External Traffic Policy: Cluster Events: <none> You could observe the Endpoints created and added to the service. Browse to http://IPADDRESS:NODEPORT to see the application deployed.","title":"Creating  blue deployment"},{"location":"vote-deployement_strategies/#deploying-new-version-with-green-release","text":"While deploying a new version with blue-green strategy, we would Create a new deployment in parallel Test it by creating another service Cut over to new release by updating selector in the main service Lets create the deployment with new version and a service to test it. Lets call it the green deployment kubectl apply -f blue-green/vote-bg-svc.yaml kubectl apply -f blue-green/vote-bg-svc.yaml kubectl apply -f blue-green/vote-green-deploy.yaml kubectl rollout status deploy/vote-green [sample output] Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 0 of 15 updated replicas are available... Waiting for rollout to finish: 7 of 15 updated replicas are available... deployment \"vote-green\" successfully rolled out Validate kubectl get pods,rs,deploy,svc You could also test it by going to the http://host:nodeport for service vote-bg","title":"Deploying new version with green release"},{"location":"vote-deployement_strategies/#switching-to-new-version","text":"Now that you have the new version running in parallel, you could quickly switch to it by updating selector for main vote service which is live. Please note, while switching there may be a momentory downtime. Steps visit http://HOST:NODEPORT for vote service update vote service to select green release apply service definition visit http://HOST:NODEPORT for vote service again to validate file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote release: bluegreen code: green ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort Apply it with kubectl apply -f vote-svc.yaml kubectl describe svc/vote If you visit http://HOST:NODEPORT for vote service, you should see the application version updated","title":"Switching to new version"},{"location":"vote-deployement_strategies/#clean-up-the-previous-version","text":"kubectl delete deploy/vote-blue","title":"Clean up the previous version"},{"location":"vote-deployement_strategies/#clean-up-blue-green-configs","text":"Now that you are done testing blue green release, lets revert to our previous configurations. kubectl delete deploy/vote-green kubectl apply -f vote-deploy.yaml Also update the service definition and remove following selectors added for blue green release release: bluegreen code: blue file: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: name: vote labels: role: vote spec: selector: role: vote ports: - port: 80 targetPort: 80 nodePort: 30000 type: NodePort And apply kubectl apply -f vote-svc.yaml","title":"Clean up blue-green configs"},{"location":"vote-deployement_strategies/#pauseunpause","text":"When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations, 1. we can pause the update, 2. fix the issue, 3. resume the update. Let us change the image tag to V4 in pod spec. File: vote-deploy.yaml spec: containers: - name: app image: schoolofdevops/vote:V4 ports: - containerPort: 80 protocol: TCP Apply the changes. kubectl apply -f vote-deploy.yaml kubectl get pods [Output] NAME READY STATUS RESTARTS AGE vote-6c4f7b49d8-g5dgc 1/1 Running 0 16m vote-765554cc7-xsbhs 0/1 ErrImagePull 0 9s Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag. Now pause the update kubectl rollout pause deploy/vote Set the deployment to use v4 version of the image. Now resume the update kubectl rollout resume deployment vote kubectl rollout status deployment vote [Ouput] deployment \"vote\" successfully rolled out and validate kubectl get pods,rs,deploy [Output] NAME READY STATUS RESTARTS AGE vote-6875c8df8f-k4hls 1/1 Running 0 1m When you do this, you skip the need of creating a new rolling update altogether.","title":"Pause/Unpause"},{"location":"vote-persistent-volumes/","text":"Steps to set up NFS based Persistent Volumes Set up NFS Common On all kubernetes nodes, if you have not already installed nfs, use the following command to do so sudo apt-get update sudo apt-get install nfs-common [Skip this step if you are using a vagrant setup recommended as part of this course. ] Set up NFS Provisioner in kubernetes Change into nfs provisioner installation dir cd k8s-code/storage Deploy nfs-client provisioner. kubectl apply -f nfs This will create all the objects required to setup a nfs provisioner. Creating a Persistent Volume Claim switch to project directory cd k8s-code/projects/instavote/dev/ file: db-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: db-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: nfs And lets create the Persistent Volume Claim kubectl get pvc,storageclass kubectl logs -f nfs-provisioner-0 kubectl apply -f db-pvc.yaml kubectl get pvc,storageclass kubectl describe pvc db-pvc file: db-deploy.yaml ... spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP #mount db-vol to postgres data path volumeMounts: - name: db-vol mountPath: /var/lib/postgresql/data #create a volume with pvc volumes: - name: db-vol persistentVolumeClaim: claimName: db-pvc Observe which host db pod is running on kubectl get pod -o wide --selector='role=db' And apply this code as kubectl apply -f db-deploy.yaml kubectl get pod -o wide --selector='role=db' Observe the volume and its content created on the nfs server Observe which host the pod for db was created this time. Analyse the behavior of a deployment controller.","title":"Making Data Persist"},{"location":"vote-persistent-volumes/#steps-to-set-up-nfs-based-persistent-volumes","text":"","title":"Steps to set up NFS based Persistent Volumes"},{"location":"vote-persistent-volumes/#set-up-nfs-common","text":"On all kubernetes nodes, if you have not already installed nfs, use the following command to do so sudo apt-get update sudo apt-get install nfs-common [Skip this step if you are using a vagrant setup recommended as part of this course. ]","title":"Set up NFS Common"},{"location":"vote-persistent-volumes/#set-up-nfs-provisioner-in-kubernetes","text":"Change into nfs provisioner installation dir cd k8s-code/storage Deploy nfs-client provisioner. kubectl apply -f nfs This will create all the objects required to setup a nfs provisioner.","title":"Set up NFS Provisioner in kubernetes"},{"location":"vote-persistent-volumes/#creating-a-persistent-volume-claim","text":"switch to project directory cd k8s-code/projects/instavote/dev/ file: db-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: db-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: nfs And lets create the Persistent Volume Claim kubectl get pvc,storageclass kubectl logs -f nfs-provisioner-0 kubectl apply -f db-pvc.yaml kubectl get pvc,storageclass kubectl describe pvc db-pvc file: db-deploy.yaml ... spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP #mount db-vol to postgres data path volumeMounts: - name: db-vol mountPath: /var/lib/postgresql/data #create a volume with pvc volumes: - name: db-vol persistentVolumeClaim: claimName: db-pvc Observe which host db pod is running on kubectl get pod -o wide --selector='role=db' And apply this code as kubectl apply -f db-deploy.yaml kubectl get pod -o wide --selector='role=db' Observe the volume and its content created on the nfs server Observe which host the pod for db was created this time. Analyse the behavior of a deployment controller.","title":"Creating a Persistent Volume Claim"},{"location":"extras/rbac/","text":"TLS in Kubernetes Kubernetes API typically runs on two ports. * 8080 * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager). * This port is insecure. * 6443 * TLS Enabled. * Available for kubectl and others. Verbs Verbs are the action to be taken on resources. Some of the verbs in Kubernetes are, get list create update patch watch delete Resources Resources are the object being manipulated by the verb. Ex: pods, deployments, service, namespaces, nodes, etc., Role Based Access Control (RBAC) RBAC is the most used form of access control to grant or revoke permissions to users. It is used for dynamically configuring policies through API. RBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies. Objects that are used to create policies, Roles, ClusterRoles. Objects that are used to implement policies, RoleBindings, ClusterRoleBindings. Roles Roles grant access to resources within a single namespace . Roles cannot grant permission for global(cluster-wide) resources. ClusterRoles ClusterRoles works similar to Roles, but for cluster-wide resources. RoleBindings RoleBidings are used to grant permission defined in a Role to a user or a set of users . RoleBindings can also refer to ClusterRoles . ClusterRoleBindings ClusterRoleBindings works same as RoleBindings, but cluster-wide.","title":"Rbac"},{"location":"extras/rbac/#tls-in-kubernetes","text":"Kubernetes API typically runs on two ports. * 8080 * This port is available only for processes running on master machine (kube-scheduler, kube-controller-manager). * This port is insecure. * 6443 * TLS Enabled. * Available for kubectl and others.","title":"TLS in Kubernetes"},{"location":"extras/rbac/#verbs","text":"Verbs are the action to be taken on resources. Some of the verbs in Kubernetes are, get list create update patch watch delete","title":"Verbs"},{"location":"extras/rbac/#resources","text":"Resources are the object being manipulated by the verb. Ex: pods, deployments, service, namespaces, nodes, etc.,","title":"Resources"},{"location":"extras/rbac/#role-based-access-control-rbac","text":"RBAC is the most used form of access control to grant or revoke permissions to users. It is used for dynamically configuring policies through API. RBAC has two objects that are used for creating polices and another two objects that are used for implementing those policies. Objects that are used to create policies, Roles, ClusterRoles. Objects that are used to implement policies, RoleBindings, ClusterRoleBindings.","title":"Role Based Access Control (RBAC)"},{"location":"extras/rbac/#roles","text":"Roles grant access to resources within a single namespace . Roles cannot grant permission for global(cluster-wide) resources.","title":"Roles"},{"location":"extras/rbac/#clusterroles","text":"ClusterRoles works similar to Roles, but for cluster-wide resources.","title":"ClusterRoles"},{"location":"extras/rbac/#rolebindings","text":"RoleBidings are used to grant permission defined in a Role to a user or a set of users . RoleBindings can also refer to ClusterRoles .","title":"RoleBindings"},{"location":"extras/rbac/#clusterrolebindings","text":"ClusterRoleBindings works same as RoleBindings, but cluster-wide.","title":"ClusterRoleBindings"}]}