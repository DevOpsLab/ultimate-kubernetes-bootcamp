{
    "docs": [
        {
            "location": "/", 
            "text": "Ultimate Kubernetes Bootcamp\n\n\nWelcome to Kubernetes Fundamentals by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat", 
            "title": "Home"
        }, 
        {
            "location": "/#ultimate-kubernetes-bootcamp", 
            "text": "Welcome to Kubernetes Fundamentals by School of Devops  This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Ultimate Kubernetes Bootcamp"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy  Venkat", 
            "title": "Team"
        }, 
        {
            "location": "/minikube/", 
            "text": "Single node k8s cluster with Minikube\n\n\nMinikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.\n\n\nThis document explains how to setup and work with single node kubernetes cluster with minikube.\n\n\nInstall Minikube\n\n\nInstructions to install minikube may vary based on the operating system and choice of the hypervisor.\n\nThis is the official document\n which explains how to install minikube.\n\n\nStart all in one single node cluster with minikube\n\n\nminikube status\n\n\n\n\n[output]\n\n\nminikube:\ncluster:\nkubectl:\n\n\n\n\nminikube start\n\n\n\n\n[output]\n\n\nStarting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n\n\n\nminikube status\n\n\n\n\n\n[output]\n\n\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100\n\n\n\n\nLaunch a kubernetes dashboard\n\n\nminikube dashboard\n\n\n\n\nSetting up docker environment\n\n\nminikube docker-env\nexport DOCKER_TLS_VERIFY=\n1\n\nexport DOCKER_HOST=\ntcp://192.168.99.100:2376\n\nexport DOCKER_CERT_PATH=\n/Users/gouravshah/.minikube/certs\n\nexport DOCKER_API_VERSION=\n1.23\n\n# Run this command to configure your shell:\n# eval $(minikube docker-env)\n\n\n\n\nRun the command given above,\ne.g.\n\n\neval $(minikube docker-env)\n\n\n\n\nNow your docker client should be able to connect with the minikube cluster\n\n\ndocker ps\n\n\n\n\nAdditional Commands\n\n\nminikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Using minikube to setup single node environment"
        }, 
        {
            "location": "/minikube/#single-node-k8s-cluster-with-minikube", 
            "text": "Minikube offers one of the easiest zero to dev experience\nto setup a single node kubernetes cluster. Its also the ideal way\nto create a local dev environment to test kubernetes code on.  This document explains how to setup and work with single node kubernetes cluster with minikube.", 
            "title": "Single node k8s cluster with Minikube"
        }, 
        {
            "location": "/minikube/#install-minikube", 
            "text": "Instructions to install minikube may vary based on the operating system and choice of the hypervisor. This is the official document  which explains how to install minikube.", 
            "title": "Install Minikube"
        }, 
        {
            "location": "/minikube/#start-all-in-one-single-node-cluster-with-minikube", 
            "text": "minikube status  [output]  minikube:\ncluster:\nkubectl:  minikube start  [output]  Starting local Kubernetes v1.8.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.  minikube status  [output]  minikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100", 
            "title": "Start all in one single node cluster with minikube"
        }, 
        {
            "location": "/minikube/#launch-a-kubernetes-dashboard", 
            "text": "minikube dashboard", 
            "title": "Launch a kubernetes dashboard"
        }, 
        {
            "location": "/minikube/#setting-up-docker-environment", 
            "text": "minikube docker-env\nexport DOCKER_TLS_VERIFY= 1 \nexport DOCKER_HOST= tcp://192.168.99.100:2376 \nexport DOCKER_CERT_PATH= /Users/gouravshah/.minikube/certs \nexport DOCKER_API_VERSION= 1.23 \n# Run this command to configure your shell:\n# eval $(minikube docker-env)  Run the command given above,\ne.g.  eval $(minikube docker-env)  Now your docker client should be able to connect with the minikube cluster  docker ps", 
            "title": "Setting up docker environment"
        }, 
        {
            "location": "/minikube/#additional-commands", 
            "text": "minikube ip\nminikube get-k8s-versions\nminikube logs", 
            "title": "Additional Commands"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/", 
            "text": "Install VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a VM Template\n\n\nIf you have already copied/downloaded the box file \nubuntu-xenial64.box\n, go to the directory which contains that file. If you do not have a box file, skip to next section.\n\n\nvagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up\n\n\n\n\n\nLogin to nodes\n\n\nOpen three different terminals to login to 3 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh kube-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh kube-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh kube-03\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nInitialization of Master\n onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning VMs with Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#importing-a-vm-template", 
            "text": "If you have already copied/downloaded the box file  ubuntu-xenial64.box , go to the directory which contains that file. If you do not have a box file, skip to next section.  vagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list", 
            "title": "Importing a VM Template"
        }, 
        {
            "location": "/2_kube_cluster_vagrant/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/kubernetes/vagrant-kube-cluster\n\nvagrant up  Login to nodes  Open three different terminals to login to 3 nodes created with above command  Terminal 1  vagrant ssh kube-01\nsudo su  Terminal 2  vagrant ssh kube-02\nsudo su  Terminal 3  vagrant ssh kube-03\nsudo su  Once the environment is setup, follow  Initialization of Master  onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/3_install_kubernetes/", 
            "text": "Kubeadm : Bring Your Own Nodes (BYON)\n\n\nThis documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses \nkubeadm\n to install and configure kubernetes cluster.\n\n\nCompatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup (Skip if using vagrant)\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also   \ndownload and run this script\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tips\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\nkubectl get cs\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 31000/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 31000, and go to the browser, and access the dashboard with the following URL\n\ndo not use the IP above, use master node IP instead\n\n\nhttp://NODEIP:31000\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nCheck out the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Setup Kubernetes Cluster"
        }, 
        {
            "location": "/3_install_kubernetes/#kubeadm-bring-your-own-nodes-byon", 
            "text": "This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses  kubeadm  to install and configure kubernetes cluster.", 
            "title": "Kubeadm : Bring Your Own Nodes (BYON)"
        }, 
        {
            "location": "/3_install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/3_install_kubernetes/#base-setup-skip-if-using-vagrant", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also    download and run this script", 
            "title": "Base Setup (Skip if using vagrant)"
        }, 
        {
            "location": "/3_install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/3_install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/3_install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/3_install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/3_install_kubernetes/#troubleshooting-tips", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/3_install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/3_install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/3_install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes\nkubectl get cs  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/3_install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  31000/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 31000, and go to the browser, and access the dashboard with the following URL do not use the IP above, use master node IP instead  http://NODEIP:31000  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/3_install_kubernetes/#check-out-the-supporting-code", 
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  git clone https://github.com/schoolofdevops/k8s-code.git", 
            "title": "Check out the supporting code"
        }, 
        {
            "location": "/kube_visualizer/", 
            "text": "Kubernetes Visualizer\n\n\nIn this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time.\n\n\nSet up\n\n\nFork the repository and deploy the visualizer on kubernetes\n\n\ngit clone  https://github.com/schoolofdevops/kube-ops-view\nkubectl apply -f kube-ops-view/deploy/\n\n\n\n\n\n[Sample Output]\n\n\nserviceaccount \nkube-ops-view\n created\nclusterrole \nkube-ops-view\n created\nclusterrolebinding \nkube-ops-view\n created\ndeployment \nkube-ops-view\n created\ningress \nkube-ops-view\n created\ndeployment \nkube-ops-view-redis\n created\nservice \nkube-ops-view-redis\n created\nservice \nkube-ops-view\n created\n\n\n\n\nGet the nodeport for the service.\n\n\nkubectl get svc\n\n[output]\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkube-ops-view         NodePort    10.107.204.74   \nnone\n        80:**30073**/TCP   1m\nkube-ops-view-redis   ClusterIP   10.104.50.176   \nnone\n        6379/TCP       1m\nkubernetes            ClusterIP   10.96.0.1       \nnone\n        443/TCP        8m\n\n\n\n\nIn my case, port \n30073\n is the nodeport.\n\n\nVisit the port from the browser. You could add /#scale=2.0 or similar option where 2.0 = 200% the scale.\n\n\nhttp://\nNODE_IP:NODE_PORT\n/#scale=2.0", 
            "title": "Kuberentes Visualizer"
        }, 
        {
            "location": "/kube_visualizer/#kubernetes-visualizer", 
            "text": "In this chapter we will see how to set up kubernetes visualizer that will show us the changes in our cluster in real time.", 
            "title": "Kubernetes Visualizer"
        }, 
        {
            "location": "/kube_visualizer/#set-up", 
            "text": "Fork the repository and deploy the visualizer on kubernetes  git clone  https://github.com/schoolofdevops/kube-ops-view\nkubectl apply -f kube-ops-view/deploy/  [Sample Output]  serviceaccount  kube-ops-view  created\nclusterrole  kube-ops-view  created\nclusterrolebinding  kube-ops-view  created\ndeployment  kube-ops-view  created\ningress  kube-ops-view  created\ndeployment  kube-ops-view-redis  created\nservice  kube-ops-view-redis  created\nservice  kube-ops-view  created  Get the nodeport for the service.  kubectl get svc\n\n[output]\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkube-ops-view         NodePort    10.107.204.74    none         80:**30073**/TCP   1m\nkube-ops-view-redis   ClusterIP   10.104.50.176    none         6379/TCP       1m\nkubernetes            ClusterIP   10.96.0.1        none         443/TCP        8m  In my case, port  30073  is the nodeport.  Visit the port from the browser. You could add /#scale=2.0 or similar option where 2.0 = 200% the scale.  http:// NODE_IP:NODE_PORT /#scale=2.0", 
            "title": "Set up"
        }, 
        {
            "location": "/quickdive/", 
            "text": "Kubernetes Quick Dive\n\n\nLaunch vote application with kubernetes. (simiar to docker run command)\n\n\nkubectl  run vote --image=schoolofdevops/vote\n\nkubectl get pods\n\n\n\n\nPublish the application (similar to using -P for port mapping)\n\n\n\n\nkubectl expose deployment vote --type=NodePort --port 80\n\nkubectl get svc\n\n\n\n\n\nScale the vote app to run 4 instances.\n\n\nkubectl scale --replicas=4 deployment/vote\nkubectl get pods\n\n\n\n\nConnect to the app,  refresh the page to see it load balancing.  Also try to vote and observe what happens.  \n\n\nSetup additional apps\n\n\nNow lets launch rest of the apps.\n\n\nkubectl  run redis  --image=redis:alpine\n\nkubectl expose deployment redis --port 6379\n\n\nkubectl  run worker --image=schoolofdevops/worker\n\nkubectl  run db --image=postgres:9.4\n\nkubectl expose deployment db --port 5432\n\nkubectl run result --image=schoolofdevops/vote-result\n\nkubectl expose deployment result --type=NodePort --port 80\n\n\n\n\n\nCleaing up\n\n\nOnce you are done observing, you could delete it with the following commands,\n\n\n\nkubectl delete deploy db redis vote worker result\n\nkubectl delete svc db redis results vote", 
            "title": "Kubernetes Quickdive"
        }, 
        {
            "location": "/quickdive/#kubernetes-quick-dive", 
            "text": "Launch vote application with kubernetes. (simiar to docker run command)  kubectl  run vote --image=schoolofdevops/vote\n\nkubectl get pods  Publish the application (similar to using -P for port mapping)  \n\nkubectl expose deployment vote --type=NodePort --port 80\n\nkubectl get svc  Scale the vote app to run 4 instances.  kubectl scale --replicas=4 deployment/vote\nkubectl get pods  Connect to the app,  refresh the page to see it load balancing.  Also try to vote and observe what happens.", 
            "title": "Kubernetes Quick Dive"
        }, 
        {
            "location": "/quickdive/#setup-additional-apps", 
            "text": "Now lets launch rest of the apps.  kubectl  run redis  --image=redis:alpine\n\nkubectl expose deployment redis --port 6379\n\n\nkubectl  run worker --image=schoolofdevops/worker\n\nkubectl  run db --image=postgres:9.4\n\nkubectl expose deployment db --port 5432\n\nkubectl run result --image=schoolofdevops/vote-result\n\nkubectl expose deployment result --type=NodePort --port 80", 
            "title": "Setup additional apps"
        }, 
        {
            "location": "/quickdive/#cleaing-up", 
            "text": "Once you are done observing, you could delete it with the following commands,  \nkubectl delete deploy db redis vote worker result\n\nkubectl delete svc db redis results vote", 
            "title": "Cleaing up"
        }, 
        {
            "location": "/5-vote-deploying_pods/", 
            "text": "Deploying Pods\n\n\nLife of a pod\n\n\n\n\nPending : in progress\n\n\nRunning\n\n\nSucceeded : successfully exited\n\n\nFailed\n\n\nUnknown\n\n\n\n\nProbes\n\n\n\n\nlivenessProbe : Containers are Alive\n\n\nreadinessProbe : Ready to Serve Traffic\n\n\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n =\n Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nSpec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/\n\n\nTo list supported version of apis\n\n\nkubectl api-versions\n\n\n\n\nWriting Pod Spec\n\n\nLets now create the  Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows.\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion:\nkind: Pod\nmetadata:\nspec:\n\n\n\n\nLets edit this and add the pod specs\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1\n      ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n\n\nUse this link to refer to pod spec\n\n\nLaunching and operating a Pod\n\n\nTo launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured.\n\n\nwatch -n 1  kubectl get pods,deploy,rs,svc\n\n\n\n\n\nkubectl Syntax:\n\n\nkubectl\nkubectl apply --help\nkubectl apply -f FILE\n\n\n\n\nTo Launch pod using configs above,\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\n\nTo view pods\n\n\nkubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods vote\n\n\n\n\n[Output:]\n\n\nName:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:    \nnone\n\nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:      \nnone\n\nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:    \nnone\n\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image \nschoolofdevops/vote:latest\n\n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image \nschoolofdevops/vote:latest\n\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945\n\n\n\n\nCommands to operate the pod\n\n\n\nkubectl logs vote\n\nkubectl exec -it vote  sh\n\n\n\n\n\n\nInside the container in a pod\n\n\nifconfig\ncat /etc/issue\nhostname\ncat /proc/cpuinfo\nps aux\n\n\n\n\nLab: Examine pods from the dashboard\n\n\nPort Forwarding\n\n\nThis works if you have setup \nkubectl\n on a local laptop.\n\n\nkubectl port-forward --help\nkubectl port-forward vote 8000:80\n\n\n\n\nTroubleshooting Tip\n\n\nIf you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.\n\n\nkubectl get pod vote -o yaml\n\n\n\n\nLets learn by example. Update pod spec and change the image to something that does not exist.\n\n\nkubectl edit pod vote\n\n\n\n\nThis will open a editor. Go to the line which defines image  and change it to a tag that does not exist\n\n\ne.g.\n\n\nspec:\n  containers:\n  - image: schoolofdevops/vote:srgwegew\n    imagePullPolicy: Always\n\n\n\n\nwhere tag \nsrgwegew\n does not exist. As soon as you save this file, kubernetes will apply the change.\n\n\nNow check the status,\n\n\nkubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m\n\n\n\n\nThe above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.\n\n\nObserve the \nstatus\n field.  \n\n\nkubectl get pod vote -o yaml\n\n\n\n\nNow the status field shows a detailed information, including what the exact error. Observe the following snippet...\n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nThis will help you to pinpoint to the exact cause and fix it quickly.\n\n\nNow that you  are done experimenting with pod, delete it with the following command,\n\n\nkubectl delete pod vote\n\nkubectl get pods\n\n\n\n\nAttach a Volume to the Pod\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nLocal host volumes are of two types:\n\n  * emptyDir\n\n  * hostPath  \n\n\nWe will pick hostPath. \nRefer to this doc to read more about hostPath.\n\n\nFile: db-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nExercise\n : Examine \n/var/lib/pgdata\n on the systems to check if the directory is been created and if the data is present.\n\n\nCreating Multi Container Pods\n\n\nfile: multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    tier: front\n    app: nginx\n    role: ui\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n          protocol: TCP\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/html-sample-app\n\n    - name: sync\n      image: schoolofdevops/sync:v2\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/app\n\n  volumes:\n    - name: data\n      emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync\n\n\n\n\n\nObserve whats common and whats isolated in two containers running inside  the same pod using the following commands,\n\n\nshared\n\n\nhostname\nifconfig\n\n\n\n\nisolated\n\n\ncat /etc/issue\nps aux\ndf -h\n\n\n\n\n\nExercise\n\n\nCreate a pod definition for redis and deploy.\n\n\nReading List\n\n\n\n\nPodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core\n\n\nManaging Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/\n\n\nNode Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Launching Pods"
        }, 
        {
            "location": "/5-vote-deploying_pods/#deploying-pods", 
            "text": "Life of a pod   Pending : in progress  Running  Succeeded : successfully exited  Failed  Unknown", 
            "title": "Deploying Pods"
        }, 
        {
            "location": "/5-vote-deploying_pods/#probes", 
            "text": "livenessProbe : Containers are Alive  readinessProbe : Ready to Serve Traffic", 
            "title": "Probes"
        }, 
        {
            "location": "/5-vote-deploying_pods/#resource-configs", 
            "text": "Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  =  Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/  To list supported version of apis  kubectl api-versions", 
            "title": "Resource Configs"
        }, 
        {
            "location": "/5-vote-deploying_pods/#writing-pod-spec", 
            "text": "Lets now create the  Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows.  Filename: k8s-code/pods/vote-pod.yaml  apiVersion:\nkind: Pod\nmetadata:\nspec:  Lets edit this and add the pod specs  Filename: k8s-code/pods/vote-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1\n      ports:\n        - containerPort: 80\n          protocol: TCP  Use this link to refer to pod spec", 
            "title": "Writing Pod Spec"
        }, 
        {
            "location": "/5-vote-deploying_pods/#launching-and-operating-a-pod", 
            "text": "To launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured.  watch -n 1  kubectl get pods,deploy,rs,svc  kubectl Syntax:  kubectl\nkubectl apply --help\nkubectl apply -f FILE  To Launch pod using configs above,  kubectl apply -f vote-pod.yaml  To view pods  kubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote  To get detailed info  kubectl describe pods vote  [Output:]  Name:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:     none \nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:       none \nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:     none \nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image  schoolofdevops/vote:latest \n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image  schoolofdevops/vote:latest \n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945  Commands to operate the pod  \nkubectl logs vote\n\nkubectl exec -it vote  sh  Inside the container in a pod  ifconfig\ncat /etc/issue\nhostname\ncat /proc/cpuinfo\nps aux", 
            "title": "Launching and operating a Pod"
        }, 
        {
            "location": "/5-vote-deploying_pods/#lab-examine-pods-from-the-dashboard", 
            "text": "", 
            "title": "Lab: Examine pods from the dashboard"
        }, 
        {
            "location": "/5-vote-deploying_pods/#port-forwarding", 
            "text": "This works if you have setup  kubectl  on a local laptop.  kubectl port-forward --help\nkubectl port-forward vote 8000:80", 
            "title": "Port Forwarding"
        }, 
        {
            "location": "/5-vote-deploying_pods/#troubleshooting-tip", 
            "text": "If you would like to know whats the current status of the pod, and if its in a error state, find out the cause of the error, following command could be very handy.  kubectl get pod vote -o yaml  Lets learn by example. Update pod spec and change the image to something that does not exist.  kubectl edit pod vote  This will open a editor. Go to the line which defines image  and change it to a tag that does not exist  e.g.  spec:\n  containers:\n  - image: schoolofdevops/vote:srgwegew\n    imagePullPolicy: Always  where tag  srgwegew  does not exist. As soon as you save this file, kubernetes will apply the change.  Now check the status,  kubectl get pods  \n\nNAME      READY     STATUS             RESTARTS   AGE\nvote      0/1       ImagePullBackOff   0          27m  The above output will only show the status, with a vague error. To find the exact error, lets get the stauts of the pod.  Observe the  status  field.    kubectl get pod vote -o yaml  Now the status field shows a detailed information, including what the exact error. Observe the following snippet...  status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248  This will help you to pinpoint to the exact cause and fix it quickly.  Now that you  are done experimenting with pod, delete it with the following command,  kubectl delete pod vote\n\nkubectl get pods", 
            "title": "Troubleshooting Tip"
        }, 
        {
            "location": "/5-vote-deploying_pods/#attach-a-volume-to-the-pod", 
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Local host volumes are of two types: \n  * emptyDir \n  * hostPath    We will pick hostPath.  Refer to this doc to read more about hostPath.  File: db-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate  To create this pod,  kubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events  Exercise  : Examine  /var/lib/pgdata  on the systems to check if the directory is been created and if the data is present.", 
            "title": "Attach a Volume to the Pod"
        }, 
        {
            "location": "/5-vote-deploying_pods/#creating-multi-container-pods", 
            "text": "file: multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    tier: front\n    app: nginx\n    role: ui\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n          protocol: TCP\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/html-sample-app\n\n    - name: sync\n      image: schoolofdevops/sync:v2\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/app\n\n  volumes:\n    - name: data\n      emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync  Observe whats common and whats isolated in two containers running inside  the same pod using the following commands,  shared  hostname\nifconfig  isolated  cat /etc/issue\nps aux\ndf -h", 
            "title": "Creating Multi Container Pods"
        }, 
        {
            "location": "/5-vote-deploying_pods/#exercise", 
            "text": "Create a pod definition for redis and deploy.", 
            "title": "Exercise"
        }, 
        {
            "location": "/5-vote-deploying_pods/#reading-list", 
            "text": "PodSpec: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core  Managing Volumes with Kubernetes: https://kubernetes.io/docs/concepts/storage/volumes/  Node Selectors, Affinity: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Reading List"
        }, 
        {
            "location": "/replication/", 
            "text": "Making application high available with Replication Controllers\n\n\nIf you are not running a monitoring screen, start it in a new terminal with the following command.\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc\n\n\n\n\nkubectl delete pod vote\n\n\n\n\nSetting up a Namespace\n\n\nCheck current config\n\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a namespace\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \ninstavote\n  \n\n\ncd projects/instavote\ncat instavote-ns.yaml\n\n\n\n\n[output]\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n\n\n\n\nLets create a namespace\n\n\nkubectl get ns\nkubectl apply -f instavote-ns.yaml\n\nkubectl get ns\n\n\n\n\nAnd switch to it\n\n\nkubectl config --help\n\nkubectl config get-contexts\n\nkubectl config current-context\n\nkubectl config set-context $(kubectl config current-context) --namespace=instavote\n\nkubectl config view\n\nkubectl config get-contexts\n\n\n\n\n\nExercise\n: Go back to the monitoring screen and observe what happens after switching the namespace.\n\n\nTo understand how ReplicaSets works with the selectors  lets launch a pod in the new namespace with existing specs.\n\n\ncd k8s-code/pods\nkubectl apply -f vote-pod.yaml\n\nkubectl get pods\ncd ../projects/instavote/dev/\n\n\n\n\nLets now write the spec for the Rplica Set. This is going to mainly contain,\n\n\n\n\nreplicas\n\n\nselector\n\n\ntemplate (pod spec )\n\n\nminReadySeconds\n\n\n\n\nfile: vote-rs.yaml\n\n\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 5\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n\n\n\n\nLets now add the metadata and spec from pod spec defined in vote-pod.yaml. And with that, the Replica Set Spec changes to\n\n\nfile: vote-rs.yaml\n\n\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 5\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\n\nReplica Sets in Action\n\n\nkubectl apply -f vote-rs.yaml --dry-run\n\nkubectl apply -f vote-rs.yaml\n\nkubectl get rs\n\nkubectl describe rs vote\n\nkubectl get pods\n\n\n\n\n\n\nExercise\n :  \n\n\n\n\nSwitch to monitoring screen, observe how many replicas were created  and why\n\n\nCompare selectors and labels of the pods created with and without replica sets\n\n\n\n\nkubectl get pods\n\nkubectl get pods --show-labels\n\n\n\n\nExercise: Deploying new version of the application\n\n\nkubectl edit rs/vote\n\n\n\n\nUpdate the version of the image from \nschoolofdevops/vote:v1\n to \nschoolofdevops/vote:v2\n\n\nSave the file. Observe if application got updated. Note what do you observe. Do you see the new version deployed ??\n\n\nExercise: Self Healing Replica Sets\n\n\nList the pods and kill some of those, see what replica set does.\n\n\nkubectl get pods\nkubectl delete pods  vote-xxxx  vote-yyyy\n\n\n\n\nwhere replace xxxx and yyyy with actual pod ids.\n\n\nQuestions:\n\n\n\n\nDid replica set replaced the pods ?\n\n\nWhich version of the application is running now ?\n\n\n\n\nLets now delete the pod created independent of replica set.\n\n\nkubectl get pods\nkubectl delete pods  vote\n\n\n\n\nObserve what happens.\n  * Does replica set take any action after deleting the pod created outside of its spec ? Why?", 
            "title": "Making application highly available"
        }, 
        {
            "location": "/replication/#making-application-high-available-with-replication-controllers", 
            "text": "If you are not running a monitoring screen, start it in a new terminal with the following command.  watch -n 1 kubectl get  pod,deploy,rs,svc  kubectl delete pod vote", 
            "title": "Making application high available with Replication Controllers"
        }, 
        {
            "location": "/replication/#setting-up-a-namespace", 
            "text": "Check current config  \nkubectl config view  You could also examine the current configs in file  cat ~/.kube/config", 
            "title": "Setting up a Namespace"
        }, 
        {
            "location": "/replication/#creating-a-namespace", 
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  instavote     cd projects/instavote\ncat instavote-ns.yaml  [output]  kind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote  Lets create a namespace  kubectl get ns\nkubectl apply -f instavote-ns.yaml\n\nkubectl get ns  And switch to it  kubectl config --help\n\nkubectl config get-contexts\n\nkubectl config current-context\n\nkubectl config set-context $(kubectl config current-context) --namespace=instavote\n\nkubectl config view\n\nkubectl config get-contexts  Exercise : Go back to the monitoring screen and observe what happens after switching the namespace.  To understand how ReplicaSets works with the selectors  lets launch a pod in the new namespace with existing specs.  cd k8s-code/pods\nkubectl apply -f vote-pod.yaml\n\nkubectl get pods\ncd ../projects/instavote/dev/  Lets now write the spec for the Rplica Set. This is going to mainly contain,   replicas  selector  template (pod spec )  minReadySeconds   file: vote-rs.yaml  apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 5\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:  Lets now add the metadata and spec from pod spec defined in vote-pod.yaml. And with that, the Replica Set Spec changes to  file: vote-rs.yaml  apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 5\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP", 
            "title": "Creating a namespace"
        }, 
        {
            "location": "/replication/#replica-sets-in-action", 
            "text": "kubectl apply -f vote-rs.yaml --dry-run\n\nkubectl apply -f vote-rs.yaml\n\nkubectl get rs\n\nkubectl describe rs vote\n\nkubectl get pods  Exercise  :     Switch to monitoring screen, observe how many replicas were created  and why  Compare selectors and labels of the pods created with and without replica sets   kubectl get pods\n\nkubectl get pods --show-labels", 
            "title": "Replica Sets in Action"
        }, 
        {
            "location": "/replication/#exercise-deploying-new-version-of-the-application", 
            "text": "kubectl edit rs/vote  Update the version of the image from  schoolofdevops/vote:v1  to  schoolofdevops/vote:v2  Save the file. Observe if application got updated. Note what do you observe. Do you see the new version deployed ??", 
            "title": "Exercise: Deploying new version of the application"
        }, 
        {
            "location": "/replication/#exercise-self-healing-replica-sets", 
            "text": "List the pods and kill some of those, see what replica set does.  kubectl get pods\nkubectl delete pods  vote-xxxx  vote-yyyy  where replace xxxx and yyyy with actual pod ids.  Questions:   Did replica set replaced the pods ?  Which version of the application is running now ?   Lets now delete the pod created independent of replica set.  kubectl get pods\nkubectl delete pods  vote  Observe what happens.\n  * Does replica set take any action after deleting the pod created outside of its spec ? Why?", 
            "title": "Exercise: Self Healing Replica Sets"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/", 
            "text": "Exposing Application with  a Service\n\n\nTypes of Services:   \n\n\n\n\nClusterIP\n\n\nNodePort\n\n\nLoadBalancer\n\n\nExternalName\n\n\n\n\n\n\nkubectl get pods\nkubectl get svc\n\n\n\n\nSample Output:\n\n\nNAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m\n\n\n\n\nSetting up monitoring\n\n\nIf you are not running a monitoring screen, start it in a new terminal with the following command.\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc\n\n\n\n\nWriting Service Spec\n\n\nLets start writing the  meta information for service.  \n\n\nFilename: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n\n\n\n\nAnd then add the spec to it. Refer to Service (v1 core) api at this page https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\n\nSave the file.\n\n\nNow to create a service:\n\n\nkubectl apply -f vote-svc.yaml --dry-run\nkubectl apply -f vote-svc.yaml\nkubectl get svc\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service vote\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nsvc\n,\ntier\n:\nfront\n},\nname\n:\nvote\n,\nnamespace\n:\ninstavote\n},\nspec\n:{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                     \nunset\n  80/TCP\nTargetPort:               80/TCP\nNodePort:                 \nunset\n  31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \nnone\n\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 31429.\n\n\nSample output will be:\n\n\n\n\nExposing the app with ExternalIP\n\n\nspec:\n  selector:\n    role: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - xx.xx.xx.xx\n    - yy.yy.yy.yy\n\n\n\n\nWhere\n\n\nreplace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts.\n\n\napply\n\n\nkubectl  get svc\nkubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote\n\n\n\n\n[sample output]\n\n\nNAME      TYPE       CLUSTER-IP      EXTERNAL-IP                    PORT(S)        AGE\nvote      NodePort   10.107.71.204   206.189.150.190,159.65.8.227   80:30000/TCP   11m\n\n\n\n\nwhere,\n\n\nEXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http://\n:\n to access this application.  e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on.\n\n\nInternal Service Discovery\n\n\n\n\nVisit the vote app from browser\n\n\nAttemp to vote by clicking on one of the options\n\n\n\n\nobserve what happens. Does it go through?  \n\n\nDebugging,\n\n\nkubectl get pod\nkubectl exec vote-xxxx ping redis\n\n\n\n\n\n[replace xxxx with the actual pod id of one of the vote pods ]\n\n\nkeep the above command on a watch. You should create a new terminal to run the watch command.\n\n\ne.g.\n\n\nwatch  kubectl exec vote-kvc7j ping redis\n\n\n\n\nwhere, vote-kvc7j is one of the vote pods that I am running. Replace this with the actual pod id.\n\n\nNow create \nredis\n service\n\n\nkubectl apply -f redis-svc.yaml\n\nkubectl get svc\n\nkubectl describe svc redis\n\n\n\n\nWatch the ping and observe if its able to resolve \nredis\n by hostname and its pointing to an IP address.\n\n\ne.g.\n\n\nPING redis (10.102.77.6): 56 data bytes\n\n\n\n\nwhere \n10.102.77.6\n is the ClusterIP assigned to the service.  \n\n\nWhat happened here?\n\n\n\n\nService \nredis\n was created with a ClusterIP e.g. 10.102.77.6\n\n\nA DNS entry was created for this service. The fqdn of the service is \nredis.instavote.svc.cluster.local\n and it takes the form of my-svc.my-namespace.svc.cluster.local\n\n\nEach pod points to  internal  DNS server running in the cluster. You could see the details of this by running the following commands\n\n\n\n\nkubectl exec vote-xxxx cat /etc/resolv.conf\n\n\n\n\n[replace vote-xxxx with actual pod id]\n\n\n[sample output]\n\n\nnameserver 10.96.0.10\nsearch instavote.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n\n\n\n\nwhere \n10.96.0.10\n is the ClusterIP assigned to the DNS service. You could co relate that with,\n\n\nkubectl get svc -n kube-system\n\n\nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkube-dns               ClusterIP   10.96.0.10     \nnone\n        53/UDP,53/TCP   1h\nkubernetes-dashboard   NodePort    10.104.42.73   \nnone\n        80:31000/TCP    23m\n\n\n\n\n\nwhere, \n10.96.0.10\n is the ClusterIP assigned to \nkube-dns\n and matches the configuration in \n/etc/resolv.conf\n above.\n\n\nCreating Endpoints for Redis\n\n\nService is been created, but you still need to launch the actual pods running \nredis\n application.\n\n\nCreate the endpoints now,\n\n\nkubectl apply -f redis-deploy.yaml\nkubectl describe svc redis\n\n\n\n\n\n[sample output]\n\n\nName:              redis\nNamespace:         instavote\nLabels:            role=redis\n                   tier=back\nAnnotations:       kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nredis\n,\ntier\n:\nback\n},\nname\n:\nredis\n,\nnamespace\n:\ninstavote\n},\nspec\n...\nSelector:          app=redis\nType:              ClusterIP\nIP:                10.102.77.6\nPort:              \nunset\n  6379/TCP\nTargetPort:        6379/TCP\nEndpoints:         10.32.0.6:6379,10.46.0.6:6379\nSession Affinity:  None\nEvents:            \nnone\n\n\n\n\n\nAgain, visit the vote app from browser, attempt to register your vote  and observe what happens now.\n\n\nReading\n\n\nDebugging \n\n  * Services\n\n  * \nKubernetes Services Documentation\n\n  * \nService API Specs for Kubernetes Version 1.10", 
            "title": "Publishing appliaction and Service Discovery"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#exposing-application-with-a-service", 
            "text": "Types of Services:      ClusterIP  NodePort  LoadBalancer  ExternalName    kubectl get pods\nkubectl get svc  Sample Output:  NAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m", 
            "title": "Exposing Application with  a Service"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#setting-up-monitoring", 
            "text": "If you are not running a monitoring screen, start it in a new terminal with the following command.  watch -n 1 kubectl get  pod,deploy,rs,svc", 
            "title": "Setting up monitoring"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#writing-service-spec", 
            "text": "Lets start writing the  meta information for service.    Filename: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:  And then add the spec to it. Refer to Service (v1 core) api at this page https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  Save the file.  Now to create a service:  kubectl apply -f vote-svc.yaml --dry-run\nkubectl apply -f vote-svc.yaml\nkubectl get svc  Now to check which port the pod is connected  kubectl describe service vote  Check for the Nodeport here  Sample Output  Name:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : svc , tier : front }, name : vote , namespace : instavote }, spec :{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                      unset   80/TCP\nTargetPort:               80/TCP\nNodePort:                  unset   31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                    none   Go to browser and check hostip:NodePort  Here the node port is 31429.  Sample output will be:", 
            "title": "Writing Service Spec"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#exposing-the-app-with-externalip", 
            "text": "spec:\n  selector:\n    role: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n  externalIPs:\n    - xx.xx.xx.xx\n    - yy.yy.yy.yy  Where  replace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts.  apply  kubectl  get svc\nkubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote  [sample output]  NAME      TYPE       CLUSTER-IP      EXTERNAL-IP                    PORT(S)        AGE\nvote      NodePort   10.107.71.204   206.189.150.190,159.65.8.227   80:30000/TCP   11m  where,  EXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http:// :  to access this application.  e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on.", 
            "title": "Exposing the app with ExternalIP"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#internal-service-discovery", 
            "text": "Visit the vote app from browser  Attemp to vote by clicking on one of the options   observe what happens. Does it go through?    Debugging,  kubectl get pod\nkubectl exec vote-xxxx ping redis  [replace xxxx with the actual pod id of one of the vote pods ]  keep the above command on a watch. You should create a new terminal to run the watch command.  e.g.  watch  kubectl exec vote-kvc7j ping redis  where, vote-kvc7j is one of the vote pods that I am running. Replace this with the actual pod id.  Now create  redis  service  kubectl apply -f redis-svc.yaml\n\nkubectl get svc\n\nkubectl describe svc redis  Watch the ping and observe if its able to resolve  redis  by hostname and its pointing to an IP address.  e.g.  PING redis (10.102.77.6): 56 data bytes  where  10.102.77.6  is the ClusterIP assigned to the service.    What happened here?   Service  redis  was created with a ClusterIP e.g. 10.102.77.6  A DNS entry was created for this service. The fqdn of the service is  redis.instavote.svc.cluster.local  and it takes the form of my-svc.my-namespace.svc.cluster.local  Each pod points to  internal  DNS server running in the cluster. You could see the details of this by running the following commands   kubectl exec vote-xxxx cat /etc/resolv.conf  [replace vote-xxxx with actual pod id]  [sample output]  nameserver 10.96.0.10\nsearch instavote.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5  where  10.96.0.10  is the ClusterIP assigned to the DNS service. You could co relate that with,  kubectl get svc -n kube-system\n\n\nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkube-dns               ClusterIP   10.96.0.10      none         53/UDP,53/TCP   1h\nkubernetes-dashboard   NodePort    10.104.42.73    none         80:31000/TCP    23m  where,  10.96.0.10  is the ClusterIP assigned to  kube-dns  and matches the configuration in  /etc/resolv.conf  above.", 
            "title": "Internal Service Discovery"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#creating-endpoints-for-redis", 
            "text": "Service is been created, but you still need to launch the actual pods running  redis  application.  Create the endpoints now,  kubectl apply -f redis-deploy.yaml\nkubectl describe svc redis  [sample output]  Name:              redis\nNamespace:         instavote\nLabels:            role=redis\n                   tier=back\nAnnotations:       kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : redis , tier : back }, name : redis , namespace : instavote }, spec ...\nSelector:          app=redis\nType:              ClusterIP\nIP:                10.102.77.6\nPort:               unset   6379/TCP\nTargetPort:        6379/TCP\nEndpoints:         10.32.0.6:6379,10.46.0.6:6379\nSession Affinity:  None\nEvents:             none   Again, visit the vote app from browser, attempt to register your vote  and observe what happens now.", 
            "title": "Creating Endpoints for Redis"
        }, 
        {
            "location": "/7-vote-exposing_app_with_service/#reading", 
            "text": "Debugging  \n  * Services \n  *  Kubernetes Services Documentation \n  *  Service API Specs for Kubernetes Version 1.10", 
            "title": "Reading"
        }, 
        {
            "location": "/6-vote-kubernetes_deployment/", 
            "text": "Creating a Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nDeployment has mainly two responsibilities,\n\n\n\n\nProvide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n\n\nUpdate Strategy: Define a release strategy and update the pods accordingly.\n\n\n\n\n/k8s-code/projects/instavote/dev/\ncp vote-rs.yaml vote-deploy.yaml\n\n\n\n\nDeployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows,\n\n\nFile: vote-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 8\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v2\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v2\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nThis time, start monitoring with --show-labels options added.\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc --show-labels\n\n\n\n\nLets  create the Deployment. Do monitor the labels of the pod while applying this.\n\n\nkubectl apply -f vote-deploy.yaml\n\n\n\n\nObserve the chances to pod labels, specifically the \npod-template-hash\n.\n\n\nNow that the deployment is created. To validate,\n\n\nkubectl get deployment\nkubectl get rs --show-labels\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/vote --replicas=12\n\nkubectl rollout status deployment/vote\n\n\n\n\n\nSample output:\n\n\n\nWaiting for rollout to finish: 5 of 12 updated replicas are available...\nWaiting for rollout to finish: 6 of 12 updated replicas are available...\ndeployment \nvote\n successfully rolled out\n\n\n\n\nYou could also update the deployment by editing it.\n\n\nkubectl edit deploy/vote\n\n\n\n\n[change replicas to 15 from the editor, save and observe]\n\n\nRolling Updates in Action\n\n\nNow, update the deployment spec to apply\n\n\nfile: vote-deploy.yaml\n\n\nspec:\n...\n  replicas: 15\n...\nlabels:\n   app: python\n   role: vote\n   version: v3\n...\ntemplate:   \n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v3\n\n\n\n\n\napply\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl rollout status deployment/vote\n\n\n\n\nObserve rollout status and monitoring screen.\n\n\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=1\n\n\n\n\n\nUndo and Rollback\n\n\nfile: vote-deploy.yaml\n\n\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:rgjerdf\n\n\n\n\n\napply\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl rollout status\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=xx\n\n\n\n\nwhere replace xxx with revisions\n\n\nFind out the previous revision with sane configs.\n\n\nTo undo to a sane version (for example revision 3)\n\n\nkubectl rollout undo deploy/vote --to-revision=3", 
            "title": "Defining Update Strategy"
        }, 
        {
            "location": "/6-vote-kubernetes_deployment/#creating-a-deployment", 
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Deployment has mainly two responsibilities,   Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.  Update Strategy: Define a release strategy and update the pods accordingly.   /k8s-code/projects/instavote/dev/\ncp vote-rs.yaml vote-deploy.yaml  Deployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows,  File: vote-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 8\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v2\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v2\n          ports:\n            - containerPort: 80\n              protocol: TCP  This time, start monitoring with --show-labels options added.  watch -n 1 kubectl get  pod,deploy,rs,svc --show-labels  Lets  create the Deployment. Do monitor the labels of the pod while applying this.  kubectl apply -f vote-deploy.yaml  Observe the chances to pod labels, specifically the  pod-template-hash .  Now that the deployment is created. To validate,  kubectl get deployment\nkubectl get rs --show-labels\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/6-vote-kubernetes_deployment/#scaling-a-deployment", 
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/vote --replicas=12\n\nkubectl rollout status deployment/vote  Sample output:  \nWaiting for rollout to finish: 5 of 12 updated replicas are available...\nWaiting for rollout to finish: 6 of 12 updated replicas are available...\ndeployment  vote  successfully rolled out  You could also update the deployment by editing it.  kubectl edit deploy/vote  [change replicas to 15 from the editor, save and observe]", 
            "title": "Scaling a deployment"
        }, 
        {
            "location": "/6-vote-kubernetes_deployment/#rolling-updates-in-action", 
            "text": "Now, update the deployment spec to apply  file: vote-deploy.yaml  spec:\n...\n  replicas: 15\n...\nlabels:\n   app: python\n   role: vote\n   version: v3\n...\ntemplate:   \n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v3  apply  kubectl apply -f vote-deploy.yaml\n\nkubectl rollout status deployment/vote  Observe rollout status and monitoring screen.  \nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=1", 
            "title": "Rolling Updates in Action"
        }, 
        {
            "location": "/6-vote-kubernetes_deployment/#undo-and-rollback", 
            "text": "file: vote-deploy.yaml  spec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:rgjerdf  apply  kubectl apply -f vote-deploy.yaml\n\nkubectl rollout status\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=xx  where replace xxx with revisions  Find out the previous revision with sane configs.  To undo to a sane version (for example revision 3)  kubectl rollout undo deploy/vote --to-revision=3", 
            "title": "Undo and Rollback"
        }, 
        {
            "location": "/11_deploying_sample_app/", 
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all components/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier applicable\n\n\nLaunch/apply the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nready\n\n\nready\n\n\n\n\n\n\nworker\n\n\nTODO\n\n\nn/a\n\n\n\n\n\n\ndb\n\n\nready\n\n\nready\n\n\n\n\n\n\nresults\n\n\nTODO\n\n\nTODO\n\n\n\n\n\n\n\n\nSpecs:\n\n\n\n\nworker\n\n\nimage: schoolofdevops/worker:latest\n\n\n\n\n\n\nresults\n\n\nimage: schoolofdevops/vote-result\n\n\nport: 80\n\n\nservice type: NodePort\n\n\n\n\n\n\n\n\nDeploying the sample application\n\n\nTo create deploy the sample applications,\n\n\nkubectl create -f projects/instavote/dev\n\n\n\n\nSample output is like:\n\n\ndeployment \ndb\n created\nservice \ndb\n created\ndeployment \nredis\n created\nservice \nredis\n created\ndeployment \nvote\n created\nservice \nvote\n created\ndeployment \nworker\n created\ndeployment \nresults\n created\nservice \nresults\n created\n\n\n\n\nTo Validate:\n\n\nkubectl get svc -n instavote\n\n\n\n\nSample Output is:\n\n\nkubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   \npending\n     80:31808/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:31808\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   \npending\n     80:32511/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:32511\n\n\n\n\n\n\nThis is the page where you should see the results for the vote application stack.", 
            "title": "Mini Project"
        }, 
        {
            "location": "/11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack", 
            "text": "In this project , you would write definitions for deploying the vote application stack with all components/tiers which include,   vote ui  redis  worker  db  results ui", 
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        }, 
        {
            "location": "/11_deploying_sample_app/#tasks", 
            "text": "Create deployments for all applications  Define services for each tier applicable  Launch/apply the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  ready  ready    worker  TODO  n/a    db  ready  ready    results  TODO  TODO     Specs:   worker  image: schoolofdevops/worker:latest    results  image: schoolofdevops/vote-result  port: 80  service type: NodePort", 
            "title": "Tasks"
        }, 
        {
            "location": "/11_deploying_sample_app/#deploying-the-sample-application", 
            "text": "To create deploy the sample applications,  kubectl create -f projects/instavote/dev  Sample output is like:  deployment  db  created\nservice  db  created\ndeployment  redis  created\nservice  redis  created\ndeployment  vote  created\nservice  vote  created\ndeployment  worker  created\ndeployment  results  created\nservice  results  created", 
            "title": "Deploying the sample application"
        }, 
        {
            "location": "/11_deploying_sample_app/#to-validate", 
            "text": "kubectl get svc -n instavote  Sample Output is:  kubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243    pending      80:31808/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:31808   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16    pending      80:32511/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:32511   This is the page where you should see the results for the vote application stack.", 
            "title": "To Validate:"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/", 
            "text": "Configmap\n\n\nConfigmap is one of the ways to provide configurations to your application.\n\n\nInjecting env variables with configmaps\n\n\nCreate our configmap for vote app\n\n\nfile:  projects/instavote/dev/vote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: Visa\n  OPTION_B: Mastercard\n\n\n\n\nIn the above given configmap, we define two environment variables,\n\n\n\n\nOPTION_A=EMACS\n\n\nOPTION_B=VI\n\n\n\n\nLets create the configmap object\n\n\nkubectl get cm\nkubectl apply -f vote-cm.yaml\nkubectl get cm\nkubectl describe cm vote\n\n\n\n\n\n\nIn order to use this configmap in the deployment, we need to reference it from the deployment file.\n\n\nCheck the deployment file for vote add for the following block.\n\n\nfile: \nvote-deploy.yaml\n\n\n...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always\n\n\n\n\nSo when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code.\n\n\nkubectl apply -f vote-deploy.yaml\n\n\n\n\nWatch the monitoring screen for deployment in progress.\n\n\nkubectl get deploy --show-labels\nkubectl get rs --show-labels\nkubectl  rollout status deploy/vote\n\n\n\n\n\n\n\nProviding environment Specific Configs\n\n\nCopy the dev env to staging\n\n\ncd k8s-code/projects/instavote\ncp -r dev staging\n\n\n\n\nEdit the configurations   for staging\n\n\ncd staging\n\n\n\n\n\nEdit \nvote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\ndata:\n  OPTION_A: Apple\n  OPTION_B: Samsung\n\n\n\n\nEdit  \nvote-svc.yaml\n\n\n\n\nremove namespace if set\n\n\nremove extIP\n\n\nremove hard coded nodePort config if any\n\n\n\n\nfile: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n  type: NodePort\n\n\n\n\nEdit \nvote-deploy.yaml\n\n\n\n\nremove namespace if set\n\n\nchange replicas to 2   \n\n\n\n\nLets launch it all in another namespace. We could use \ndefault\n namespace for this purpose.\n\n\nkubectl config set-context $(kubectl config  current-context) --namespace=default\n\n\n\n\nVerify the namespace has been switched by observing the monitoring screen.\n\n\nDeploy new objects in this nmespace\n\n\nkubectl apply -f vote-svc.yaml\nkubectl apply -f vote-cm.yaml\nkubectl apply -f vote-deploy.yaml\n\n\n\n\n\nNow connect to the vote service by finding out the nodePort configs\n\n\nkubectl get svc vote\n\n\n\n\n\nTroubleshooting\n\n\n\n\nDo you see the application when you browse to http://host:nodeport\n\n\nIf not, why? Find the root cause and fix it.\n\n\n\n\nClean up and Switch back the  namespace\n\n\nVerify the environment specific options are in effect. Once verified, you could switch the namespace back to instavote.\n\n\nkubectl delete deploy/vote svc/vote\n\nkubectl config set-context $(kubectl config  current-context) --namespace=instavote\n\ncd ../dev/\n\n\n\n\n\nConfiguration file as ConfigMap\n\n\nIn the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.\n\n\nSyntax for consuming file as a configmap is as follows\n\n\n  kubectl create configmap --from-file \nCONF-FILE-PATH\n \nNAME-OF-CONFIGMAP\n\n\n\n\n\nWe have redis configuration as a file named \nprojects/instavote/config/redis.conf\n. We are going to convert this file into a configmap\n\n\ncd k8s-code/projects/instavote/config/\n\nkubectl create configmap --from-file  redis.conf redis\n\n\n\n\nNow validate the configs  \n\n\n\nkubectl get cm\n\nkubectl describe cm redis\n\n\n\n\nTo use this confif file, update your redis-deploy.yaml file to use it by mounting it as a volume.\n\n\nFile: \ndev/redis-deploy.yaml\n\n\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always\n\n\n\n\nAnd apply it\n\n\nkubectl apply -f redis-deploy.yaml\n\nkubectl apply -f redis-svc.yaml\n\nkubectl get rs,deploy --show-labels\n\n\n\n\nExercise:\n Connect to redis pod and verify configs.\n\n\nSecrets\n\n\nSecrets are for storing sensitive data like \npasswords and keychains\n. We will see how db deployment uses username and password in form of a secret.\n\n\nYou would define two fields for db,\n\n\n\n\nusername\n\n\npassword\n\n\n\n\nTo create secrets for db you need to generate  \nbase64\n format as follows,\n\n\necho \nadmin\n | base64\necho \npassword\n | base64\n\n\n\n\nwhere \nadmin\n and \npassword\n are the actual values that you would want to inject into the pod environment.\n\n\nIf you do not have a unix host, you can make use of online base64 utility to generate these strings.\n\n\nhttp://www.utilities-online.info/base64\n\n\n\n\nLets now add it to the secrets file,\n\n\nFile: projects/instavote/dev/db-secrets.yaml\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  POSTGRES_PASSWD: cGFzc3dvcmQ=\n\n\n\n\nkubectl apply -f db-secrets.yaml\n\nkubectl get secrets\n\nkubectl describe secret db\n\n\n\n\nSecrets can be referred to inside a container spec with following syntax\n\n\nenv:\n  - name: VAR\n    valueFrom:\n      secretKeyRef:\n        name: db\n        key: SECRET_VAR\n\n\n\n\nTo consume these secrets, update the deployment for db\n\n\nfile: db-deploy.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        # Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always\n\n\n\n\nTo apply this,\n\n\nkubectl apply -f db-deploy.yaml\n\nkubectl apply -f db-svc.yaml\n\nkubectl get rs,deploy --show-labels\n\n\n\n\nNote: Automatic Updation of deployments on ConfigMap  Updates\n\n\nCurrently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.  \n\n\nThere is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368\n\n\nCurrently, this can be done by using immutable configMaps.  \n\n\n\n\nCreate a configMaps and apply it with deployment.\n\n\nTo update, create a new configMaps and do not update the previous one. Treat it as immutable.\n\n\nUpdate deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Managing Configurations and Secrets"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#configmap", 
            "text": "Configmap is one of the ways to provide configurations to your application.", 
            "title": "Configmap"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#injecting-env-variables-with-configmaps", 
            "text": "Create our configmap for vote app  file:  projects/instavote/dev/vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: Visa\n  OPTION_B: Mastercard  In the above given configmap, we define two environment variables,   OPTION_A=EMACS  OPTION_B=VI   Lets create the configmap object  kubectl get cm\nkubectl apply -f vote-cm.yaml\nkubectl get cm\nkubectl describe cm vote  In order to use this configmap in the deployment, we need to reference it from the deployment file.  Check the deployment file for vote add for the following block.  file:  vote-deploy.yaml  ...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always  So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code.  kubectl apply -f vote-deploy.yaml  Watch the monitoring screen for deployment in progress.  kubectl get deploy --show-labels\nkubectl get rs --show-labels\nkubectl  rollout status deploy/vote", 
            "title": "Injecting env variables with configmaps"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#providing-environment-specific-configs", 
            "text": "Copy the dev env to staging  cd k8s-code/projects/instavote\ncp -r dev staging  Edit the configurations   for staging  cd staging  Edit  vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\ndata:\n  OPTION_A: Apple\n  OPTION_B: Samsung  Edit   vote-svc.yaml   remove namespace if set  remove extIP  remove hard coded nodePort config if any   file: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n  type: NodePort  Edit  vote-deploy.yaml   remove namespace if set  change replicas to 2      Lets launch it all in another namespace. We could use  default  namespace for this purpose.  kubectl config set-context $(kubectl config  current-context) --namespace=default  Verify the namespace has been switched by observing the monitoring screen.  Deploy new objects in this nmespace  kubectl apply -f vote-svc.yaml\nkubectl apply -f vote-cm.yaml\nkubectl apply -f vote-deploy.yaml  Now connect to the vote service by finding out the nodePort configs  kubectl get svc vote  Troubleshooting   Do you see the application when you browse to http://host:nodeport  If not, why? Find the root cause and fix it.", 
            "title": "Providing environment Specific Configs"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#clean-up-and-switch-back-the-namespace", 
            "text": "Verify the environment specific options are in effect. Once verified, you could switch the namespace back to instavote.  kubectl delete deploy/vote svc/vote\n\nkubectl config set-context $(kubectl config  current-context) --namespace=instavote\n\ncd ../dev/", 
            "title": "Clean up and Switch back the  namespace"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#configuration-file-as-configmap", 
            "text": "In the  topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file.  Syntax for consuming file as a configmap is as follows    kubectl create configmap --from-file  CONF-FILE-PATH   NAME-OF-CONFIGMAP   We have redis configuration as a file named  projects/instavote/config/redis.conf . We are going to convert this file into a configmap  cd k8s-code/projects/instavote/config/\n\nkubectl create configmap --from-file  redis.conf redis  Now validate the configs    \nkubectl get cm\n\nkubectl describe cm redis  To use this confif file, update your redis-deploy.yaml file to use it by mounting it as a volume.  File:  dev/redis-deploy.yaml      spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n        volumeMounts:\n          - name: redis\n            subPath: redis.conf\n            mountPath: /etc/redis.conf\n      volumes:\n      - name: redis\n        configMap:\n          name: redis\n      restartPolicy: Always  And apply it  kubectl apply -f redis-deploy.yaml\n\nkubectl apply -f redis-svc.yaml\n\nkubectl get rs,deploy --show-labels  Exercise:  Connect to redis pod and verify configs.", 
            "title": "Configuration file as ConfigMap"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#secrets", 
            "text": "Secrets are for storing sensitive data like  passwords and keychains . We will see how db deployment uses username and password in form of a secret.  You would define two fields for db,   username  password   To create secrets for db you need to generate   base64  format as follows,  echo  admin  | base64\necho  password  | base64  where  admin  and  password  are the actual values that you would want to inject into the pod environment.  If you do not have a unix host, you can make use of online base64 utility to generate these strings.  http://www.utilities-online.info/base64  Lets now add it to the secrets file,  File: projects/instavote/dev/db-secrets.yaml  apiVersion: v1\nkind: Secret\nmetadata:\n  name: db\n  namespace: instavote\ntype: Opaque\ndata:\n  POSTGRES_USER: YWRtaW4=\n  POSTGRES_PASSWD: cGFzc3dvcmQ=  kubectl apply -f db-secrets.yaml\n\nkubectl get secrets\n\nkubectl describe secret db  Secrets can be referred to inside a container spec with following syntax  env:\n  - name: VAR\n    valueFrom:\n      secretKeyRef:\n        name: db\n        key: SECRET_VAR  To consume these secrets, update the deployment for db  file: db-deploy.yaml  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: instavote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: back\n      app: postgres\n  minReadySeconds: 10\n  template:\n    metadata:\n      labels:\n        app: postgres\n        role: db\n        tier: back\n    spec:\n      containers:\n      - image: postgres:9.4\n        imagePullPolicy: Always\n        name: db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        # Secret definition\n        env:\n          - name: POSTGRES_USER\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_USER\n          - name: POSTGRES_PASSWD\n            valueFrom:\n              secretKeyRef:\n                name: db\n                key: POSTGRES_PASSWD\n      restartPolicy: Always  To apply this,  kubectl apply -f db-deploy.yaml\n\nkubectl apply -f db-svc.yaml\n\nkubectl get rs,deploy --show-labels", 
            "title": "Secrets"
        }, 
        {
            "location": "/9-vote-configmaps_and_secrets/#note-automatic-updation-of-deployments-on-configmap-updates", 
            "text": "Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.    There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368  Currently, this can be done by using immutable configMaps.     Create a configMaps and apply it with deployment.  To update, create a new configMaps and do not update the previous one. Treat it as immutable.  Update deployment spec to use the new version of the configMaps. This will ensure immediate update.", 
            "title": "Note: Automatic Updation of deployments on ConfigMap  Updates"
        }, 
        {
            "location": "/vote-persistent-volumes/", 
            "text": "Steps to set up NFS based Persistent Volumes\n\n\nSet up NFS Common\n\n\nOn all kubernetes nodes, if you have not already installed nfs, use the following command to do so\n\n\nsudo apt-get update\nsudo apt-get install nfs-common\n\n\n\n\n[Skip this step if you are using a vagrant setup recommended as part of this course. ]\n\n\nSet up NFS Provisioner in kubernetes\n\n\nChange into nfs provisioner installation dir\n\n\ncd k8s-code/storage\n\n\n\n\nDeploy nfs-client provisioner.\n\n\nkubectl apply -f nfs\n\n\n\n\n\nThis will create all the objects required to setup a nfs provisioner.\n\n\nCreating a Persistent Volume Claim\n\n\nswitch to project directory\n\n\ncd k8s-code/projects/instavote/dev/\n\n\n\n\nfile: db-pvc.yaml\n\n\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: db-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: nfs\n\n\n\n\n\nAnd lets create the Persistent Volume Claim\n\n\nkubectl get pvc,storageclass\n\nkubectl logs -f nfs-provisioner-0\n\nkubectl apply -f db-pvc.yaml\n\nkubectl get pvc,storageclass\n\nkubectl describe pvc db-pvc\n\n\n\n\nfile: db-deploy.yaml\n\n\n...\nspec:\n   containers:\n   - image: postgres:9.4\n     imagePullPolicy: Always\n     name: db\n     ports:\n     - containerPort: 5432\n       protocol: TCP\n     #mount db-vol to postgres data path\n     volumeMounts:\n     - name: db-vol\n       mountPath: /var/lib/postgresql/data\n   #create a volume with pvc\n   volumes:\n   - name: db-vol\n     persistentVolumeClaim:\n       claimName: db-pvc\n\n\n\n\nObserve which host \ndb\n pod is running on\n\n\nkubectl get pod -o wide --selector='role=db'\n\n\n\n\n\nAnd apply this code as\n\n\nkubectl apply -f db-deploy.yaml\n\nkubectl get pod -o wide --selector='role=db'\n\n\n\n\n\n\n\nObserve the volume and its content created on the nfs server\n\n\nObserve which host the pod for \ndb\n was created this time. Analyse the behavior of a deployment controller.", 
            "title": "Making Data Persist"
        }, 
        {
            "location": "/vote-persistent-volumes/#steps-to-set-up-nfs-based-persistent-volumes", 
            "text": "", 
            "title": "Steps to set up NFS based Persistent Volumes"
        }, 
        {
            "location": "/vote-persistent-volumes/#set-up-nfs-common", 
            "text": "On all kubernetes nodes, if you have not already installed nfs, use the following command to do so  sudo apt-get update\nsudo apt-get install nfs-common  [Skip this step if you are using a vagrant setup recommended as part of this course. ]", 
            "title": "Set up NFS Common"
        }, 
        {
            "location": "/vote-persistent-volumes/#set-up-nfs-provisioner-in-kubernetes", 
            "text": "Change into nfs provisioner installation dir  cd k8s-code/storage  Deploy nfs-client provisioner.  kubectl apply -f nfs  This will create all the objects required to setup a nfs provisioner.", 
            "title": "Set up NFS Provisioner in kubernetes"
        }, 
        {
            "location": "/vote-persistent-volumes/#creating-a-persistent-volume-claim", 
            "text": "switch to project directory  cd k8s-code/projects/instavote/dev/  file: db-pvc.yaml  kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: db-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: nfs  And lets create the Persistent Volume Claim  kubectl get pvc,storageclass\n\nkubectl logs -f nfs-provisioner-0\n\nkubectl apply -f db-pvc.yaml\n\nkubectl get pvc,storageclass\n\nkubectl describe pvc db-pvc  file: db-deploy.yaml  ...\nspec:\n   containers:\n   - image: postgres:9.4\n     imagePullPolicy: Always\n     name: db\n     ports:\n     - containerPort: 5432\n       protocol: TCP\n     #mount db-vol to postgres data path\n     volumeMounts:\n     - name: db-vol\n       mountPath: /var/lib/postgresql/data\n   #create a volume with pvc\n   volumes:\n   - name: db-vol\n     persistentVolumeClaim:\n       claimName: db-pvc  Observe which host  db  pod is running on  kubectl get pod -o wide --selector='role=db'  And apply this code as  kubectl apply -f db-deploy.yaml\n\nkubectl get pod -o wide --selector='role=db'   Observe the volume and its content created on the nfs server  Observe which host the pod for  db  was created this time. Analyse the behavior of a deployment controller.", 
            "title": "Creating a Persistent Volume Claim"
        }, 
        {
            "location": "/kubespray-prereqs/", 
            "text": "Provisioning HA Lab Cluster  with Vagrant\n\n\nVagrant Setup:\n\n\nThis tutorial assumes you have Vagrant+VirtualBox setup. While Vagrant is used for basic infrastructure requirements, the lessons learned in this tutorial can be applied to other platforms. Start from \nSet up Kubernetes Using Kubespray\n(or) Refer to this \nDocument\n, if you have VMs running elsewhere\n\n\nSoftware Requirements on Host Machine:\n\n\n\n\nVirtual Box (latest)\n\n\nVagrant (latest)\n\n\nGit Bash (Only for Windows)\n\n\nConemu (Only for Windows)\n\n\n\n\nSet up Learning Environment:\n\n\nSetup the repo\n\n\ngit clone https://github.com/schoolofdevops/kubespray-1\n\n\n\n\n\nBring up the VMs\n\n\ncd kubespray-1\n\nvagrant up\n\nvagrant status\n\n\n\n\nLogin to nodes\n\n\nOpen four different terminals to login to 4 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh k8s-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh k8s-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh k8s-03\nsudo su\n\n\n\n\nTerminal 4\n\n\nvagrant ssh k8s-04\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nProduction Grade Setup with Kubespray", 
            "title": "Kubespray HA lab setup with Vagrant"
        }, 
        {
            "location": "/kubespray-prereqs/#provisioning-ha-lab-cluster-with-vagrant", 
            "text": "", 
            "title": "Provisioning HA Lab Cluster  with Vagrant"
        }, 
        {
            "location": "/kubespray-prereqs/#vagrant-setup", 
            "text": "This tutorial assumes you have Vagrant+VirtualBox setup. While Vagrant is used for basic infrastructure requirements, the lessons learned in this tutorial can be applied to other platforms. Start from  Set up Kubernetes Using Kubespray (or) Refer to this  Document , if you have VMs running elsewhere", 
            "title": "Vagrant Setup:"
        }, 
        {
            "location": "/kubespray-prereqs/#software-requirements-on-host-machine", 
            "text": "Virtual Box (latest)  Vagrant (latest)  Git Bash (Only for Windows)  Conemu (Only for Windows)", 
            "title": "Software Requirements on Host Machine:"
        }, 
        {
            "location": "/kubespray-prereqs/#set-up-learning-environment", 
            "text": "Setup the repo  git clone https://github.com/schoolofdevops/kubespray-1  Bring up the VMs  cd kubespray-1\n\nvagrant up\n\nvagrant status  Login to nodes  Open four different terminals to login to 4 nodes created with above command  Terminal 1  vagrant ssh k8s-01\nsudo su  Terminal 2  vagrant ssh k8s-02\nsudo su  Terminal 3  vagrant ssh k8s-03\nsudo su  Terminal 4  vagrant ssh k8s-04\nsudo su  Once the environment is setup, follow  Production Grade Setup with Kubespray", 
            "title": "Set up Learning Environment:"
        }, 
        {
            "location": "/cluster_setup_kubespray/", 
            "text": "High Available Kubernetes Cluster Setup using Kubespray\n\n\nKubespray is an \nAnsible\n based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.\n\n\nPrerequisites\n\n\nHardware Pre requisites\n\n\n\n\n4 Nodes: Virtual/Physical Machines  \n\n\nMemory: 2GB   \n\n\nCPU: 1 Core\n\n\nHard disk: 20GB available  \n\n\n\n\nSoftware Pre Requisites\n\n\nOn All Nodes\n\n\n\n\nUbuntu 16.04 OS  \n\n\nPython  \n\n\nSSH Server\n\n\nPrivileged user\n\n\n\n\nOn Ansible Control  Node\n    \n\n\n\n\nAnsible version  2.4 or greater  \n\n\nJinja  \n\n\n\n\nNetworking Pre Requisites\n\n\n\n\nInternet access to download docker images and install softwares\n\n\nIPv4 Forwarding should be enabled  \n\n\nFirewall should allow ssh access as well as ports required by Kubernetes. Internally open all the ports between node.\n\n\n\n\nArchitecture of a high available kubernetes cluster\n\n\nPreparing the nodes\n\n\nRun instructions in the section \nOn all nodes\n in the cluster. This includes Ansible controller too.\n\n\nInstall Python\n\n\nAnsible needs python to be installed on all the machines.\n\n\nsudo apt update\nsudo apt install python\n\n\n\n\nEnable IPv4 Forwarding\n\n\nOn all nodes\n\n\nEnalbe IPv4 forwarding by uncommenting the following line\n\n\necho \nnet.ipv4.ip_forward=1\n \n /etc/sysctl.conf\n\n\n\n\nDisable Swap\n\n\nswapoff -a\n\n\n\n\nSetup passwordless SSH between ansible controller and kubernetes nodes\n\n\nOn control node\n\n\nAnsible uses passwordless ssh\n1\n to create the cluster. Let us see how to set it up from your \ncontrol node\n.\n\n\nGenerate ssh keypair if not present already using the following command.\n\n\nssh-keygen -t rsa\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+\n\n\n\n\nJust leave the fields to defaults. This command will generate a public key and private key for you.\n\n\nCopy over the public key to all nodes.\n\n\nExample,  assuming \nubuntu\n as the user which has a privileged access on the node with ip address \n10.10.1.101\n,\n\n\nssh-copy-id ubuntu@10.10.1.101\n\n\n\n\nThis will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace \n10.40.1.26\n with your respective machine's IP.\n\n\ne.g.\n\n\nssh ubuntu@10.10.1.101\n\n\n\n\nMake sure to copy the public key to all kubernetes nodes. \nReplace username with the actual user on your system\n.\nIf the above mentioned command fails, then copy your public key and paste it in the remote machine's \n~/.ssh/authorized_keys\n file.\n\n\ne.g. (Only if ssh-copy-id fails)\n\n\ncat ~/.ssh/id_rsa.pub\nssh ubunut@10.10.1.101\nvim ~/.ssh/authorized_keys\n# Paste the public key\n\n\n\n\nSetup Ansible Control node and Kubespray\n\n\nOn control node\n\n\nSet Locale\n\n\nexport LC_ALL=\nen_US.UTF-8\n\nexport LC_CTYPE=\nen_US.UTF-8\n\nsudo dpkg-reconfigure locales\n\n\n\n\nDo no select any other locale in the menu. Just press (\nOK\n) in the next two screens.\n\n\nSetup kubespray\n\n\nKubespray is hosted on GitHub. Let us the clone the \nofficial repository\n.\n\n\ngit clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray\n\n\n\n\nInstall Prerequisites\n\n\nInstall the python dependencies. \nThis step installs Ansible as well. You do not need to install Ansible separately\n.\n\n\nsudo apt install python-pip -y\nsudo pip install -r requirements.txt\n\n\n\n\nSet Remote User for Ansible\n\n\nAdd the following section in ansible.cfg file\n\n\nremote_user=ubuntu\n\n\n\n\nIf the user you are going to connect is differnt, use that instead.\n\n\nYour \nansible.cfg\n file should look like this.\n\n\n[ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu\n\n\n\n\nCreate Inventory\n\n\ncp -rfp inventory/sample inventory/prod\n\n\n\n\nwhere \nprod\n is the custom configuration name. Replace is with whatever name you would like to assign to the current cluster.\n\n\nTo build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments\n\n\nCONFIG_FILE=inventory/prod/hosts.ini python3 contrib/inventory_builder/inventory.py 10.10.1.101 10.10.1.102 10.10.1.103 10.10.1.104\n\n\n\n\nWhere replace the IP addresses (e.g. 10.10.1.101) with the actual IPs of your nodes\n\n\nOnce its run, you should see an inventory file generated which may look similar to below\n\n\nfile: inventory/prod/hosts.ini\n\n\n[all]\nnode1    ansible_host=10.10.1.101 ip=10.10.1.101\nnode2    ansible_host=10.10.1.102 ip=10.10.1.102\nnode3    ansible_host=10.10.1.103 ip=10.10.1.103\nnode4    ansible_host=10.10.1.104 ip=10.10.1.104\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3\n\n\n\n\nCustomise Kubernetes Cluster Configs\n\n\nThere are two configs files in your inventroy directory's group_vars (e.g. inventory/prod/group_vars) viz.\n\n\n\n\nall.yml  \n\n\nk8s-cluster.yml\n\n\n\n\nAnsible is data driven, and most of the configurations of the cluster can be tweaked by changing the variable values from the above files.\n\n\nFew of the configurations you may want to modify\n\n\nfile: inventory/prod/group_vars/k8s-cluster.yml\n\n\nkubelet_max_pods: 100\ncluster_name: prod\nhelm_enabled: true\n\n\n\n\nProvisioning  kubernetes cluster with kubespray\n\n\nOn control node\n\n\nWe are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.\n\n\nansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml\n\n\n\n\nOption -i = Inventory file path\n\nOption -b = Become as root user\n\nOption -v = Give verbose output  \n\n\nIf you face this following error, while running \nansible-playbook\n command, you can fix it by running following instructions\n\n\nERROR\n:  \n\n\nERROR! Unexpected Exception, this is probably a bug: (cryptography 1.2.3 (/usr/lib/python3/dist-packages), Requirement.parse('cryptography\n=1.5'), {'paramiko'})\n\n\n\n\nFIX\n:  \n\n\nsudo pip install --upgrade pip\nsudo pip uninstall cryptography\nsudo pip install cryptography\nansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml\n\n\n\n\nThis Ansible run will take around 30 mins to complete.\n\n\nKubectl Configs\n\n\nOn kube master node\n\n\nOnce the cluster setup is done, copy the configuration and setup the permissions.\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nCheck the State of the Cluster\n\n\nOn the node where kubectl is setup\n\n\nLet us check the state of the cluster by running,\n\n\nkubectl cluster-info\n\nKubernetes master is running at https://10.10.1.101:6443\nKubeDNS is running at https://10.10.1.101:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n\n\n\nkubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0\n\n\n\n\nIf you are able to see this, your cluster has been set up successfully.\n\n\n\n\n1\n You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.\n\n\nAccess Kubernetes Cluster Remotely (Optional)\n\n\nOn your local machine\n\n\nYou could also install kubectl on your laptop/workstation. To learn how to install it for your OS,   \nrefer to the  procedure here\n.\n\n\ne.g.\nTo install \nkubectl\n on Ubuntu,\n\n\nsudo apt-get update \n sudo apt-get install -y apt-transport-https\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo\n\napt-key add -\n\nsudo touch /etc/apt/sources.list.d/kubernetes.list\n\necho \ndeb http://apt.kubernetes.io/ kubernetes-xenial main\n | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\n\nsudo apt-get install -y kubectl\n\n\n\n\nCopy kubernetes config to your local machine\n\n\nCopy \nkubeconfig\n file to your local machine\n\n\nmkdir ~/.kube\nscp -r ubuntu@MASTER_HOST_IP:/etc/kubernetes/admin.conf  ~/.kube/config\n\nkubectl get nodes\n\n\n\n\n\nDeploy Kubernetes Objects\n\n\nSince its a new cluster, which is differnt than what you have created with kubeadm earlier, or if this is the first time you are creating a  kubernetes cluster with kubespray as part of \nAdvanced Workshop\n, you need to deploy services which have been covered as part of the previous topics.  \n\n\nIn order to do that, use the following commands on the node where you have configured kubectl\n\n\ngit clone https://github.com/schoolofdevops/k8s-code.git\n\ncd k8s-code/projects/instavote\n\nkubectl apply -f instavote-ns.yaml\nkubectl apply -f prod/\n\n\n\n\nSwitch to \ninstavote\n namespace  and  validate,\n\n\n\nkubectl config set-context $(kubectl config current-context)  --namespace=instavote\n\nkubectl get pods,deploy,svc\n\n\n\n\nwhere,\n\n\n\n\n--cluster=prod : prod is the cluter name you created earlier. If not, use the correct name of the cluster ( kubectl config view)\n\n\n--user=admin-prod: is the admin user created by default while installing with kubespray\n\n\n--namespace=instavote : the namespace you just created to deploy instavote app stack\n\n\n\n\n[sample output]\n\n\n$ kubectl get pods,deploy,svc\n\nNAME                          READY     STATUS    RESTARTS   AGE\npod/db-66496667c9-qggzd       1/1       Running   0          7m\npod/redis-6555998885-4k5cr    1/1       Running   0          7m\npod/redis-6555998885-fb8rk    1/1       Running   0          7m\npod/result-5c7569bcb7-4fptr   1/1       Running   0          7m\npod/result-5c7569bcb7-s4rdx   1/1       Running   0          7m\npod/vote-5d88d47fc8-gbzbq     1/1       Running   0          7m\npod/vote-5d88d47fc8-q4vj6     1/1       Running   0          7m\npod/worker-7c98c96fb4-7tzzw   1/1       Running   0          7m\n\nNAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/db       1         1         1            1           7m\ndeployment.extensions/redis    2         2         2            2           7m\ndeployment.extensions/result   2         2         2            2           7m\ndeployment.extensions/vote     2         2         2            2           7m\ndeployment.extensions/worker   1         1         1            1           7m\n\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/db       ClusterIP   10.233.16.207   \nnone\n        5432/TCP       7m\nservice/redis    ClusterIP   10.233.14.61    \nnone\n        6379/TCP       7m\nservice/result   NodePort    10.233.22.10    \nnone\n        80:30100/TCP   7m\nservice/vote     NodePort    10.233.19.111   \nnone\n        80:30000/TCP   7m\n\n\n\n\nReferences\n\n\n\n\nInstalling Kubernetes On Premises/On Cloud with Kubespray \n  \n\n\nKubespray on Github", 
            "title": "HA setup with Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#high-available-kubernetes-cluster-setup-using-kubespray", 
            "text": "Kubespray is an  Ansible  based kubernetes provisioner. It helps us to setup a production grade, highly available and highly scalable Kubernetes cluster.", 
            "title": "High Available Kubernetes Cluster Setup using Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#hardware-pre-requisites", 
            "text": "4 Nodes: Virtual/Physical Machines    Memory: 2GB     CPU: 1 Core  Hard disk: 20GB available", 
            "title": "Hardware Pre requisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#software-pre-requisites", 
            "text": "On All Nodes   Ubuntu 16.04 OS    Python    SSH Server  Privileged user   On Ansible Control  Node        Ansible version  2.4 or greater    Jinja", 
            "title": "Software Pre Requisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#networking-pre-requisites", 
            "text": "Internet access to download docker images and install softwares  IPv4 Forwarding should be enabled    Firewall should allow ssh access as well as ports required by Kubernetes. Internally open all the ports between node.", 
            "title": "Networking Pre Requisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#architecture-of-a-high-available-kubernetes-cluster", 
            "text": "", 
            "title": "Architecture of a high available kubernetes cluster"
        }, 
        {
            "location": "/cluster_setup_kubespray/#preparing-the-nodes", 
            "text": "Run instructions in the section  On all nodes  in the cluster. This includes Ansible controller too.", 
            "title": "Preparing the nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-python", 
            "text": "Ansible needs python to be installed on all the machines.  sudo apt update\nsudo apt install python", 
            "title": "Install Python"
        }, 
        {
            "location": "/cluster_setup_kubespray/#enable-ipv4-forwarding", 
            "text": "On all nodes  Enalbe IPv4 forwarding by uncommenting the following line  echo  net.ipv4.ip_forward=1    /etc/sysctl.conf  Disable Swap  swapoff -a", 
            "title": "Enable IPv4 Forwarding"
        }, 
        {
            "location": "/cluster_setup_kubespray/#setup-passwordless-ssh-between-ansible-controller-and-kubernetes-nodes", 
            "text": "On control node  Ansible uses passwordless ssh 1  to create the cluster. Let us see how to set it up from your  control node .  Generate ssh keypair if not present already using the following command.  ssh-keygen -t rsa\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:yC4Tl6RYc+saTPcLKFdGlTLOWOIuDgO1my/NrMBnRxA ubuntu@node1\nThe key's randomart image is:\n+---[RSA 2048]----+\n|   E    ..       |\n|  . o +..        |\n| . +o*+o         |\n|. .o+Bo+         |\n|. .++.X S        |\n|+ +ooX .         |\n|.=.OB.+ .        |\n| .=o*= . .       |\n|  .o.   .        |\n+----[SHA256]-----+  Just leave the fields to defaults. This command will generate a public key and private key for you.  Copy over the public key to all nodes.  Example,  assuming  ubuntu  as the user which has a privileged access on the node with ip address  10.10.1.101 ,  ssh-copy-id ubuntu@10.10.1.101  This will copy our newly generated public key to the remote machine. After running this command you will be able to SSH into the machine directly without using a password. Replace  10.40.1.26  with your respective machine's IP.  e.g.  ssh ubuntu@10.10.1.101  Make sure to copy the public key to all kubernetes nodes.  Replace username with the actual user on your system .\nIf the above mentioned command fails, then copy your public key and paste it in the remote machine's  ~/.ssh/authorized_keys  file.  e.g. (Only if ssh-copy-id fails)  cat ~/.ssh/id_rsa.pub\nssh ubunut@10.10.1.101\nvim ~/.ssh/authorized_keys\n# Paste the public key", 
            "title": "Setup passwordless SSH between ansible controller and kubernetes nodes"
        }, 
        {
            "location": "/cluster_setup_kubespray/#setup-ansible-control-node-and-kubespray", 
            "text": "On control node", 
            "title": "Setup Ansible Control node and Kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#set-locale", 
            "text": "export LC_ALL= en_US.UTF-8 \nexport LC_CTYPE= en_US.UTF-8 \nsudo dpkg-reconfigure locales  Do no select any other locale in the menu. Just press ( OK ) in the next two screens.", 
            "title": "Set Locale"
        }, 
        {
            "location": "/cluster_setup_kubespray/#setup-kubespray", 
            "text": "Kubespray is hosted on GitHub. Let us the clone the  official repository .  git clone https://github.com/kubernetes-incubator/kubespray.git\ncd kubespray", 
            "title": "Setup kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#install-prerequisites", 
            "text": "Install the python dependencies.  This step installs Ansible as well. You do not need to install Ansible separately .  sudo apt install python-pip -y\nsudo pip install -r requirements.txt", 
            "title": "Install Prerequisites"
        }, 
        {
            "location": "/cluster_setup_kubespray/#set-remote-user-for-ansible", 
            "text": "Add the following section in ansible.cfg file  remote_user=ubuntu  If the user you are going to connect is differnt, use that instead.  Your  ansible.cfg  file should look like this.  [ssh_connection]\npipelining=True\nssh_args = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\n#control_path = ~/.ssh/ansible-%%r@%%h:%%p\n[defaults]\nhost_key_checking=False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /tmp\nstdout_callback = skippy\nlibrary = ./library\ncallback_whitelist = profile_tasks\nroles_path = roles:$VIRTUAL_ENV/usr/local/share/kubespray/roles:$VIRTUAL_ENV/usr/local/share/ansible/roles\ndeprecation_warnings=False\nremote_user=ubuntu", 
            "title": "Set Remote User for Ansible"
        }, 
        {
            "location": "/cluster_setup_kubespray/#create-inventory", 
            "text": "cp -rfp inventory/sample inventory/prod  where  prod  is the custom configuration name. Replace is with whatever name you would like to assign to the current cluster.  To build the inventory file, execute the inventory script along with the IP addresses of our cluster as arguments  CONFIG_FILE=inventory/prod/hosts.ini python3 contrib/inventory_builder/inventory.py 10.10.1.101 10.10.1.102 10.10.1.103 10.10.1.104  Where replace the IP addresses (e.g. 10.10.1.101) with the actual IPs of your nodes  Once its run, you should see an inventory file generated which may look similar to below  file: inventory/prod/hosts.ini  [all]\nnode1    ansible_host=10.10.1.101 ip=10.10.1.101\nnode2    ansible_host=10.10.1.102 ip=10.10.1.102\nnode3    ansible_host=10.10.1.103 ip=10.10.1.103\nnode4    ansible_host=10.10.1.104 ip=10.10.1.104\n\n[kube-master]\nnode1\nnode2\n\n[kube-node]\nnode1\nnode2\nnode3\nnode4\n\n[etcd]\nnode1\nnode2\nnode3\n\n[k8s-cluster:children]\nkube-node\nkube-master\n\n[calico-rr]\n\n[vault]\nnode1\nnode2\nnode3", 
            "title": "Create Inventory"
        }, 
        {
            "location": "/cluster_setup_kubespray/#customise-kubernetes-cluster-configs", 
            "text": "There are two configs files in your inventroy directory's group_vars (e.g. inventory/prod/group_vars) viz.   all.yml    k8s-cluster.yml   Ansible is data driven, and most of the configurations of the cluster can be tweaked by changing the variable values from the above files.  Few of the configurations you may want to modify  file: inventory/prod/group_vars/k8s-cluster.yml  kubelet_max_pods: 100\ncluster_name: prod\nhelm_enabled: true", 
            "title": "Customise Kubernetes Cluster Configs"
        }, 
        {
            "location": "/cluster_setup_kubespray/#provisioning-kubernetes-cluster-with-kubespray", 
            "text": "On control node  We are set to provision the cluster. Run the following ansible-playbook command to provision our Kubernetes cluster.  ansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml  Option -i = Inventory file path \nOption -b = Become as root user \nOption -v = Give verbose output    If you face this following error, while running  ansible-playbook  command, you can fix it by running following instructions  ERROR :    ERROR! Unexpected Exception, this is probably a bug: (cryptography 1.2.3 (/usr/lib/python3/dist-packages), Requirement.parse('cryptography =1.5'), {'paramiko'})  FIX :    sudo pip install --upgrade pip\nsudo pip uninstall cryptography\nsudo pip install cryptography\nansible-playbook -b -v -i inventory/prod/hosts.ini cluster.yml  This Ansible run will take around 30 mins to complete.", 
            "title": "Provisioning  kubernetes cluster with kubespray"
        }, 
        {
            "location": "/cluster_setup_kubespray/#kubectl-configs", 
            "text": "On kube master node  Once the cluster setup is done, copy the configuration and setup the permissions.  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Kubectl Configs"
        }, 
        {
            "location": "/cluster_setup_kubespray/#check-the-state-of-the-cluster", 
            "text": "On the node where kubectl is setup  Let us check the state of the cluster by running,  kubectl cluster-info\n\nKubernetes master is running at https://10.10.1.101:6443\nKubeDNS is running at https://10.10.1.101:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  kubectl get nodes\n\nNAME      STATUS    ROLES         AGE       VERSION\nnode1     Ready     master,node   21h       v1.9.0+coreos.0\nnode2     Ready     master,node   21h       v1.9.0+coreos.0\nnode3     Ready     node          21h       v1.9.0+coreos.0\nnode4     Ready     node          21h       v1.9.0+coreos.0  If you are able to see this, your cluster has been set up successfully.   1  You can use private key / password instead of passwordless ssh. But it requires additional knowledge in using Ansible.", 
            "title": "Check the State of the Cluster"
        }, 
        {
            "location": "/cluster_setup_kubespray/#access-kubernetes-cluster-remotely-optional", 
            "text": "On your local machine  You could also install kubectl on your laptop/workstation. To learn how to install it for your OS,    refer to the  procedure here .  e.g.\nTo install  kubectl  on Ubuntu,  sudo apt-get update   sudo apt-get install -y apt-transport-https\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo\n\napt-key add -\n\nsudo touch /etc/apt/sources.list.d/kubernetes.list\n\necho  deb http://apt.kubernetes.io/ kubernetes-xenial main  | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\n\nsudo apt-get install -y kubectl", 
            "title": "Access Kubernetes Cluster Remotely (Optional)"
        }, 
        {
            "location": "/cluster_setup_kubespray/#copy-kubernetes-config-to-your-local-machine", 
            "text": "Copy  kubeconfig  file to your local machine  mkdir ~/.kube\nscp -r ubuntu@MASTER_HOST_IP:/etc/kubernetes/admin.conf  ~/.kube/config\n\nkubectl get nodes", 
            "title": "Copy kubernetes config to your local machine"
        }, 
        {
            "location": "/cluster_setup_kubespray/#deploy-kubernetes-objects", 
            "text": "Since its a new cluster, which is differnt than what you have created with kubeadm earlier, or if this is the first time you are creating a  kubernetes cluster with kubespray as part of  Advanced Workshop , you need to deploy services which have been covered as part of the previous topics.    In order to do that, use the following commands on the node where you have configured kubectl  git clone https://github.com/schoolofdevops/k8s-code.git\n\ncd k8s-code/projects/instavote\n\nkubectl apply -f instavote-ns.yaml\nkubectl apply -f prod/  Switch to  instavote  namespace  and  validate,  \nkubectl config set-context $(kubectl config current-context)  --namespace=instavote\n\nkubectl get pods,deploy,svc  where,   --cluster=prod : prod is the cluter name you created earlier. If not, use the correct name of the cluster ( kubectl config view)  --user=admin-prod: is the admin user created by default while installing with kubespray  --namespace=instavote : the namespace you just created to deploy instavote app stack   [sample output]  $ kubectl get pods,deploy,svc\n\nNAME                          READY     STATUS    RESTARTS   AGE\npod/db-66496667c9-qggzd       1/1       Running   0          7m\npod/redis-6555998885-4k5cr    1/1       Running   0          7m\npod/redis-6555998885-fb8rk    1/1       Running   0          7m\npod/result-5c7569bcb7-4fptr   1/1       Running   0          7m\npod/result-5c7569bcb7-s4rdx   1/1       Running   0          7m\npod/vote-5d88d47fc8-gbzbq     1/1       Running   0          7m\npod/vote-5d88d47fc8-q4vj6     1/1       Running   0          7m\npod/worker-7c98c96fb4-7tzzw   1/1       Running   0          7m\n\nNAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/db       1         1         1            1           7m\ndeployment.extensions/redis    2         2         2            2           7m\ndeployment.extensions/result   2         2         2            2           7m\ndeployment.extensions/vote     2         2         2            2           7m\ndeployment.extensions/worker   1         1         1            1           7m\n\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/db       ClusterIP   10.233.16.207    none         5432/TCP       7m\nservice/redis    ClusterIP   10.233.14.61     none         6379/TCP       7m\nservice/result   NodePort    10.233.22.10     none         80:30100/TCP   7m\nservice/vote     NodePort    10.233.19.111    none         80:30000/TCP   7m", 
            "title": "Deploy Kubernetes Objects"
        }, 
        {
            "location": "/cluster_setup_kubespray/#references", 
            "text": "Installing Kubernetes On Premises/On Cloud with Kubespray      Kubespray on Github", 
            "title": "References"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/", 
            "text": "Kubernetes Access Control:  Authentication and Authorization\n\n\nIn  this lab you are going to,\n\n\n\n\nCreate users and groups and setup certs based authentication\n\n\nCreate service accounts for applications\n\n\nCreate Roles and ClusterRoles to define authorizations\n\n\nMap Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings.\n\n\n\n\nHow one can access the Kubernetes API?\n\n\nThe Kubernetes API can be accessed by three ways.\n\n\n\n\nKubectl - A command line utility of Kubernetes\n\n\nClient libraries - Go, Python, etc.,\n\n\nREST requests\n\n\n\n\nWho can access the Kubernetes API?\n\n\nKubernetes API can be accessed by,\n\n\n\n\nHuman Users\n\n\nService Accounts  \n\n\n\n\nEach of these topics will be discussed in detail in the later part of this chapter.\n\n\nStages of a Request\n\n\nWhen a request tries to contact the API , it goes through various stages as illustrated in the image given below.\n\n\n\n  \nsource: official kubernetes site\n\n\napi groups and resources\n\n\n\n\n\n\n\n\napiGroup\n\n\nResources\n\n\n\n\n\n\n\n\n\n\napps\n\n\ndaemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale\n\n\n\n\n\n\ncore\n\n\nconfigmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy\n\n\n\n\n\n\nautoscaling\n\n\nhorizontalpodautoscalers\n\n\n\n\n\n\nbatch\n\n\ncronjobs, jobs\n\n\n\n\n\n\npolicy\n\n\npoddisruptionbudgets\n\n\n\n\n\n\nnetworking.k8s.io\n\n\nnetworkpolicies\n\n\n\n\n\n\nauthorization.k8s.io\n\n\nlocalsubjectaccessreviews\n\n\n\n\n\n\nrbac.authorization.k8s.io\n\n\nrolebindings,roles\n\n\n\n\n\n\nextensions\n\n\ndeprecated (read notes)\n\n\n\n\n\n\n\n\nNotes\n\n\nIn addition to the above apiGroups, you may see \nextensions\n being used in some example code snippets. Please note that \nextensions\n was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.  \nYou could read this comment and the thread\n to get clarity on this.  \n\n\nRole Based Access Control (RBAC)\n\n\n\n\n\n\n\n\nGroup\n\n\nUser\n\n\nNamespaces\n\n\nResources\n\n\nAccess Type (verbs)\n\n\n\n\n\n\n\n\n\n\nops\n\n\nmaya\n\n\nall\n\n\nall\n\n\nget, list, watch, update, patch, create, delete, deletecollection\n\n\n\n\n\n\ndev\n\n\nkim\n\n\ninstavote\n\n\ndeployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims\n\n\nget, list , watch, update, patch, create\n\n\n\n\n\n\ninterns\n\n\nyono\n\n\ninstavote\n\n\nreadonly\n\n\nget, list, watch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService Accounts\n\n\nNamespace\n\n\nResources\n\n\nAccess Type (verbs)\n\n\n\n\n\n\n\n\n\n\nmonitoring\n\n\nall\n\n\nall\n\n\nreadonly\n\n\n\n\n\n\n\n\nCreating Kubernetes Users and Groups\n\n\nGenerate the user's private key\n\n\nmkdir -p  ~/.kube/users\ncd ~/.kube/users\n\nopenssl genrsa -out maya.key 2048\nopenssl genrsa -out kim.key 2048\nopenssl genrsa -out yono.key 2048\n\n\n\n\n\n[sample Output]\n\n\nopenssl genrsa -out maya.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............................................................+++\n.........................+++\ne is 65537 (0x10001)\n\n\n\n\n\nLets now create a \nCertification Signing Request (CSR)\n for each of the users. When you generate the csr make sure you also provide\n\n\n\n\nCN: This will be set as username\n\n\nO: Org name. This is actually used as a \ngroup\n by kubernetes while authenticating/authorizing users.  You could add as many as you need\n\n\n\n\ne.g.\n\n\nopenssl req -new -key maya.key -out maya.csr -subj \n/CN=maya/O=ops/O=example.org\n\nopenssl req -new -key kim.key -out kim.csr -subj \n/CN=kim/O=dev/O=example.org\n\nopenssl req -new -key yono.key -out yono.csr -subj \n/CN=yono/O=interns/O=example.org\n\n\n\n\n\n\nIn order to be deemed authentic, these CSRs need to be signed by the \nCertification Authority (CA)\n which in this case is Kubernetes Master.   You need access to the folllwing files on  kubernetes master.\n\n\n\n\nCertificate : ca.crt (kubeadm) or ca.key (kubespray)\n\n\nPricate Key : ca.key (kubeadm) or ca-key.pem  (kubespray)\n\n\n\n\nYou would typically find it at one of the following paths\n\n\n\n\n/etc/kubernetes/pki (kubeadm)\n\n\n/etc/kubernetes/ssl (kubespray)\n\n\n\n\nTo verify which one is your cert and which one is key, use the following command,\n\n\n$ file ca.pem\nca.pem: PEM certificate\n\n\n$ file ca-key.pem\nca-key.pem: PEM RSA private key\n\n\n\n\nOnce signed, .csr files with added signatures become the certificates that could be used to authenticate.\n\n\nYou could either\n\n\n\n\nmove the crt files to k8s master, sign and download  \n\n\ncopy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure.\n\n\n\n\nIn the example here, I have already downloaded \nca.pem\n and \nca-key.pem\n to my management workstation, which are used to sign the CSRs.  \n\n\nAssuming all the files are in the same directory, sign the CSR as,\n\n\nopenssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in maya.csr -out maya.crt\n\nopenssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in kim.csr -out kim.crt\n\nopenssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in yono.csr -out yono.crt\n\n\n\n\nSetting up User configs with kubectl\n\n\nIn order to configure the users that you created above, following steps need to be performed with kubectl\n\n\n\n\nAdd credentials in the configurations\n\n\nSet context to login as a user to a cluster\n\n\nSwitch context in order to assume the user's identity while working with the cluster\n\n\n\n\nto add credentials,\n\n\nkubectl config set-credentials maya --client-certificate=/absolute/path/to/maya.crt --client-key=/absolute/path/to/maya.key\n\nkubectl config set-credentials kim --client-certificate=/absolute/path/to/kim.crt --client-key=~/.kube/users/kim.key\n\nkubectl config set-credentials yono --client-certificate=/absolute/path/to/yono.crt --client-key=~/.kube/users/yono.key\n\n\n\n\n\nwhere,\n\n\n\n\nReplace /absolute/path/to/ with the path to these files.\n\n\ninvalid\n :  ~/.kube/users/yono.crt\n\n\nvalid\n :  /home/xyz/.kube/users/yono.crt\n\n\n\n\n\n\n\n\nAnd proceed to set/create  contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find,\n\n\nkubectl config get-contexts\n\n\n\n\n\n[sample output]\n\n\nCURRENT   NAME                          CLUSTER         AUTHINFO              NAMESPACE\n          admin-prod           prod   admin-cluster.local   instavote\n          admin-cluster4                cluster4        admin-cluster4        instavote\n*         kubernetes-admin@kubernetes   kubernetes      kubernetes-admin      instavote\n\n\n\n\nwhere,  \nprod\n, \ncluster4\n and \nkubernetes\n are cluster names.\n\n\nTo set context for \nprod\n cluster,\n\n\nkubectl config set-context maya-prod --cluster=prod  --user=maya --namespace=instavote\n\nkubectl config set-context kim-prod --cluster=prod  --user=kim --namespace=instavote\n\nkubectl config set-context yono-prod --cluster=prod  --user=yono --namespace=instavote\n\n\n\n\n\nWhere,\n\n\n\n\nmaya-prod : name of the context  \n\n\nprod  : name of the kubernetes cluster you set while creating it  \n\n\nmaya : user you created and configured above to connect to the cluster  \n\n\n\n\nYou could verify the configs with\n\n\nkubectl config get-contexts\n\nCURRENT   NAME         CLUSTER   AUTHINFO     NAMESPACE\n*         admin-prod   prod      admin-prod\n          kim-prod     prod      kim\n          maya-prod    prod      maya\n          yono-prod    prod      yono\n\n\n\n\nand\n\n\nkubectl config view\n\napiVersion: v1\nclusters:\n- cluster:\n   certificate-authority-data: REDACTED\n   server: https://128.199.248.240:6443\n name: prod\ncontexts:\n- context:\n   cluster: prod\n   user: admin-prod\n name: admin-prod\n- context:\n   cluster: prod\n   user: kim\n name: kim-prod\n- context:\n   cluster: prod\n   user: maya\n name: maya-prod\n- context:\n   cluster: prod\n   user: yono\n name: yono-prod\ncurrent-context: admin-prod\nkind: Config\npreferences: {}\nusers:\n- name: admin-prod\n user:\n   client-certificate-data: REDACTED\n   client-key-data: REDACTED\n- name: maya\n user:\n   client-certificate: users/~/.kube/users/maya.crt\n   client-key: users/~/.kube/users/maya.key\n\n\n\n\nYou could assume the identity of user \nyono\n and connect  to the \nprod\n cluster as,\n\n\nkubectl config use-context yono-prod\n\nkubectl config get-contexts\n\nCURRENT   NAME         CLUSTER   AUTHINFO     NAMESPACE\n          admin-prod   prod      admin-prod\n          kim-prod     prod      kim\n          maya-prod    prod      maya\n*         yono-prod    prod      yono\n\n\n\n\nAnd then try running any command as,\n\n\nkubectl get pods\n\n\n\n\nAlternately, if you are a admin user, you could impersonate a user and run a command with that literally using  --as option\n\n\nkubectl config use-context admin-prod\nkubectl get pods --as yono\n\n\n\n\n[Sample Output]\n\n\nNo resources found.\nError from server (Forbidden): pods is forbidden: User \nyono\n cannot list pods in the namespace \ninstavote\n\n\n\n\n\n\nEither ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section.\n\n\nDefine authorisation rules with Roles and ClusterRoles\n\n\nWhats the difference between Roles and ClusterRoles ??\n\n\n\n\nRole is  limited to a namespace (Projects/Orgs/Env)\n\n\nClusterRole is Global\n\n\n\n\nLets say you want to provide read only access to \ninstavote\n, a project specific namespace to all users in the \nexample.org\n\n\nfile: interns-role.yaml\n\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  namespace: instavote\n  name: interns\nrules:\n- apiGroups: [\n*\n]\n  resources: [\n*\n]\n  verbs: [\nget\n, \nlist\n, \nwatch\n]\n\n\n\n\nIn order to map it to all users in \nexample.org\n, create a RoleBinding as\n\n\ninterns-rolebinding.yml\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: interns\n  namespace: instavote\nsubjects:\n- kind: Group\n  name: interns\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: interns\n  apiGroup: rbac.authorization.k8s.io\n\n\n\n\nkubectl create -f interns-role.yml\n\nkubectl create -f interns-rolebinding.yml\n\n\n\n\nTo gt information about the objects created above,\n\n\nkubectl get roles -n instavote\nkubectl get roles,rolebindings -n instavote\n\nkubectl describe role interns\nkubectl describe rolebinding interns\n\n\n\n\n\nTo validate the access,\n\n\nkubectl config use-context yono-prod\nkubectl get pods\n\n\n\n\n\nTo switch back to admin,\n\n\nkubectl config use-context admin-prod\n\n\n\n\n\nExercise\n\n\nCreate a Role and Rolebinding for \ndev\n group with the authorizations defined in the table above. Once applied, test it", 
            "title": "Authentication and Authorization (RBAC)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#kubernetes-access-control-authentication-and-authorization", 
            "text": "In  this lab you are going to,   Create users and groups and setup certs based authentication  Create service accounts for applications  Create Roles and ClusterRoles to define authorizations  Map Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings.", 
            "title": "Kubernetes Access Control:  Authentication and Authorization"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#how-one-can-access-the-kubernetes-api", 
            "text": "The Kubernetes API can be accessed by three ways.   Kubectl - A command line utility of Kubernetes  Client libraries - Go, Python, etc.,  REST requests", 
            "title": "How one can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#who-can-access-the-kubernetes-api", 
            "text": "Kubernetes API can be accessed by,   Human Users  Service Accounts     Each of these topics will be discussed in detail in the later part of this chapter.", 
            "title": "Who can access the Kubernetes API?"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#stages-of-a-request", 
            "text": "When a request tries to contact the API , it goes through various stages as illustrated in the image given below.  \n   source: official kubernetes site", 
            "title": "Stages of a Request"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#api-groups-and-resources", 
            "text": "apiGroup  Resources      apps  daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale    core  configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy    autoscaling  horizontalpodautoscalers    batch  cronjobs, jobs    policy  poddisruptionbudgets    networking.k8s.io  networkpolicies    authorization.k8s.io  localsubjectaccessreviews    rbac.authorization.k8s.io  rolebindings,roles    extensions  deprecated (read notes)", 
            "title": "api groups and resources"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#notes", 
            "text": "In addition to the above apiGroups, you may see  extensions  being used in some example code snippets. Please note that  extensions  was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.   You could read this comment and the thread  to get clarity on this.", 
            "title": "Notes"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#role-based-access-control-rbac", 
            "text": "Group  User  Namespaces  Resources  Access Type (verbs)      ops  maya  all  all  get, list, watch, update, patch, create, delete, deletecollection    dev  kim  instavote  deployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims  get, list , watch, update, patch, create    interns  yono  instavote  readonly  get, list, watch        Service Accounts  Namespace  Resources  Access Type (verbs)      monitoring  all  all  readonly", 
            "title": "Role Based Access Control (RBAC)"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#creating-kubernetes-users-and-groups", 
            "text": "Generate the user's private key  mkdir -p  ~/.kube/users\ncd ~/.kube/users\n\nopenssl genrsa -out maya.key 2048\nopenssl genrsa -out kim.key 2048\nopenssl genrsa -out yono.key 2048  [sample Output]  openssl genrsa -out maya.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............................................................+++\n.........................+++\ne is 65537 (0x10001)  Lets now create a  Certification Signing Request (CSR)  for each of the users. When you generate the csr make sure you also provide   CN: This will be set as username  O: Org name. This is actually used as a  group  by kubernetes while authenticating/authorizing users.  You could add as many as you need   e.g.  openssl req -new -key maya.key -out maya.csr -subj  /CN=maya/O=ops/O=example.org \nopenssl req -new -key kim.key -out kim.csr -subj  /CN=kim/O=dev/O=example.org \nopenssl req -new -key yono.key -out yono.csr -subj  /CN=yono/O=interns/O=example.org   In order to be deemed authentic, these CSRs need to be signed by the  Certification Authority (CA)  which in this case is Kubernetes Master.   You need access to the folllwing files on  kubernetes master.   Certificate : ca.crt (kubeadm) or ca.key (kubespray)  Pricate Key : ca.key (kubeadm) or ca-key.pem  (kubespray)   You would typically find it at one of the following paths   /etc/kubernetes/pki (kubeadm)  /etc/kubernetes/ssl (kubespray)   To verify which one is your cert and which one is key, use the following command,  $ file ca.pem\nca.pem: PEM certificate\n\n\n$ file ca-key.pem\nca-key.pem: PEM RSA private key  Once signed, .csr files with added signatures become the certificates that could be used to authenticate.  You could either   move the crt files to k8s master, sign and download    copy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure.   In the example here, I have already downloaded  ca.pem  and  ca-key.pem  to my management workstation, which are used to sign the CSRs.    Assuming all the files are in the same directory, sign the CSR as,  openssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in maya.csr -out maya.crt\n\nopenssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in kim.csr -out kim.crt\n\nopenssl x509 -req -CA ca.pem -CAkey ca-key.pem -CAcreateserial -days 730 -in yono.csr -out yono.crt", 
            "title": "Creating Kubernetes Users and Groups"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#setting-up-user-configs-with-kubectl", 
            "text": "In order to configure the users that you created above, following steps need to be performed with kubectl   Add credentials in the configurations  Set context to login as a user to a cluster  Switch context in order to assume the user's identity while working with the cluster   to add credentials,  kubectl config set-credentials maya --client-certificate=/absolute/path/to/maya.crt --client-key=/absolute/path/to/maya.key\n\nkubectl config set-credentials kim --client-certificate=/absolute/path/to/kim.crt --client-key=~/.kube/users/kim.key\n\nkubectl config set-credentials yono --client-certificate=/absolute/path/to/yono.crt --client-key=~/.kube/users/yono.key  where,   Replace /absolute/path/to/ with the path to these files.  invalid  :  ~/.kube/users/yono.crt  valid  :  /home/xyz/.kube/users/yono.crt     And proceed to set/create  contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find,  kubectl config get-contexts  [sample output]  CURRENT   NAME                          CLUSTER         AUTHINFO              NAMESPACE\n          admin-prod           prod   admin-cluster.local   instavote\n          admin-cluster4                cluster4        admin-cluster4        instavote\n*         kubernetes-admin@kubernetes   kubernetes      kubernetes-admin      instavote  where,   prod ,  cluster4  and  kubernetes  are cluster names.  To set context for  prod  cluster,  kubectl config set-context maya-prod --cluster=prod  --user=maya --namespace=instavote\n\nkubectl config set-context kim-prod --cluster=prod  --user=kim --namespace=instavote\n\nkubectl config set-context yono-prod --cluster=prod  --user=yono --namespace=instavote  Where,   maya-prod : name of the context    prod  : name of the kubernetes cluster you set while creating it    maya : user you created and configured above to connect to the cluster     You could verify the configs with  kubectl config get-contexts\n\nCURRENT   NAME         CLUSTER   AUTHINFO     NAMESPACE\n*         admin-prod   prod      admin-prod\n          kim-prod     prod      kim\n          maya-prod    prod      maya\n          yono-prod    prod      yono  and  kubectl config view\n\napiVersion: v1\nclusters:\n- cluster:\n   certificate-authority-data: REDACTED\n   server: https://128.199.248.240:6443\n name: prod\ncontexts:\n- context:\n   cluster: prod\n   user: admin-prod\n name: admin-prod\n- context:\n   cluster: prod\n   user: kim\n name: kim-prod\n- context:\n   cluster: prod\n   user: maya\n name: maya-prod\n- context:\n   cluster: prod\n   user: yono\n name: yono-prod\ncurrent-context: admin-prod\nkind: Config\npreferences: {}\nusers:\n- name: admin-prod\n user:\n   client-certificate-data: REDACTED\n   client-key-data: REDACTED\n- name: maya\n user:\n   client-certificate: users/~/.kube/users/maya.crt\n   client-key: users/~/.kube/users/maya.key  You could assume the identity of user  yono  and connect  to the  prod  cluster as,  kubectl config use-context yono-prod\n\nkubectl config get-contexts\n\nCURRENT   NAME         CLUSTER   AUTHINFO     NAMESPACE\n          admin-prod   prod      admin-prod\n          kim-prod     prod      kim\n          maya-prod    prod      maya\n*         yono-prod    prod      yono  And then try running any command as,  kubectl get pods  Alternately, if you are a admin user, you could impersonate a user and run a command with that literally using  --as option  kubectl config use-context admin-prod\nkubectl get pods --as yono  [Sample Output]  No resources found.\nError from server (Forbidden): pods is forbidden: User  yono  cannot list pods in the namespace  instavote   Either ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section.", 
            "title": "Setting up User configs with kubectl"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#define-authorisation-rules-with-roles-and-clusterroles", 
            "text": "Whats the difference between Roles and ClusterRoles ??   Role is  limited to a namespace (Projects/Orgs/Env)  ClusterRole is Global   Lets say you want to provide read only access to  instavote , a project specific namespace to all users in the  example.org  file: interns-role.yaml  apiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  namespace: instavote\n  name: interns\nrules:\n- apiGroups: [ * ]\n  resources: [ * ]\n  verbs: [ get ,  list ,  watch ]  In order to map it to all users in  example.org , create a RoleBinding as  interns-rolebinding.yml  kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: interns\n  namespace: instavote\nsubjects:\n- kind: Group\n  name: interns\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: interns\n  apiGroup: rbac.authorization.k8s.io  kubectl create -f interns-role.yml\n\nkubectl create -f interns-rolebinding.yml  To gt information about the objects created above,  kubectl get roles -n instavote\nkubectl get roles,rolebindings -n instavote\n\nkubectl describe role interns\nkubectl describe rolebinding interns  To validate the access,  kubectl config use-context yono-prod\nkubectl get pods  To switch back to admin,  kubectl config use-context admin-prod", 
            "title": "Define authorisation rules with Roles and ClusterRoles"
        }, 
        {
            "location": "/configuring_authentication_and_authorization/#exercise", 
            "text": "Create a Role and Rolebinding for  dev  group with the authorizations defined in the table above. Once applied, test it", 
            "title": "Exercise"
        }, 
        {
            "location": "/advanced_pod_scheduling/", 
            "text": "Advanced Pod Scheduling\n\n\nIn the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling.\n\n\nFrom the \napi document for version 1.11\n following are the pod specs which are relevant from scheduling perspective.\n\n\n\n\nnodeSelector\n\n\nnodeName\n\n\naffinity\n\n\nschedulerName\n\n\ntolerations\n\n\n\n\nUsing Node Selectors\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes \nnode-name\n zone=aaa\n\nkubectl get nodes --show-labels\n\n\n\n\n\ne.g.\n\n\nkubectl label nodes node1 zone=bbb\nkubectl label nodes node2 zone=bbb\nkubectl label nodes node3 zone=aaa\nkubectl label nodes node4 zone=aaa\nkubectl get nodes --show-labels\n\n\n\n\n[sample output]\n\n\nNAME      STATUS    ROLES         AGE       VERSION   LABELS\nnode1     Ready     master,node   22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode2     Ready     master,node   22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode3     Ready     node          22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa\nnode4     Ready     node          21h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa\n\n\n\n\nCheck how the pods are distributed on the nodes using the following command,\n\n\nkubectl get pods -o wide --selector=\nrole=vote\n\nNAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-5d88d47fc8-6rflg   1/1       Running   0          1m        10.233.75.9      node2\nvote-5d88d47fc8-gbzbq   1/1       Running   0          1h        10.233.74.76     node4\nvote-5d88d47fc8-q4vj6   1/1       Running   0          1h        10.233.102.133   node1\nvote-5d88d47fc8-znd2z   1/1       Running   0          1m        10.233.71.20     node3\n\n\n\n\nFrom the above output, you could see that the pods running \nvote\n app are currently equally distributed.\nNow, update pod definition to make it schedule only on nodes in zone \nbbb\n\n\nfile: k8s-code/pods/vote-pod.yml\n\n\n\n....\n\ntemplate:\n...\n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v1\n        ports:\n          - containerPort: 80\n            protocol: TCP\n    nodeSelector:\n      zone: 'bbb'\n\n\n\n\nFor this change, pod needs to be re created.\n\n\nkubectl apply -f vote-pod.yml\n\n\n\n\nYou would notice that, the moment you make that change, a new rollout kicks off, which will start redistributing the pods, now following the \nnodeSelector\n constraint that you added.\n\n\nWatch the output of the following command\n\n\n\nwatch kubectl get pods -o wide --selector=\nrole=vote\n\n\n\n\n\n\nYou will see the following while it transitions\n\n\n\nNAME                        READY     STATUS              RESTARTS   AGE       IP               NODE\npod/vote-5d88d47fc8-6rflg   0/1       Terminating         0          5m        10.233.75.9      node2\npod/vote-5d88d47fc8-gbzbq   0/1       Terminating         0          1h        10.233.74.76     node4\npod/vote-5d88d47fc8-q4vj6   0/1       Terminating         0          1h        10.233.102.133   node1\npod/vote-67d7dd8f89-2w5wl   1/1       Running             0          44s       10.233.75.10     node2\npod/vote-67d7dd8f89-gm6bq   0/1       ContainerCreating   0          2s        \nnone\n           node2\npod/vote-67d7dd8f89-w87n9   1/1       Running             0          44s       10.233.102.134   node1\npod/vote-67d7dd8f89-xccl8   1/1       Running             0          44s       10.233.102.135   node1\n\n\n\n\n\nand after the rollout completes,\n\n\n\nNAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-67d7dd8f89-2w5wl   1/1       Running   0          2m        10.233.75.10     node2\nvote-67d7dd8f89-gm6bq   1/1       Running   0          1m        10.233.75.11     node2\nvote-67d7dd8f89-w87n9   1/1       Running   0          2m        10.233.102.134   node1\nvote-67d7dd8f89-xccl8   1/1       Running   0          2m        10.233.102.135   node1\n\n\n\n\n\nExercise\n\n\nJust like \nnodeSelector\n above, you could enforce a pod to run on a specific node using \nnodeName\n. Try using that property to run all pods for results application on \nnode3\n\n\nDefining  affinity and anti-affinity\n\n\nWe have discussed about scheduling a pod on a particular node using \nNodeSelector\n, but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.\n\n\n\n\nrequired\n\n\n\n\npreferred\n\n\n\n\n\n\nDuringScheduling\n\n\n\n\nDuringExecution\n\n\n\n\nOperators\n\n\n\n\nIn\n\n\nNotIn\n\n\nExists\n\n\nDoesNotExist\n\n\nGt\n\n\nLt\n\n\n\n\nNode Affinity\n\n\nExamine  the current pod distribution  \n\n\nkubectl get pods -o wide --selector=\nrole=vote\n\n\n\n\n\n\nNAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-8546bbd84d-22d6x   1/1       Running   0          35s       10.233.102.137   node1\nvote-8546bbd84d-8f9bc   1/1       Running   0          1m        10.233.102.136   node1\nvote-8546bbd84d-bpg8f   1/1       Running   0          1m        10.233.75.12     node2\nvote-8546bbd84d-d8j9g   1/1       Running   0          1m        10.233.75.13     node2\n\n\n\n\nand node labels\n\n\nkubectl get nodes --show-labels\n\n\n\n\n\nNAME      STATUS    ROLES         AGE       VERSION   LABELS\nnode1     Ready     master,node   1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode2     Ready     master,node   1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode3     Ready     node          1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa\nnode4     Ready     node          1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa\n\n\n\n\nLets create node affinity criteria as\n\n\n\n\nPods for vote app \nmust\n not run on the master nodes\n\n\nPods for vote app \npreferably\n run on a node in zone \nbbb\n\n\n\n\nFirst is a \nhard\n affinity versus second being \nsoft\n affinity.\n\n\nfile: vote-deploy.yaml\n\n\n....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: DoesNotExist\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              preference:\n                matchExpressions:\n                - key: zone\n\n\n\n\nPod Affinity\n\n\nLets define pod affinity criteria as,\n\n\n\n\nPods for \nvote\n and \nredis\n should be co located as much as possible (preferred)\n\n\nNo two pods with \nredis\n app should be running on the same node (required)\n\n\n\n\nkubectl get pods -o wide --selector=\nrole in (vote,redis)\n\n\n\n\n\n\n[sample output]\n\n\nNAME                     READY     STATUS    RESTARTS   AGE       IP               NODE\nredis-6555998885-4k5cr   1/1       Running   0          4h        10.233.71.19     node3\nredis-6555998885-fb8rk   1/1       Running   0          4h        10.233.102.132   node1\nvote-74c894d6f5-bql8z    1/1       Running   0          22m       10.233.74.78     node4\nvote-74c894d6f5-nnzmc    1/1       Running   0          21m       10.233.71.22     node3\nvote-74c894d6f5-ss929    1/1       Running   0          22m       10.233.74.77     node4\nvote-74c894d6f5-tpzgm    1/1       Running   0          22m       10.233.71.21     node3\n\n\n\n\nfile: vote-deploy.yaml\n\n\n...\n    template:\n...\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n...\n\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                  - key: role\n                    operator: In\n                    values:\n                    - redis\n                topologyKey: kubernetes.io/hostname\n\n\n\n\nfile: redis-deploy.yaml\n\n\n....\n  template:\n...\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n      restartPolicy: Always\n\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: role\n                operator: In\n                values:\n                - redis\n            topologyKey: \nkubernetes.io/hostname\n\n\n\n\n\napply\n\n\nkubectl apply -f redis-deploy.yaml\nkubectl apply -f vote-deploy.yaml\n\n\n\n\n\ncheck the pods distribution\n\n\nkubectl get pods -o wide --selector=\nrole in (vote,redis)\n\n\n\n\n\n[sample output ]\n\n\nNAME                     READY     STATUS    RESTARTS   AGE       IP             NODE\nredis-5bf748dbcf-gr8zg   1/1       Running   0          13m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx   1/1       Running   0          13m       10.233.74.79   node4\nvote-56bf599b9c-22lpw    1/1       Running   0          12m       10.233.74.80   node4\nvote-56bf599b9c-nvvfd    1/1       Running   0          13m       10.233.71.25   node3\nvote-56bf599b9c-w6jc9    1/1       Running   0          13m       10.233.71.23   node3\nvote-56bf599b9c-ztdgm    1/1       Running   0          13m       10.233.71.24   node3\n\n\n\n\nObservations from the above output,\n\n\n\n\nSince redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes  (node2 and node4)\n\n\nsince vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3\n\n\n\n\nIf you kill the pods on node3, at the time of  scheduling new ones, scheduler meets all affinity rules\n\n\n$ kubectl delete pods vote-56bf599b9c-nvvfd vote-56bf599b9c-w6jc9 vote-56bf599b9c-ztdgm\npod \nvote-56bf599b9c-nvvfd\n deleted\npod \nvote-56bf599b9c-w6jc9\n deleted\npod \nvote-56bf599b9c-ztdgm\n deleted\n\n\n$ kubectl get pods -o wide --selector=\nrole in (vote,redis)\n\nNAME                     READY     STATUS    RESTARTS   AGE       IP             NODE\nredis-5bf748dbcf-gr8zg   1/1       Running   0          19m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx   1/1       Running   0          19m       10.233.74.79   node4\nvote-56bf599b9c-22lpw    1/1       Running   0          19m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc    1/1       Running   0          20s       10.233.74.83   node4\nvote-56bf599b9c-bqsrq    1/1       Running   0          20s       10.233.74.82   node4\nvote-56bf599b9c-xw7zc    1/1       Running   0          19s       10.233.74.81   node4\n\n\n\n\nTaints and tolerations\n\n\n\n\nAffinity is defined for pods\n\n\nTaints are defined for nodes\n\n\n\n\nYou could add the taints with criteria and effects. Effetcs can be\n\n\nTaint Specs\n:   \n\n\n\n\neffect  \n\n\nNoSchedule  \n\n\nPreferNoSchedule  \n\n\nNoExecute  \n\n\n\n\n\n\nkey  \n\n\nvalue  \n\n\ntimeAdded (only written for NoExecute taints)  \n\n\n\n\nObserve the pods distribution\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-gr8zg    1/1       Running   0          27m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx    1/1       Running   0          27m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          26m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          8m        10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          8m        10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          8m        10.233.74.81   node4\nworker-7c98c96fb4-7tzzw   1/1       Running   1          4h        10.233.75.8    node2\n\n\n\n\nLets taint a node.\n\n\nkubectl taint node node2 dedicated=worker:NoExecute\n\n\n\n\nafter taining the node\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74     node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          2m        10.233.71.26     node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          30m       10.233.74.79     node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18     node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75     node4\nvote-56bf599b9c-22lpw     1/1       Running   0          29m       10.233.74.80     node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          11m       10.233.74.83     node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          11m       10.233.74.82     node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          11m       10.233.74.81     node4\nworker-7c98c96fb4-46ltl   1/1       Running   0          2m        10.233.102.140   node1\n\n\n\n\nAll pods running on node2 just got evicted.\n\n\nAdd toleration in the Deployment for worker.\n\n\nFile: worker-deploy.yml\n\n\napiVersion: apps/v1\n.....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote-worker:latest\n\n      tolerations:\n        - key: \ndedicated\n\n          operator: \nEqual\n\n          value: \nworker\n\n          effect: \nNoExecute\n\n\n\n\n\napply\n\n\nkubectl apply -f worker-deploy.yml\n\n\n\n\n\nObserve the pod distribution now.\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          3m        10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          31m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          30m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          12m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          12m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          12m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          1m        10.233.75.15   node2\n\n\n\n\nYou should see worker being scheduled on node2\n\n\nTo remove the taint created above\n\n\nkubectl taint node node2 dedicate:NoExecute-", 
            "title": "Advanced Pod Scheduling"
        }, 
        {
            "location": "/advanced_pod_scheduling/#advanced-pod-scheduling", 
            "text": "In the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling.  From the  api document for version 1.11  following are the pod specs which are relevant from scheduling perspective.   nodeSelector  nodeName  affinity  schedulerName  tolerations", 
            "title": "Advanced Pod Scheduling"
        }, 
        {
            "location": "/advanced_pod_scheduling/#using-node-selectors", 
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes  node-name  zone=aaa\n\nkubectl get nodes --show-labels  e.g.  kubectl label nodes node1 zone=bbb\nkubectl label nodes node2 zone=bbb\nkubectl label nodes node3 zone=aaa\nkubectl label nodes node4 zone=aaa\nkubectl get nodes --show-labels  [sample output]  NAME      STATUS    ROLES         AGE       VERSION   LABELS\nnode1     Ready     master,node   22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode2     Ready     master,node   22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode3     Ready     node          22h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa\nnode4     Ready     node          21h       v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa  Check how the pods are distributed on the nodes using the following command,  kubectl get pods -o wide --selector= role=vote \nNAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-5d88d47fc8-6rflg   1/1       Running   0          1m        10.233.75.9      node2\nvote-5d88d47fc8-gbzbq   1/1       Running   0          1h        10.233.74.76     node4\nvote-5d88d47fc8-q4vj6   1/1       Running   0          1h        10.233.102.133   node1\nvote-5d88d47fc8-znd2z   1/1       Running   0          1m        10.233.71.20     node3  From the above output, you could see that the pods running  vote  app are currently equally distributed.\nNow, update pod definition to make it schedule only on nodes in zone  bbb  file: k8s-code/pods/vote-pod.yml  \n....\n\ntemplate:\n...\n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v1\n        ports:\n          - containerPort: 80\n            protocol: TCP\n    nodeSelector:\n      zone: 'bbb'  For this change, pod needs to be re created.  kubectl apply -f vote-pod.yml  You would notice that, the moment you make that change, a new rollout kicks off, which will start redistributing the pods, now following the  nodeSelector  constraint that you added.  Watch the output of the following command  \nwatch kubectl get pods -o wide --selector= role=vote   You will see the following while it transitions  \nNAME                        READY     STATUS              RESTARTS   AGE       IP               NODE\npod/vote-5d88d47fc8-6rflg   0/1       Terminating         0          5m        10.233.75.9      node2\npod/vote-5d88d47fc8-gbzbq   0/1       Terminating         0          1h        10.233.74.76     node4\npod/vote-5d88d47fc8-q4vj6   0/1       Terminating         0          1h        10.233.102.133   node1\npod/vote-67d7dd8f89-2w5wl   1/1       Running             0          44s       10.233.75.10     node2\npod/vote-67d7dd8f89-gm6bq   0/1       ContainerCreating   0          2s         none            node2\npod/vote-67d7dd8f89-w87n9   1/1       Running             0          44s       10.233.102.134   node1\npod/vote-67d7dd8f89-xccl8   1/1       Running             0          44s       10.233.102.135   node1  and after the rollout completes,  \nNAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-67d7dd8f89-2w5wl   1/1       Running   0          2m        10.233.75.10     node2\nvote-67d7dd8f89-gm6bq   1/1       Running   0          1m        10.233.75.11     node2\nvote-67d7dd8f89-w87n9   1/1       Running   0          2m        10.233.102.134   node1\nvote-67d7dd8f89-xccl8   1/1       Running   0          2m        10.233.102.135   node1", 
            "title": "Using Node Selectors"
        }, 
        {
            "location": "/advanced_pod_scheduling/#exercise", 
            "text": "Just like  nodeSelector  above, you could enforce a pod to run on a specific node using  nodeName . Try using that property to run all pods for results application on  node3", 
            "title": "Exercise"
        }, 
        {
            "location": "/advanced_pod_scheduling/#defining-affinity-and-anti-affinity", 
            "text": "We have discussed about scheduling a pod on a particular node using  NodeSelector , but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.   required   preferred    DuringScheduling   DuringExecution   Operators   In  NotIn  Exists  DoesNotExist  Gt  Lt", 
            "title": "Defining  affinity and anti-affinity"
        }, 
        {
            "location": "/advanced_pod_scheduling/#node-affinity", 
            "text": "Examine  the current pod distribution    kubectl get pods -o wide --selector= role=vote   NAME                    READY     STATUS    RESTARTS   AGE       IP               NODE\nvote-8546bbd84d-22d6x   1/1       Running   0          35s       10.233.102.137   node1\nvote-8546bbd84d-8f9bc   1/1       Running   0          1m        10.233.102.136   node1\nvote-8546bbd84d-bpg8f   1/1       Running   0          1m        10.233.75.12     node2\nvote-8546bbd84d-d8j9g   1/1       Running   0          1m        10.233.75.13     node2  and node labels  kubectl get nodes --show-labels  NAME      STATUS    ROLES         AGE       VERSION   LABELS\nnode1     Ready     master,node   1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode2     Ready     master,node   1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true,zone=bbb\nnode3     Ready     node          1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true,zone=aaa\nnode4     Ready     node          1d        v1.10.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node4,node-role.kubernetes.io/node=true,zone=aaa  Lets create node affinity criteria as   Pods for vote app  must  not run on the master nodes  Pods for vote app  preferably  run on a node in zone  bbb   First is a  hard  affinity versus second being  soft  affinity.  file: vote-deploy.yaml  ....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: DoesNotExist\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              preference:\n                matchExpressions:\n                - key: zone", 
            "title": "Node Affinity"
        }, 
        {
            "location": "/advanced_pod_scheduling/#pod-affinity", 
            "text": "Lets define pod affinity criteria as,   Pods for  vote  and  redis  should be co located as much as possible (preferred)  No two pods with  redis  app should be running on the same node (required)   kubectl get pods -o wide --selector= role in (vote,redis)   [sample output]  NAME                     READY     STATUS    RESTARTS   AGE       IP               NODE\nredis-6555998885-4k5cr   1/1       Running   0          4h        10.233.71.19     node3\nredis-6555998885-fb8rk   1/1       Running   0          4h        10.233.102.132   node1\nvote-74c894d6f5-bql8z    1/1       Running   0          22m       10.233.74.78     node4\nvote-74c894d6f5-nnzmc    1/1       Running   0          21m       10.233.71.22     node3\nvote-74c894d6f5-ss929    1/1       Running   0          22m       10.233.74.77     node4\nvote-74c894d6f5-tpzgm    1/1       Running   0          22m       10.233.71.21     node3  file: vote-deploy.yaml  ...\n    template:\n...\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n...\n\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                  - key: role\n                    operator: In\n                    values:\n                    - redis\n                topologyKey: kubernetes.io/hostname  file: redis-deploy.yaml  ....\n  template:\n...\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n      restartPolicy: Always\n\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: role\n                operator: In\n                values:\n                - redis\n            topologyKey:  kubernetes.io/hostname   apply  kubectl apply -f redis-deploy.yaml\nkubectl apply -f vote-deploy.yaml  check the pods distribution  kubectl get pods -o wide --selector= role in (vote,redis)   [sample output ]  NAME                     READY     STATUS    RESTARTS   AGE       IP             NODE\nredis-5bf748dbcf-gr8zg   1/1       Running   0          13m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx   1/1       Running   0          13m       10.233.74.79   node4\nvote-56bf599b9c-22lpw    1/1       Running   0          12m       10.233.74.80   node4\nvote-56bf599b9c-nvvfd    1/1       Running   0          13m       10.233.71.25   node3\nvote-56bf599b9c-w6jc9    1/1       Running   0          13m       10.233.71.23   node3\nvote-56bf599b9c-ztdgm    1/1       Running   0          13m       10.233.71.24   node3  Observations from the above output,   Since redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes  (node2 and node4)  since vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3   If you kill the pods on node3, at the time of  scheduling new ones, scheduler meets all affinity rules  $ kubectl delete pods vote-56bf599b9c-nvvfd vote-56bf599b9c-w6jc9 vote-56bf599b9c-ztdgm\npod  vote-56bf599b9c-nvvfd  deleted\npod  vote-56bf599b9c-w6jc9  deleted\npod  vote-56bf599b9c-ztdgm  deleted\n\n\n$ kubectl get pods -o wide --selector= role in (vote,redis) \nNAME                     READY     STATUS    RESTARTS   AGE       IP             NODE\nredis-5bf748dbcf-gr8zg   1/1       Running   0          19m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx   1/1       Running   0          19m       10.233.74.79   node4\nvote-56bf599b9c-22lpw    1/1       Running   0          19m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc    1/1       Running   0          20s       10.233.74.83   node4\nvote-56bf599b9c-bqsrq    1/1       Running   0          20s       10.233.74.82   node4\nvote-56bf599b9c-xw7zc    1/1       Running   0          19s       10.233.74.81   node4", 
            "title": "Pod Affinity"
        }, 
        {
            "location": "/advanced_pod_scheduling/#taints-and-tolerations", 
            "text": "Affinity is defined for pods  Taints are defined for nodes   You could add the taints with criteria and effects. Effetcs can be  Taint Specs :      effect    NoSchedule    PreferNoSchedule    NoExecute      key    value    timeAdded (only written for NoExecute taints)     Observe the pods distribution  $ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-gr8zg    1/1       Running   0          27m       10.233.75.14   node2\nredis-5bf748dbcf-vxppx    1/1       Running   0          27m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          26m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          8m        10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          8m        10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          8m        10.233.74.81   node4\nworker-7c98c96fb4-7tzzw   1/1       Running   1          4h        10.233.75.8    node2  Lets taint a node.  kubectl taint node node2 dedicated=worker:NoExecute  after taining the node  $ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74     node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          2m        10.233.71.26     node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          30m       10.233.74.79     node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18     node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75     node4\nvote-56bf599b9c-22lpw     1/1       Running   0          29m       10.233.74.80     node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          11m       10.233.74.83     node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          11m       10.233.74.82     node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          11m       10.233.74.81     node4\nworker-7c98c96fb4-46ltl   1/1       Running   0          2m        10.233.102.140   node1  All pods running on node2 just got evicted.  Add toleration in the Deployment for worker.  File: worker-deploy.yml  apiVersion: apps/v1\n.....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote-worker:latest\n\n      tolerations:\n        - key:  dedicated \n          operator:  Equal \n          value:  worker \n          effect:  NoExecute   apply  kubectl apply -f worker-deploy.yml  Observe the pod distribution now.  $ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          3m        10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          31m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          30m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          12m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          12m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          12m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          1m        10.233.75.15   node2  You should see worker being scheduled on node2  To remove the taint created above  kubectl taint node node2 dedicate:NoExecute-", 
            "title": "Taints and tolerations"
        }, 
        {
            "location": "/pod-adv-specs/", 
            "text": "Additional  Pod Specs  - Resources, Security Specs\n\n\nResource requests and limits\n\n\nWe can control the amount of resource requested and used by all the pods. This can be done by adding following data the deployment template.\n\n\nResource Request\n\n\nFile: code/frontend-deploy.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n          resources:\n            requests:\n              memory: \n128Mi\n\n              cpu: \n250m\n\n\n\n\n\nThis ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use \nresource limit\n to limit the resource usage by a pod.\n\n\nExpected output:\n\n\nkubectl describe pod front-end-5c64b7c5cc-cwgr5\n\n[...]\nContainers:\n  front-end:\n    Container ID:\n    Image:          schoolofdevops/frontend\n    Image ID:\n    Port:           8079/TCP\n    State:          Waiting\n      Reason:       ContainerCreating\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:        250m\n      memory:     128Mi\n\n\n\n\nResource limit\n\n\nFile: code/frontend-deploy.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n          resources:\n            requests:\n              memory: \n128Mi\n\n              cpu: \n250m\n\n            limits:\n              memory: \n256Mi\n\n              cpu: \n500m\n\n\n\n\n\nExpected output:\n\n\nkubectl describe pod front-end-5b877b4dff-5twdd\n\n[...]\nContainers:\n  front-end:\n    Container ID:   docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554\n    Image:          schoolofdevops/frontend\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:           8079/TCP\n    State:          Running\n      Started:      Thu, 08 Feb 2018 17:14:54 +0530\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     500m\n      memory:  256Mi\n    Requests:\n      cpu:        250m\n      memory:     128Mi", 
            "title": "Pod Resource and Security Specs"
        }, 
        {
            "location": "/pod-adv-specs/#additional-pod-specs-resources-security-specs", 
            "text": "", 
            "title": "Additional  Pod Specs  - Resources, Security Specs"
        }, 
        {
            "location": "/pod-adv-specs/#resource-requests-and-limits", 
            "text": "We can control the amount of resource requested and used by all the pods. This can be done by adding following data the deployment template.", 
            "title": "Resource requests and limits"
        }, 
        {
            "location": "/pod-adv-specs/#resource-request", 
            "text": "File: code/frontend-deploy.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n          resources:\n            requests:\n              memory:  128Mi \n              cpu:  250m   This ensures that pod always get the minimum cpu and memory specified. But this does not restrict the pod from accessing additional resources if needed. Thats why we have to use  resource limit  to limit the resource usage by a pod.  Expected output:  kubectl describe pod front-end-5c64b7c5cc-cwgr5\n\n[...]\nContainers:\n  front-end:\n    Container ID:\n    Image:          schoolofdevops/frontend\n    Image ID:\n    Port:           8079/TCP\n    State:          Waiting\n      Reason:       ContainerCreating\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:        250m\n      memory:     128Mi", 
            "title": "Resource Request"
        }, 
        {
            "location": "/pod-adv-specs/#resource-limit", 
            "text": "File: code/frontend-deploy.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n          resources:\n            requests:\n              memory:  128Mi \n              cpu:  250m \n            limits:\n              memory:  256Mi \n              cpu:  500m   Expected output:  kubectl describe pod front-end-5b877b4dff-5twdd\n\n[...]\nContainers:\n  front-end:\n    Container ID:   docker://d49a08c18fd9651af2f3dd28772da977b238a4010f14372e72e0ca24dcec8554\n    Image:          schoolofdevops/frontend\n    Image ID:       docker-pullable://schoolofdevops/frontend@sha256:94b7a0843f99223a8a1d284fdeeb3fd5a731c03aea57a52751c6ebde40be1f50\n    Port:           8079/TCP\n    State:          Running\n      Started:      Thu, 08 Feb 2018 17:14:54 +0530\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     500m\n      memory:  256Mi\n    Requests:\n      cpu:        250m\n      memory:     128Mi", 
            "title": "Resource limit"
        }, 
        {
            "location": "/pods-health-probes/", 
            "text": "Checking health with Probes\n\n\nAdding health checks\n\n\nHealth checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks,\n  * Liveness Probe\n  * Readiness Probe\nProbes are simply a \ndiagnostic action\n performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely,\n  * \nExecAction\n: Executes a command inside the pod. Assumed successful when the command \nreturns 0\n as exit code.\n  * \nTCPSocketAction\n: Checks for a state of a particular port on the pod. Considered successful when the state of \nthe port is open\n.\n  * \nHTTPGetAction\n: Performs a GET request on pod's IP. Assumed successful when the status code is \ngreater than 200 and less than 400\n\n\nIn cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice.\n\n\nLiveness Probe\n\n\nLiveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is \nSuccess\n.\n\n\nLet us add liveness probe to our \nfrontend\n deployment. The following probe will check whether it is able to \naccess the port or not\n.\n\n\nFile: code/frontend-deploy.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n\n\n\n\nExpected output:\n\n\nkubectl apply -f front-end/frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-757db58546-fkgdw\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              22s   default-scheduler  Successfully assigned front-end-757db58546-fkgdw to node4\n  Normal  SuccessfulMountVolume  22s   kubelet, node4     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal  Pulling                20s   kubelet, node4     pulling image \nschoolofdevops/frontend\n\n  Normal  Pulled                 17s   kubelet, node4     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal  Created                17s   kubelet, node4     Created container\n  Normal  Started                17s   kubelet, node4     Started container\n\n\n\n\nLet us change the livenessProbe check port to 8080.\n\n\n          livenessProbe:\n            tcpSocket:\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5\n\n\n\n\nApply this deployment file and check the description of the pod\n\n\nExpected output:\n\n\nkubectl apply -f frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-bf86ffd8b-bjb7p\n\n[...]\nEvents:\n  Type     Reason                 Age               From               Message\n  ----     ------                 ----              ----               -------\n  Normal   Scheduled              1m                default-scheduler  Successfully assigned front-end-bf86ffd8b-bjb7p to node3\n  Normal   SuccessfulMountVolume  1m                kubelet, node3     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal   Pulling                38s (x2 over 1m)  kubelet, node3     pulling image \nschoolofdevops/frontend\n\n  Normal   Killing                38s               kubelet, node3     Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated.\n  Normal   Pulled                 35s (x2 over 1m)  kubelet, node3     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal   Created                35s (x2 over 1m)  kubelet, node3     Created container\n  Normal   Started                35s (x2 over 1m)  kubelet, node3     Started container\n  Warning  Unhealthy              27s (x5 over 1m)  kubelet, node3     Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused\n\n\n\n\nReadiness Probe\n\n\nReadiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is \nSuccess\n.\n\n\nReadiness probe is configured just like liveness probe. But this time we will use \nhttpGet request\n.\n\n\nFile: code/frontend-deploy.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3\n\n\n\n\nExpected output:\n\n\nkubectl apply -f front-end/frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-c5bc89b57-g42nc\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              11s   default-scheduler  Successfully assigned front-end-c5bc89b57-g42nc to node4\n  Normal  SuccessfulMountVolume  10s   kubelet, node4     MountVolume.SetUp succeeded for volume \ndefault-token-w4279\n\n  Normal  Pulling                8s    kubelet, node4     pulling image \nschoolofdevops/frontend\n\n  Normal  Pulled                 6s    kubelet, node4     Successfully pulled image \nschoolofdevops/frontend\n\n  Normal  Created                5s    kubelet, node4     Created container\n  Normal  Started                5s    kubelet, node4     Started container\n\n\n\n\nTask\n: Change the readinessProbe port to 8080 and check what happens to the pod.", 
            "title": "Adding health checks with Probes"
        }, 
        {
            "location": "/pods-health-probes/#checking-health-with-probes", 
            "text": "", 
            "title": "Checking health with Probes"
        }, 
        {
            "location": "/pods-health-probes/#adding-health-checks", 
            "text": "Health checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks,\n  * Liveness Probe\n  * Readiness Probe\nProbes are simply a  diagnostic action  performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely,\n  *  ExecAction : Executes a command inside the pod. Assumed successful when the command  returns 0  as exit code.\n  *  TCPSocketAction : Checks for a state of a particular port on the pod. Considered successful when the state of  the port is open .\n  *  HTTPGetAction : Performs a GET request on pod's IP. Assumed successful when the status code is  greater than 200 and less than 400  In cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice.", 
            "title": "Adding health checks"
        }, 
        {
            "location": "/pods-health-probes/#liveness-probe", 
            "text": "Liveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is  Success .  Let us add liveness probe to our  frontend  deployment. The following probe will check whether it is able to  access the port or not .  File: code/frontend-deploy.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5  Expected output:  kubectl apply -f front-end/frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-757db58546-fkgdw\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              22s   default-scheduler  Successfully assigned front-end-757db58546-fkgdw to node4\n  Normal  SuccessfulMountVolume  22s   kubelet, node4     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal  Pulling                20s   kubelet, node4     pulling image  schoolofdevops/frontend \n  Normal  Pulled                 17s   kubelet, node4     Successfully pulled image  schoolofdevops/frontend \n  Normal  Created                17s   kubelet, node4     Created container\n  Normal  Started                17s   kubelet, node4     Started container  Let us change the livenessProbe check port to 8080.            livenessProbe:\n            tcpSocket:\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5  Apply this deployment file and check the description of the pod  Expected output:  kubectl apply -f frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-bf86ffd8b-bjb7p\n\n[...]\nEvents:\n  Type     Reason                 Age               From               Message\n  ----     ------                 ----              ----               -------\n  Normal   Scheduled              1m                default-scheduler  Successfully assigned front-end-bf86ffd8b-bjb7p to node3\n  Normal   SuccessfulMountVolume  1m                kubelet, node3     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal   Pulling                38s (x2 over 1m)  kubelet, node3     pulling image  schoolofdevops/frontend \n  Normal   Killing                38s               kubelet, node3     Killing container with id docker://front-end:Container failed liveness probe.. Container will be killed and recreated.\n  Normal   Pulled                 35s (x2 over 1m)  kubelet, node3     Successfully pulled image  schoolofdevops/frontend \n  Normal   Created                35s (x2 over 1m)  kubelet, node3     Created container\n  Normal   Started                35s (x2 over 1m)  kubelet, node3     Started container\n  Warning  Unhealthy              27s (x5 over 1m)  kubelet, node3     Liveness probe failed: Get http://10.233.71.50:8080/: dial tcp 10.233.71.50:8080: getsockopt: connection refused", 
            "title": "Liveness Probe"
        }, 
        {
            "location": "/pods-health-probes/#readiness-probe", 
            "text": "Readiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is  Success .  Readiness probe is configured just like liveness probe. But this time we will use  httpGet request .  File: code/frontend-deploy.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: instavote\n  labels:\n    app: front-end\n    env: dev\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: front-end\n        env: dev\n    spec:\n      containers:\n        - name: front-end\n          image: schoolofdevops/frontend\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8079\n          livenessProbe:\n            tcpSocket:\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8079\n            initialDelaySeconds: 5\n            periodSeconds: 3  Expected output:  kubectl apply -f front-end/frontend-deploy.yml\nkubectl get pods\nkubectl describe pod front-end-c5bc89b57-g42nc\n\n[...]\nEvents:\n  Type    Reason                 Age   From               Message\n  ----    ------                 ----  ----               -------\n  Normal  Scheduled              11s   default-scheduler  Successfully assigned front-end-c5bc89b57-g42nc to node4\n  Normal  SuccessfulMountVolume  10s   kubelet, node4     MountVolume.SetUp succeeded for volume  default-token-w4279 \n  Normal  Pulling                8s    kubelet, node4     pulling image  schoolofdevops/frontend \n  Normal  Pulled                 6s    kubelet, node4     Successfully pulled image  schoolofdevops/frontend \n  Normal  Created                5s    kubelet, node4     Created container\n  Normal  Started                5s    kubelet, node4     Started container  Task : Change the readinessProbe port to 8080 and check what happens to the pod.", 
            "title": "Readiness Probe"
        }, 
        {
            "location": "/ingress/", 
            "text": "Ingress\n\n\nPre Requisites\n\n\n\n\nIngress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources.\n\n\nOn GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress.\n\n\nDaemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service.\n\n\n\n\nCreate a Ingress Controller\n\n\nAn ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for \nGCE\n and \nnginx\n ingress controllers, however additional softwares are commonly used too.  As part of this implementation you are going to use \nTraefik\n as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support.  \n\n\n\n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n\n\n\n\n\n\nThere are commonly two ways you could deploy an ingress\n\n\n\n\nUsing Deployments with HA setup\n\n\nUsing DaemonSets which run on every node\n\n\n\n\nWe pick \nDaemonSet\n, which will ensure that one instance of \ntraefik\n is run on every node.  Also, we use a specific configuration \nhostNetwork\n so that the pod running \ntraefik\n attaches to the network of underlying host, and not go through \nkube-proxy\n. This would avoid extra network hop and increase performance  a bit.  \n\n\nDeploy ingress controller with daemonset as\n\n\ncd k8s-code/ingress/traefik\n\nkubectl get ds -n kube-system\n\nkubectl apply -f traefik-rbac.yaml\nkubectl apply -f traefik-ds.yaml\n\n\n\n\nValidate\n\n\nkubectl get svc,ds,pods -n kube-system  --selector='k8s-app=traefik-ingress-lb'\n\n\n\n\n[output]\n\n\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/traefik-ingress-service   ClusterIP   10.109.182.203   \nnone\n        80/TCP,8080/TCP   11h\n\nNAME                                              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.extensions/traefik-ingress-controller   2         2         2         2            2           \nnone\n          11h\n\nNAME                                   READY     STATUS    RESTARTS   AGE\npod/traefik-ingress-controller-bmwn7   1/1       Running   0          11h\npod/traefik-ingress-controller-vl296   1/1       Running   0          11h\n\n\n\n\nYou would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see  traefik's management UI.\n\n\n\n\nSetting up Named Based Routing for Vote App\n\n\nWe will direct all our request to the ingress controller now, but with differnt hostname e.g. \nvote.example.org\n or \nresults.example.org\n. And it should direct to the correct service based on the host name.\n\n\nIn order to achieve this you, as a user would create a \ningress\n object with a set of rules,\n\n\n\n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n     |              +-----+----+\n     +---watch----\n | ingress  | \n------- user\n                    +----------+\n\n\n\n\n\nfile: vote-ing.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type: \nbasic\n\n    ingress.kubernetes.io/auth-secret: \nmysecret\n\nspec:\n  rules:\n    - host: vote.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81\n\n\n\n\nAnd apply\n\n\nkubectl get ing\nkubectl apply -f vote-ing.yaml --dry-run\nkubectl apply -f vote-ing.yaml\n\n\n\n\nSince the ingress controller  is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows.\n\n\n\n                    +----------+\n     +--create----\n | traefik  |\n     |              |  rules   |\n     |              +----------+\n+----+----+--+            ^\n| ingress    |            :\n| controller |            :\n+----+-------+            :\n     |              +-----+----+\n     +---watch----\n | ingress  | \n------- user\n                    +----------+\n\n\n\n\n\nwhere,\n\n\n\n\nA user creates a ingress object with the rules. This could be a named based or a path based routing.\n\n\nAn ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs.\n\n\n\n\nYou could now see the rule added to ingress controller,\n\n\n\n\nWhere,\n\n\n\n\n\n\nvote.example.org\n and \nresults.example.org\n are added as frontends. These frontends point to respective services \nvote\n and \nresults\n.\n\n\n\n\n\n\nrespective backends also appear on the right hand side of the screen, mapping to each of the service.\n\n\n\n\n\n\nAdding Local DNS\n\n\nYou have created the ingress rules based on hostnames e.g.  \nvote.example.org\n and \nresults.example.org\n. In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik.\n\n\n\n  vote.example.org     -------+                        +----- vote:81\n                              |     +-------------+    |\n                              |     |   ingress   |    |\n                              +===\n |   node:80   | ===+\n                              |     +-------------+    |\n                              |                        |\n  results.example.org  -------+                        +----- results:82\n\n\n\n\n\nTo achieve this you need to either,\n\n\n\n\nCreate a DNS entry, provided you own the domain and have access to the dns management console.\n\n\nCreate a local \nhosts\n file entry. On unix systems its in \n/etc/hosts\n file. On windows its at \nC:\\Windows\\System32\\drivers\\etc\\hosts\n. You need admin access to edit this file.\n\n\n\n\nFor example, on a linux or osx, you could edit it as,\n\n\nsudo vim /etc/hosts\n\n\n\n\nAnd add an entry such as ,\n\n\nxxx.xxx.xxx.xxx vote.example.org results.example.org\n\n\n\n\nwhere,\n\n\n\n\nxxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik.\n\n\n\n\nAnd then access the app urls using http://vote.example.org or http://results.example.org\n\n\n\n\nAdding HTTP Authentication with Annotations\n\n\nCreating htpasswd spec as Secret\n\n\nhtpasswd -c auth devops\n\n\n\n\nOr use \nOnline htpasswd generator\n to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name \nauth\n in the current directory.\n\n\nThen generate the secret as,\n\n\nkubectl create secret generic mysecret --from-file auth\n\nkubectl get secret\n\nkubectl describe secret mysecret\n\n\n\n\nAnd then add annotations to the ingress object so that it is read by the ingress controller to update configurations.\n\n\nfile: vote-ing.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type: \nbasic\n\n    ingress.kubernetes.io/auth-secret: \nmysecret\n\nspec:\n  rules:\n    - host: vote.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81\n\n\n\n\n\nwhere,\n\n\n\n\ningress.kubernetes.io/auth-type: \"basic\"\n defines authentication type that needs to be added.\n\n\ningress.kubernetes.io/auth-secret: \"mysecret\"\n refers to the secret created earlier.\n\n\n\n\napply\n\n\nkubectl apply -f vote-ing.yaml\nkubectl get ing/vote -o yaml\n\n\n\n\nObserve the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added.\n\n\n\n                      +----------+\n       +--update----\n | traefik  |\n       |              |  configs |\n       |              +----------+\n  +----+----+--+            ^\n  | ingress    |            :\n  | controller |            :\n  +----+-------+            :\n       |              +-----+-------+\n       +---watch----\n | ingress     | \n------- user\n                      | annotations |\n                      +-------------+\n\n\n\n\nAnd if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below.\n\n\n\n\nReading\n\n\n\n\nTrafeik's Guide to Kubernetes Ingress Controller\n\n\nAnnotations\n\n\nDaemonSets\n\n\n\n\nReferences\n\n\n\n\nOnline htpasswd generator\n\n\n\n\nKeywords\n\n\n\n\ntrafeik on kubernetes\n\n\nkubernetes ingress\n\n\nkubernetes annotations\n\n\ndaemonsets", 
            "title": "Application Routing with Ingress Controllers"
        }, 
        {
            "location": "/ingress/#ingress", 
            "text": "", 
            "title": "Ingress"
        }, 
        {
            "location": "/ingress/#pre-requisites", 
            "text": "Ingress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources.  On GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress.  Daemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service.", 
            "title": "Pre Requisites"
        }, 
        {
            "location": "/ingress/#create-a-ingress-controller", 
            "text": "An ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for  GCE  and  nginx  ingress controllers, however additional softwares are commonly used too.  As part of this implementation you are going to use  Traefik  as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support.    \n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+              There are commonly two ways you could deploy an ingress   Using Deployments with HA setup  Using DaemonSets which run on every node   We pick  DaemonSet , which will ensure that one instance of  traefik  is run on every node.  Also, we use a specific configuration  hostNetwork  so that the pod running  traefik  attaches to the network of underlying host, and not go through  kube-proxy . This would avoid extra network hop and increase performance  a bit.    Deploy ingress controller with daemonset as  cd k8s-code/ingress/traefik\n\nkubectl get ds -n kube-system\n\nkubectl apply -f traefik-rbac.yaml\nkubectl apply -f traefik-ds.yaml  Validate  kubectl get svc,ds,pods -n kube-system  --selector='k8s-app=traefik-ingress-lb'  [output]  NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/traefik-ingress-service   ClusterIP   10.109.182.203    none         80/TCP,8080/TCP   11h\n\nNAME                                              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.extensions/traefik-ingress-controller   2         2         2         2            2            none           11h\n\nNAME                                   READY     STATUS    RESTARTS   AGE\npod/traefik-ingress-controller-bmwn7   1/1       Running   0          11h\npod/traefik-ingress-controller-vl296   1/1       Running   0          11h  You would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see  traefik's management UI.", 
            "title": "Create a Ingress Controller"
        }, 
        {
            "location": "/ingress/#setting-up-named-based-routing-for-vote-app", 
            "text": "We will direct all our request to the ingress controller now, but with differnt hostname e.g.  vote.example.org  or  results.example.org . And it should direct to the correct service based on the host name.  In order to achieve this you, as a user would create a  ingress  object with a set of rules,  \n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n     |              +-----+----+\n     +---watch----  | ingress  |  ------- user\n                    +----------+  file: vote-ing.yaml  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type:  basic \n    ingress.kubernetes.io/auth-secret:  mysecret \nspec:\n  rules:\n    - host: vote.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81  And apply  kubectl get ing\nkubectl apply -f vote-ing.yaml --dry-run\nkubectl apply -f vote-ing.yaml  Since the ingress controller  is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows.  \n                    +----------+\n     +--create----  | traefik  |\n     |              |  rules   |\n     |              +----------+\n+----+----+--+            ^\n| ingress    |            :\n| controller |            :\n+----+-------+            :\n     |              +-----+----+\n     +---watch----  | ingress  |  ------- user\n                    +----------+  where,   A user creates a ingress object with the rules. This could be a named based or a path based routing.  An ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs.   You could now see the rule added to ingress controller,   Where,    vote.example.org  and  results.example.org  are added as frontends. These frontends point to respective services  vote  and  results .    respective backends also appear on the right hand side of the screen, mapping to each of the service.", 
            "title": "Setting up Named Based Routing for Vote App"
        }, 
        {
            "location": "/ingress/#adding-local-dns", 
            "text": "You have created the ingress rules based on hostnames e.g.   vote.example.org  and  results.example.org . In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik.  \n  vote.example.org     -------+                        +----- vote:81\n                              |     +-------------+    |\n                              |     |   ingress   |    |\n                              +===  |   node:80   | ===+\n                              |     +-------------+    |\n                              |                        |\n  results.example.org  -------+                        +----- results:82  To achieve this you need to either,   Create a DNS entry, provided you own the domain and have access to the dns management console.  Create a local  hosts  file entry. On unix systems its in  /etc/hosts  file. On windows its at  C:\\Windows\\System32\\drivers\\etc\\hosts . You need admin access to edit this file.   For example, on a linux or osx, you could edit it as,  sudo vim /etc/hosts  And add an entry such as ,  xxx.xxx.xxx.xxx vote.example.org results.example.org  where,   xxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik.   And then access the app urls using http://vote.example.org or http://results.example.org", 
            "title": "Adding Local DNS"
        }, 
        {
            "location": "/ingress/#adding-http-authentication-with-annotations", 
            "text": "", 
            "title": "Adding HTTP Authentication with Annotations"
        }, 
        {
            "location": "/ingress/#creating-htpasswd-spec-as-secret", 
            "text": "htpasswd -c auth devops  Or use  Online htpasswd generator  to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name  auth  in the current directory.  Then generate the secret as,  kubectl create secret generic mysecret --from-file auth\n\nkubectl get secret\n\nkubectl describe secret mysecret  And then add annotations to the ingress object so that it is read by the ingress controller to update configurations.  file: vote-ing.yaml  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type:  basic \n    ingress.kubernetes.io/auth-secret:  mysecret \nspec:\n  rules:\n    - host: vote.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.org\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81  where,   ingress.kubernetes.io/auth-type: \"basic\"  defines authentication type that needs to be added.  ingress.kubernetes.io/auth-secret: \"mysecret\"  refers to the secret created earlier.   apply  kubectl apply -f vote-ing.yaml\nkubectl get ing/vote -o yaml  Observe the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added.  \n                      +----------+\n       +--update----  | traefik  |\n       |              |  configs |\n       |              +----------+\n  +----+----+--+            ^\n  | ingress    |            :\n  | controller |            :\n  +----+-------+            :\n       |              +-----+-------+\n       +---watch----  | ingress     |  ------- user\n                      | annotations |\n                      +-------------+  And if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below.", 
            "title": "Creating htpasswd spec as Secret"
        }, 
        {
            "location": "/ingress/#reading", 
            "text": "Trafeik's Guide to Kubernetes Ingress Controller  Annotations  DaemonSets   References   Online htpasswd generator   Keywords   trafeik on kubernetes  kubernetes ingress  kubernetes annotations  daemonsets", 
            "title": "Reading"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/", 
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\n\n\nMetrics Server\n. This needs to be setup if you are using kubeadm etc.  and replaces \nheapster\n starting with kubernetes version  1.8.\n\n\nResource Requests and Limits. Defining \nCPU\nas well as \nMemory\n requirements for containers in Pod Spec is a must\n\n\n\n\nDeploying Metrics Server\n\n\nKubernetes Horizontal Pod Autoscaler along with \nkubectl top\n command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component.  Earlier, you would have to install a additional component called \nheapster\n in order to collect this data and feed it to the \nhpa\n controller. With 1.8 version of Kubernetes, this behavior is changed, and now \nmetrics-server\n would provide this data. Metric server  is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store.\n\n\nIf you try to pull monitoring information using the following commands\n\n\nkubectl top pod\n\nkubectl top node\n\n\n\n\nit does not show it, rather gives you a error message similar to\n\n\n[output]\n\n\nError from server (NotFound): the server could not find the requested resource (get services http:heapster:)\n\n\n\n\nEven though the error mentions heapster, its replaced with metrics server by default now.\n\n\nDeploy  metric server with the following commands,\n\n\ngit clone  https://github.com/kubernetes-incubator/metrics-server.git\nkubectl apply -f kubectl create -f metrics-server/deploy/1.8+/\n\n\n\n\nValidate\n\n\nkubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server'\n\n\n\n\n[sample output]\n\n\nNAME                                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/metrics-server   1         1         1            1           28m\n\nNAME                                  READY     STATUS    RESTARTS   AGE\npod/metrics-server-6fbfb84cdd-74jww   1/1       Running   0          28m\n\n\n\n\nMonitoring has been setup.\n\n\nDefining Resource Requests and Limits\n\n\nfile: vote-deploy.yaml\n\n\n....\nspec:\ncontainers:\n- name: app\n  image: schoolofdevops/vote:v4\n  ports:\n    - containerPort: 80\n      protocol: TCP\n  envFrom:\n  - configMapRef:\n      name: vote\n  resources:\n    limits:\n      cpu: \n200m\n\n      memory: \n250Mi\n\n    requests:\n      cpu: \n100m\n\n      memory: \n50Mi\n\n\n\n\n\n\nAnd apply\n\n\nkubectl apply -f vote-deploy.yaml\n\n\n\n\nExercise:\n\n\n\n\nDefine the value of \ncpu.request\n \n \ncpu.limit\n Try to apply and observe.\n\n\nDefine the values for \nmemory.request\n and \nmemory.limit\n higher than the total system memory. Apply and observe the deployment and pods.  \n\n\n\n\nCreate a HPA\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nfile: vote-hpa.yaml\n\n\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vote\nspec:\n  minReplicas: 4\n  maxReplicas: 15\n  targetCPUUtilizationPercentage: 40\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vote\n\n\n\n\napply\n\n\nkubectl apply -f vote-hpa.yaml\n\n\n\n\nValidate\n\n\nkubectl get hpa\n\nkubectl describe hpa vote\n\nkubectl get pod,deploy\n\n\n\n\n\n\nLoad Test\n\n\nfile: loadtest-job.yaml\n\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: loadtest\nspec:\n  template:\n    spec:\n      containers:\n      - name: siege\n        image: schoolofdevops/loadtest:v1\n        command: [\nsiege\n,  \n--concurrent=5\n, \n--benchmark\n, \n--time=10m\n, \nhttp://vote\n]\n      restartPolicy: Never\n  backoffLimit: 4\n\n\n\n\nAnd launch the loadtest\n\n\nkubectl apply -f loadtest-job.yaml\n\n\n\n\nTo monitor while the load test is running ,\n\n\nwatch kubectl top pods\n\n\n\n\n\nTo get information about the job\n\n\nkubectl get jobs\nkubectl describe  job loadtest\n\n\n\n\n\nTo check the load test output\n\n\nkubectl logs  -f loadtest-xxxx\n\n\n\n\n[replace \nloadtest-xxxx\n with the actual pod id.]\n\n\n[Sample Output]\n\n\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\nroot@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\n\n.....\n\n\nLifting the server siege...      done.\n\nTransactions:              41618 hits\nAvailability:              99.98 %\nElapsed time:             299.13 secs\nData transferred:         127.05 MB\nResponse time:              0.11 secs\nTransaction rate:         139.13 trans/sec\nThroughput:             0.42 MB/sec\nConcurrency:               14.98\nSuccessful transactions:       41618\nFailed transactions:               8\nLongest transaction:            3.70\nShortest transaction:           0.00\n\nFILE: /var/log/siege.log\nYou can disable this annoying message by editing\nthe .siegerc file in your home directory; change\nthe directive 'show-logfile' to false.\n\n\n\n\nNow check the job status again,\n\n\nkubectl get jobs\nNAME            DESIRED   SUCCESSFUL   AGE\nvote-loadtest   1         1            10m\n\n\n\n\n\nReading List\n\n\n\n\nKubernetes Monitoring Architecture\n\n\nCore Metrics Pipeline\n\n\nMetrics Server\n\n\nAssigning Resources to Containers and Pods\n\n\nHorizontal Pod Autoscaler", 
            "title": "Auto Scaling Capacity with HPA"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling", 
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user", 
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#prerequisites", 
            "text": "Metrics Server . This needs to be setup if you are using kubeadm etc.  and replaces  heapster  starting with kubernetes version  1.8.  Resource Requests and Limits. Defining  CPU as well as  Memory  requirements for containers in Pod Spec is a must", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#deploying-metrics-server", 
            "text": "Kubernetes Horizontal Pod Autoscaler along with  kubectl top  command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component.  Earlier, you would have to install a additional component called  heapster  in order to collect this data and feed it to the  hpa  controller. With 1.8 version of Kubernetes, this behavior is changed, and now  metrics-server  would provide this data. Metric server  is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store.  If you try to pull monitoring information using the following commands  kubectl top pod\n\nkubectl top node  it does not show it, rather gives you a error message similar to  [output]  Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)  Even though the error mentions heapster, its replaced with metrics server by default now.  Deploy  metric server with the following commands,  git clone  https://github.com/kubernetes-incubator/metrics-server.git\nkubectl apply -f kubectl create -f metrics-server/deploy/1.8+/  Validate  kubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server'  [sample output]  NAME                                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/metrics-server   1         1         1            1           28m\n\nNAME                                  READY     STATUS    RESTARTS   AGE\npod/metrics-server-6fbfb84cdd-74jww   1/1       Running   0          28m  Monitoring has been setup.", 
            "title": "Deploying Metrics Server"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#defining-resource-requests-and-limits", 
            "text": "file: vote-deploy.yaml  ....\nspec:\ncontainers:\n- name: app\n  image: schoolofdevops/vote:v4\n  ports:\n    - containerPort: 80\n      protocol: TCP\n  envFrom:\n  - configMapRef:\n      name: vote\n  resources:\n    limits:\n      cpu:  200m \n      memory:  250Mi \n    requests:\n      cpu:  100m \n      memory:  50Mi   And apply  kubectl apply -f vote-deploy.yaml  Exercise:   Define the value of  cpu.request     cpu.limit  Try to apply and observe.  Define the values for  memory.request  and  memory.limit  higher than the total system memory. Apply and observe the deployment and pods.", 
            "title": "Defining Resource Requests and Limits"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#create-a-hpa", 
            "text": "To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  file: vote-hpa.yaml  apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vote\nspec:\n  minReplicas: 4\n  maxReplicas: 15\n  targetCPUUtilizationPercentage: 40\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vote  apply  kubectl apply -f vote-hpa.yaml  Validate  kubectl get hpa\n\nkubectl describe hpa vote\n\nkubectl get pod,deploy", 
            "title": "Create a HPA"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#load-test", 
            "text": "file: loadtest-job.yaml  apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: loadtest\nspec:\n  template:\n    spec:\n      containers:\n      - name: siege\n        image: schoolofdevops/loadtest:v1\n        command: [ siege ,   --concurrent=5 ,  --benchmark ,  --time=10m ,  http://vote ]\n      restartPolicy: Never\n  backoffLimit: 4  And launch the loadtest  kubectl apply -f loadtest-job.yaml  To monitor while the load test is running ,  watch kubectl top pods  To get information about the job  kubectl get jobs\nkubectl describe  job loadtest  To check the load test output  kubectl logs  -f loadtest-xxxx  [replace  loadtest-xxxx  with the actual pod id.]  [Sample Output]  ** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\nroot@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\n\n.....\n\n\nLifting the server siege...      done.\n\nTransactions:              41618 hits\nAvailability:              99.98 %\nElapsed time:             299.13 secs\nData transferred:         127.05 MB\nResponse time:              0.11 secs\nTransaction rate:         139.13 trans/sec\nThroughput:             0.42 MB/sec\nConcurrency:               14.98\nSuccessful transactions:       41618\nFailed transactions:               8\nLongest transaction:            3.70\nShortest transaction:           0.00\n\nFILE: /var/log/siege.log\nYou can disable this annoying message by editing\nthe .siegerc file in your home directory; change\nthe directive 'show-logfile' to false.  Now check the job status again,  kubectl get jobs\nNAME            DESIRED   SUCCESSFUL   AGE\nvote-loadtest   1         1            10m", 
            "title": "Load Test"
        }, 
        {
            "location": "/10_kubernetes_autoscaling/#reading-list", 
            "text": "Kubernetes Monitoring Architecture  Core Metrics Pipeline  Metrics Server  Assigning Resources to Containers and Pods  Horizontal Pod Autoscaler", 
            "title": "Reading List"
        }, 
        {
            "location": "/13_redis_statefulset/", 
            "text": "Deploying Redis Cluster with StatefulSets\n\n\nWhat will you learn  \n\n\n\n\nStatefulsets  \n\n\ninitContainers\n\n\n\n\nRedis Service\n\n\nWe will use Redis as Statefulsets for our Vote application.\nIt is similar to Deployment, but Statefulsets requires a \nService Name\n.\nSo we will create a \nheadless service\n (service without endpoints) first.\n\n\nfile: redis-svc.yml\n\n\nkind: Service\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  type: ClusterIP\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  clusterIP: None\n  selector:\n    app: redis\n    role: master\n\n\n\n\napply\n\n\nkubectl apply -f redis-svc.yml\n\n\n\n\nNote: clusterIP's value is set to None\n.\n\n\nRedis ConfigMap\n\n\nRedis ConfigMap has two sections.\n  * master.conf - for Redis master\n  * slave.conf - for Redis slave\n\n\nfile: redis-cm.yml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\ndata:\n  master.conf: |\n    bind 0.0.0.0\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    daemonize no\n    supervised no\n    pidfile /var/run/redis_6379.pid\n    loglevel notice\n    logfile \n\n  slave.conf: |\n    slaveof redis-0.redis 6379\n\n\n\n\napply\n\n\nkubectl apply -f redis-svc.yml\n\n\n\n\nRedis initContainers\n\n\nWe have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has \n0\n as \nOrdinal number\n, then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration.\n\n\nfile: redis-sts.yml\n\n\n[...]\n      initContainers:\n      - name: init-redis\n        image: redis:4.0.9\n        command:\n        - bash\n        - \n-c\n\n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/master.conf /etc/redis.conf\n          else\n            cp /mnt/config-map/slave.conf /etc/redis.conf\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /etc\n          subPath: redis.conf\n        - name: config-map\n          mountPath: /mnt/config-map\n\n\n\n\nRedis Statefulsets\n\n\nThese redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume, \nconf\n, from the initContainers which has the proper Redis configuration.\n\n\nfile: redis-sts.yaml\n\n\n[...]\n      containers:\n      - name: redis\n        image: redis:4.0.9\n        command: [\nredis-server\n]\n        args: [\n/etc/redis.conf\n]\n        env:\n        - name: ALLOW_EMPTY_PASSWORD\n          value: \nyes\n\n        ports:\n        - name: redis\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-data\n          mountPath: /data\n        - name: conf\n          mountPath: /etc/\n          subPath: redis.conf\n\n\n\n\nTo apply\n\n\nkubectl apply -f redis-sts.yml\n\n\n\n\nReading List\n\n\n\n\nRedis Replication\n\n\nRun Replicated Statefulsets Applications\n\n\nInit Containers\n\n\n\n\nSearch Keywords\n\n\n\n\ninit containers\n\n\nkubernetes statefulsets\n\n\nredis replication", 
            "title": "Creating Replicated Redis Cluster with Statefulsets"
        }, 
        {
            "location": "/13_redis_statefulset/#deploying-redis-cluster-with-statefulsets", 
            "text": "What will you learn     Statefulsets    initContainers", 
            "title": "Deploying Redis Cluster with StatefulSets"
        }, 
        {
            "location": "/13_redis_statefulset/#redis-service", 
            "text": "We will use Redis as Statefulsets for our Vote application.\nIt is similar to Deployment, but Statefulsets requires a  Service Name .\nSo we will create a  headless service  (service without endpoints) first.  file: redis-svc.yml  kind: Service\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  type: ClusterIP\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  clusterIP: None\n  selector:\n    app: redis\n    role: master  apply  kubectl apply -f redis-svc.yml  Note: clusterIP's value is set to None .", 
            "title": "Redis Service"
        }, 
        {
            "location": "/13_redis_statefulset/#redis-configmap", 
            "text": "Redis ConfigMap has two sections.\n  * master.conf - for Redis master\n  * slave.conf - for Redis slave  file: redis-cm.yml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\ndata:\n  master.conf: |\n    bind 0.0.0.0\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    daemonize no\n    supervised no\n    pidfile /var/run/redis_6379.pid\n    loglevel notice\n    logfile  \n  slave.conf: |\n    slaveof redis-0.redis 6379  apply  kubectl apply -f redis-svc.yml", 
            "title": "Redis ConfigMap"
        }, 
        {
            "location": "/13_redis_statefulset/#redis-initcontainers", 
            "text": "We have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has  0  as  Ordinal number , then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration.  file: redis-sts.yml  [...]\n      initContainers:\n      - name: init-redis\n        image: redis:4.0.9\n        command:\n        - bash\n        -  -c \n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/master.conf /etc/redis.conf\n          else\n            cp /mnt/config-map/slave.conf /etc/redis.conf\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /etc\n          subPath: redis.conf\n        - name: config-map\n          mountPath: /mnt/config-map", 
            "title": "Redis initContainers"
        }, 
        {
            "location": "/13_redis_statefulset/#redis-statefulsets", 
            "text": "These redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume,  conf , from the initContainers which has the proper Redis configuration.  file: redis-sts.yaml  [...]\n      containers:\n      - name: redis\n        image: redis:4.0.9\n        command: [ redis-server ]\n        args: [ /etc/redis.conf ]\n        env:\n        - name: ALLOW_EMPTY_PASSWORD\n          value:  yes \n        ports:\n        - name: redis\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-data\n          mountPath: /data\n        - name: conf\n          mountPath: /etc/\n          subPath: redis.conf  To apply  kubectl apply -f redis-sts.yml", 
            "title": "Redis Statefulsets"
        }, 
        {
            "location": "/13_redis_statefulset/#reading-list", 
            "text": "Redis Replication  Run Replicated Statefulsets Applications  Init Containers   Search Keywords   init containers  kubernetes statefulsets  redis replication", 
            "title": "Reading List"
        }, 
        {
            "location": "/network_policies/", 
            "text": "Setting up a firewall with Network Policies\n\n\nWhile setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets  update the namespace with a label.\n\n\nfile: instavote-ns.yaml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n  labels:\n    project: instavote\n\n\n\n\n\napply\n\n\nkubectl get namespace --show-labels\nkubectl apply -f instavote-ns.yaml\nkubectl get namespace --show-labels\n\n\n\n\n\nNow, define a restrictive network policy which would,\n\n\n\n\nBlock all incoming connections from any source except for pods from the same namespace  \n\n\nBlock all outgoing connections\n\n\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\nx |    | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\nx |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nfile: \ninstavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n\n\n\napply\n\n\nkubectl get netpol\n\nkubectl apply -f instavote-netpol.yaml\n\nkubectl get netpol\n\nkubectl describe netpol/default-deny\n\n\n\n\n\nTry accessing the vote and results ui. Can you access it ?\n\n\nSetting up ingress rules for outward facing applications\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\n=====\n | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\n=====\n |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nTo the same file, add a new network policy object.\n\n\nfile: instavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}\n\n\n\n\nwhere,\n\n\ninstavote-ingress\n is a new network policy  which,\n\n\n\n\ndefines policy for pods with \nvote\n and \nresults\n role\n\n\nand allows them incoming access from anywhere\n\n\n\n\napply\n\n\nkubectl apply -f instavote-netpol.yaml\n\n\n\n\nExercise\n\n\n\n\nTry accessing the ui now and check if you are able to.\n\n\nTry to vote, see if that works? Why ?\n\n\n\n\nSetting up egress rules to allow communication between services from same project\n\n\nWhen you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the  environment, however you still need to provide inter connection between services from the same project.  Specifically \nvote\n, \nworker\n and \nresults\n apps need outgoing connection to \nredis\n and \ndb\n. Lets allow that with a egress policy.\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +------------+        +-----------+                    |\n=====\n | results    | ------\n| db        |                    |\n  |    |            |        |           | \n-------+          |\n  |    +------------+        +-----------+         |          |\n  |                                                |          |\n  |                                                |          |\n  |                                        +----+----+---+    |           \n  |                                        |   worker    |    |            \n  |                                        |             |    |           \n  |                                        +----+--------+    |           \n  |                                                |          |\n  |                                                |          |\n  |    +----------+          +-----------+         |          |\n  |    | vote     |          | redis     | \n-------+          |\n=====\n |          |  ------\n |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nEdit the same policy file  and add the following snippet,\n\n\nfile: instavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}\n\n\n\n\nwhere,\n\n\ninstavote-egress\n is a new network policy  which,\n\n\n\n\ndefines policy for pods with \nvote\n, \nworker\n and \nresults\n role\n\n\nand allows them outgoing  access to  any pods in the same namespace, and that includes \nredis\n and \ndb\n\n\n\n\nProject\n\n\nThe above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application.  \n\n\nCreate network policies with following specs,\n\n\nvote\n\n\n\n\nallow incoming connections from anywhere, only on port 80\n\n\nallow outgoing connections to \nredis\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nredis\n\n\n\n\nallow incoming connections from \nvote\n and \nworker\n, only on port 6379\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nworker\n\n\n\n\nallow outgoing connections to \nredis\n and \ndb\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\ndb\n\n\n\n\nallow incoming connections from \nworker\n and \nresults\n, only on port 5342\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nresults\n\n\n\n\nallow incoming connections from anywhere, only on port 80\n\n\nallow outgoing connections to \ndb\n\n\nblock everything else, incoming and outgoing", 
            "title": "Access Control with Network Policies"
        }, 
        {
            "location": "/network_policies/#setting-up-a-firewall-with-network-policies", 
            "text": "While setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets  update the namespace with a label.  file: instavote-ns.yaml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n  labels:\n    project: instavote  apply  kubectl get namespace --show-labels\nkubectl apply -f instavote-ns.yaml\nkubectl get namespace --show-labels  Now, define a restrictive network policy which would,   Block all incoming connections from any source except for pods from the same namespace    Block all outgoing connections   \n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\nx |    | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\nx |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  file:  instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress  apply  kubectl get netpol\n\nkubectl apply -f instavote-netpol.yaml\n\nkubectl get netpol\n\nkubectl describe netpol/default-deny  Try accessing the vote and results ui. Can you access it ?", 
            "title": "Setting up a firewall with Network Policies"
        }, 
        {
            "location": "/network_policies/#setting-up-ingress-rules-for-outward-facing-applications", 
            "text": "+-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\n=====  | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\n=====  |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  To the same file, add a new network policy object.  file: instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}  where,  instavote-ingress  is a new network policy  which,   defines policy for pods with  vote  and  results  role  and allows them incoming access from anywhere   apply  kubectl apply -f instavote-netpol.yaml  Exercise   Try accessing the ui now and check if you are able to.  Try to vote, see if that works? Why ?", 
            "title": "Setting up ingress rules for outward facing applications"
        }, 
        {
            "location": "/network_policies/#setting-up-egress-rules-to-allow-communication-between-services-from-same-project", 
            "text": "When you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the  environment, however you still need to provide inter connection between services from the same project.  Specifically  vote ,  worker  and  results  apps need outgoing connection to  redis  and  db . Lets allow that with a egress policy.  \n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +------------+        +-----------+                    |\n=====  | results    | ------ | db        |                    |\n  |    |            |        |           |  -------+          |\n  |    +------------+        +-----------+         |          |\n  |                                                |          |\n  |                                                |          |\n  |                                        +----+----+---+    |           \n  |                                        |   worker    |    |            \n  |                                        |             |    |           \n  |                                        +----+--------+    |           \n  |                                                |          |\n  |                                                |          |\n  |    +----------+          +-----------+         |          |\n  |    | vote     |          | redis     |  -------+          |\n=====  |          |  ------  |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  Edit the same policy file  and add the following snippet,  file: instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}  where,  instavote-egress  is a new network policy  which,   defines policy for pods with  vote ,  worker  and  results  role  and allows them outgoing  access to  any pods in the same namespace, and that includes  redis  and  db", 
            "title": "Setting up egress rules to allow communication between services from same project"
        }, 
        {
            "location": "/network_policies/#project", 
            "text": "The above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application.    Create network policies with following specs,  vote   allow incoming connections from anywhere, only on port 80  allow outgoing connections to  redis  block everything else, incoming and outgoing     redis   allow incoming connections from  vote  and  worker , only on port 6379  block everything else, incoming and outgoing     worker   allow outgoing connections to  redis  and  db  block everything else, incoming and outgoing     db   allow incoming connections from  worker  and  results , only on port 5342  block everything else, incoming and outgoing     results   allow incoming connections from anywhere, only on port 80  allow outgoing connections to  db  block everything else, incoming and outgoing", 
            "title": "Project"
        }, 
        {
            "location": "/cluster-administration/", 
            "text": "Kubernetes Cluster Administration\n\n\nDefining Quotas\n\n\nkubectl create namespace staging\n\n\n\n\n\nfile: staging-quota.yaml\n\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging\n  namespace: staging\nspec:\n  hard:\n    requests.cpu: \n0.5\n\n    requests.memory: 500Mi\n    limits.cpu: \n2\n\n    limits.memory: 2Gi\n    count/deployments.apps: 1\n\n\n\n\nkubectl apply -f quota.yaml\nkubectl get quota -n staging\nkubectl describe  quota -n staging\n\n\n\n\nfile: nginx-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: staging\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \n500Mi\n\n            cpu: \n500m\n\n          requests:\n            memory: \n200Mi\n\n            cpu: \n200m\n\n\n\n\n\nkubectl apply -f nginx-deploy.yaml\nkubectl describe  quota -n staging\n\nkubectl run dep2 --image=nginx -n staging\n\n\n\n\n\nNodes  Maintenance\n\n\nCordon\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          42m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          50m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          50m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          50m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          39m       10.233.75.15   node2\n\n\n\n$ kubectl cordon node4\nnode/node4 cordoned\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          43m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          51m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          51m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          51m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          40m       10.233.75.15   node2\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n$ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n\nDrain a Node\n\n\n$ kubectl drain node3\nnode/node3 cordoned\nerror: unable to drain node \nnode3\n, aborting command...\n\nThere are pending nodes to be drained:\n node3\nerror: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc\n\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready,SchedulingDisabled   node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready                      node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n$ kubectl uncordon node3\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n\n\n\nDrain with options\n\n\nkubectl drain node4 --ignore-daemonsets=true\nnode/node4 cordoned\nWARNING: Ignoring DaemonSet-managed pods: calico-node-hnw87\npod/nginx-65899c769f-lphtq evicted\npod/vote-56bf599b9c-22lpw evicted\npod/vote-56bf599b9c-bqsrq evicted\npod/vote-56bf599b9c-xw7zc evicted\npod/nginx-65899c769f-kq9qb evicted\npod/nginx-65899c769f-b59jq evicted\npod/vote-56bf599b9c-4l6bc evicted\npod/redis-5bf748dbcf-vxppx evicted\npod/db-66496667c9-qggzd evicted\npod/result-5c7569bcb7-s4rdx evicted\n\n\n\n\n$ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          1m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          52m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          1m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          1m        10.233.102.142   node1\nvote-56bf599b9c-fvcqt     1/1       Running   0          1m        10.233.71.31     node3\nvote-56bf599b9c-k6s7q     1/1       Running   0          1m        10.233.71.30     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          1m        10.233.71.29     node3\nvote-56bf599b9c-zz746     1/1       Running   0          1m        10.233.71.32     node3\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          49m       10.233.75.15     node2\n\n\n$ kubectl get pods -n kube-system -o wide\nNAME                                    READY     STATUS    RESTARTS   AGE       IP                NODE\ncalico-node-4f8xc                       1/1       Running   2          1d        128.199.249.122   node3\ncalico-node-gbgxs                       1/1       Running   2          1d        128.199.224.141   node1\ncalico-node-hnw87                       1/1       Running   4          1d        128.199.248.156   node4\ncalico-node-tq46l                       1/1       Running   0          39m       128.199.248.240   node2\nkube-apiserver-node1                    1/1       Running   3          23h       128.199.224.141   node1\nkube-apiserver-node2                    1/1       Running   2          1d        128.199.248.240   node2\nkube-controller-manager-node1           1/1       Running   3          23h       128.199.224.141   node1\nkube-controller-manager-node2           1/1       Running   1          1d        128.199.248.240   node2\nkube-dns-6d6674c7c6-2gqhv               3/3       Running   0          22h       10.233.71.15      node3\nkube-dns-6d6674c7c6-9d2zg               3/3       Running   0          22h       10.233.102.131    node1\nkube-proxy-node1                        1/1       Running   2          23h       128.199.224.141   node1\nkube-proxy-node2                        1/1       Running   2          1d        128.199.248.240   node2\nkube-proxy-node3                        1/1       Running   3          1d        128.199.249.122   node3\nkube-proxy-node4                        1/1       Running   2          1d        128.199.248.156   node4\nkube-scheduler-node1                    1/1       Running   3          23h       128.199.224.141   node1\nkube-scheduler-node2                    1/1       Running   1          1d        128.199.248.240   node2\nkubedns-autoscaler-679b8b455-tkntk      1/1       Running   2          1d        10.233.71.14      node3\nkubernetes-dashboard-55fdfd74b4-jdgch   1/1       Running   4          1d        10.233.71.12      node3\nnginx-proxy-node3                       1/1       Running   3          1d        128.199.249.122   node3\nnginx-proxy-node4                       1/1       Running   2          1d        128.199.248.156   node4\ntiller-deploy-5c688d5f9b-8hbpv          1/1       Running   0          22h       10.233.71.16      node3\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4   \nnone\n        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n$ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          2m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          53m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          2m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          2m        10.233.102.142   node1\nvote-56bf599b9c-fvcqt     1/1       Running   0          2m        10.233.71.31     node3\nvote-56bf599b9c-k6s7q     1/1       Running   0          2m        10.233.71.30     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          2m        10.233.71.29     node3\nvote-56bf599b9c-zz746     1/1       Running   0          2m        10.233.71.32     node3\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          50m       10.233.75.15     node2\n\n\n\n$ kubectl delete pods vote-56bf599b9c-k6s7q vote-56bf599b9c-k6s7q vote-56bf599b9c-zz746\npod \nvote-56bf599b9c-k6s7q\n deleted\npod \nvote-56bf599b9c-k6s7q\n deleted\npod \nvote-56bf599b9c-zz746\n deleted\n\n\n$ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          3m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          54m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          3m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          3m        10.233.102.142   node1\nvote-56bf599b9c-dzgsf     1/1       Running   0          17s       10.233.74.86     node4\nvote-56bf599b9c-fvcqt     1/1       Running   0          3m        10.233.71.31     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          3m        10.233.71.29     node3\nvote-56bf599b9c-ptd29     1/1       Running   0          17s       10.233.74.85     node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          51m       10.233.75.15     node2", 
            "title": "Cluster Administration"
        }, 
        {
            "location": "/cluster-administration/#kubernetes-cluster-administration", 
            "text": "", 
            "title": "Kubernetes Cluster Administration"
        }, 
        {
            "location": "/cluster-administration/#defining-quotas", 
            "text": "kubectl create namespace staging  file: staging-quota.yaml  apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging\n  namespace: staging\nspec:\n  hard:\n    requests.cpu:  0.5 \n    requests.memory: 500Mi\n    limits.cpu:  2 \n    limits.memory: 2Gi\n    count/deployments.apps: 1  kubectl apply -f quota.yaml\nkubectl get quota -n staging\nkubectl describe  quota -n staging  file: nginx-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: staging\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory:  500Mi \n            cpu:  500m \n          requests:\n            memory:  200Mi \n            cpu:  200m   kubectl apply -f nginx-deploy.yaml\nkubectl describe  quota -n staging\n\nkubectl run dep2 --image=nginx -n staging", 
            "title": "Defining Quotas"
        }, 
        {
            "location": "/cluster-administration/#nodes-maintenance", 
            "text": "Cordon  $ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          42m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          50m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          50m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          50m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          39m       10.233.75.15   node2\n\n\n\n$ kubectl cordon node4\nnode/node4 cordoned\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          43m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          51m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          51m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          51m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          40m       10.233.75.15   node2\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n$ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2", 
            "title": "Nodes  Maintenance"
        }, 
        {
            "location": "/cluster-administration/#drain-a-node", 
            "text": "$ kubectl drain node3\nnode/node3 cordoned\nerror: unable to drain node  node3 , aborting command...\n\nThere are pending nodes to be drained:\n node3\nerror: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc\n\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready,SchedulingDisabled   node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready                      node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n$ kubectl uncordon node3\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2  Drain with options  kubectl drain node4 --ignore-daemonsets=true\nnode/node4 cordoned\nWARNING: Ignoring DaemonSet-managed pods: calico-node-hnw87\npod/nginx-65899c769f-lphtq evicted\npod/vote-56bf599b9c-22lpw evicted\npod/vote-56bf599b9c-bqsrq evicted\npod/vote-56bf599b9c-xw7zc evicted\npod/nginx-65899c769f-kq9qb evicted\npod/nginx-65899c769f-b59jq evicted\npod/vote-56bf599b9c-4l6bc evicted\npod/redis-5bf748dbcf-vxppx evicted\npod/db-66496667c9-qggzd evicted\npod/result-5c7569bcb7-s4rdx evicted  $ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          1m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          52m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          1m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          1m        10.233.102.142   node1\nvote-56bf599b9c-fvcqt     1/1       Running   0          1m        10.233.71.31     node3\nvote-56bf599b9c-k6s7q     1/1       Running   0          1m        10.233.71.30     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          1m        10.233.71.29     node3\nvote-56bf599b9c-zz746     1/1       Running   0          1m        10.233.71.32     node3\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          49m       10.233.75.15     node2\n\n\n$ kubectl get pods -n kube-system -o wide\nNAME                                    READY     STATUS    RESTARTS   AGE       IP                NODE\ncalico-node-4f8xc                       1/1       Running   2          1d        128.199.249.122   node3\ncalico-node-gbgxs                       1/1       Running   2          1d        128.199.224.141   node1\ncalico-node-hnw87                       1/1       Running   4          1d        128.199.248.156   node4\ncalico-node-tq46l                       1/1       Running   0          39m       128.199.248.240   node2\nkube-apiserver-node1                    1/1       Running   3          23h       128.199.224.141   node1\nkube-apiserver-node2                    1/1       Running   2          1d        128.199.248.240   node2\nkube-controller-manager-node1           1/1       Running   3          23h       128.199.224.141   node1\nkube-controller-manager-node2           1/1       Running   1          1d        128.199.248.240   node2\nkube-dns-6d6674c7c6-2gqhv               3/3       Running   0          22h       10.233.71.15      node3\nkube-dns-6d6674c7c6-9d2zg               3/3       Running   0          22h       10.233.102.131    node1\nkube-proxy-node1                        1/1       Running   2          23h       128.199.224.141   node1\nkube-proxy-node2                        1/1       Running   2          1d        128.199.248.240   node2\nkube-proxy-node3                        1/1       Running   3          1d        128.199.249.122   node3\nkube-proxy-node4                        1/1       Running   2          1d        128.199.248.156   node4\nkube-scheduler-node1                    1/1       Running   3          23h       128.199.224.141   node1\nkube-scheduler-node2                    1/1       Running   1          1d        128.199.248.240   node2\nkubedns-autoscaler-679b8b455-tkntk      1/1       Running   2          1d        10.233.71.14      node3\nkubernetes-dashboard-55fdfd74b4-jdgch   1/1       Running   4          1d        10.233.71.12      node3\nnginx-proxy-node3                       1/1       Running   3          1d        128.199.249.122   node3\nnginx-proxy-node4                       1/1       Running   2          1d        128.199.248.156   node4\ntiller-deploy-5c688d5f9b-8hbpv          1/1       Running   0          22h       10.233.71.16      node3\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4    none         Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n$ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          2m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          53m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          2m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          2m        10.233.102.142   node1\nvote-56bf599b9c-fvcqt     1/1       Running   0          2m        10.233.71.31     node3\nvote-56bf599b9c-k6s7q     1/1       Running   0          2m        10.233.71.30     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          2m        10.233.71.29     node3\nvote-56bf599b9c-zz746     1/1       Running   0          2m        10.233.71.32     node3\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          50m       10.233.75.15     node2\n\n\n\n$ kubectl delete pods vote-56bf599b9c-k6s7q vote-56bf599b9c-k6s7q vote-56bf599b9c-zz746\npod  vote-56bf599b9c-k6s7q  deleted\npod  vote-56bf599b9c-k6s7q  deleted\npod  vote-56bf599b9c-zz746  deleted\n\n\n$ kubectl get pods  -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP               NODE\ndb-66496667c9-wvrrg       1/1       Running   0          3m        10.233.75.18     node2\nredis-5bf748dbcf-ckn65    1/1       Running   0          54m       10.233.71.26     node3\nredis-5bf748dbcf-qbx2t    1/1       Running   0          3m        10.233.75.17     node2\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18     node3\nresult-5c7569bcb7-h5222   1/1       Running   0          3m        10.233.102.142   node1\nvote-56bf599b9c-dzgsf     1/1       Running   0          17s       10.233.74.86     node4\nvote-56bf599b9c-fvcqt     1/1       Running   0          3m        10.233.71.31     node3\nvote-56bf599b9c-kv9qp     1/1       Running   0          3m        10.233.71.29     node3\nvote-56bf599b9c-ptd29     1/1       Running   0          17s       10.233.74.85     node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   1          51m       10.233.75.15     node2", 
            "title": "Drain a Node"
        }, 
        {
            "location": "/helm/", 
            "text": "Helm Package Manager\n\n\nInstall Helm\n\n\nTo install helm you can follow following instructions. \n\n\ncurl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \n get_helm.sh\nchmod 700 get_helm.sh\n./get_helm.sh\n\n\n\n\nVerify the installtion is successful,\n\n\nhelm --help\n\n\n\n\nSet RBAC for Tiller\n\n\nfile: tiller-rbac.yaml\n\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system\n\n\n\n\nApply the ClusterRole and ClusterRoleBinding.\n\n\nkubectl apply -f tiller-rbac.yaml\n\n\n\n\n\nInitialize\n\n\nThis is where we actually initialize Tiller in our Kubernetes cluster.\n\n\nhelm init --service-account tiller\n\n\n\n\n[sample output]\n\n\nCreating /root/.helm\nCreating /root/.helm/repository\nCreating /root/.helm/repository/cache\nCreating /root/.helm/repository/local\nCreating /root/.helm/plugins\nCreating /root/.helm/starters\nCreating /root/.helm/cache/archive\nCreating /root/.helm/repository/repositories.yaml\nAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.com\nAdding local repo with URL: http://127.0.0.1:8879/charts\n$HELM_HOME has been configured at /root/.helm.\n\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n\nPlease note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\nHappy Helming!\n\n\n\n\nInstall Wordpress with Helm\n\n\nSearch Helm repository for Wordpress chart\n\n\nhelm search wordpress\n\n\n\n\nFetch the chart to your local environment and change directory.\n\n\nhelm fetch --untar stable/wordpress\ncd wordpress\n\n\n\n\nCreate a copy of default vaules file and edit it.\n\n\ncp values.yaml my-values.yaml\nvim my-values.yaml\n\n\n\n\nRun it as a dry run to check for errors.\n\n\nhelm install --name blog --values my-values.yaml . --dry-run\n\n\n\n\nDeploy the Wordpress stack.\n\n\nhelm install --name blog --values my-values.yaml .\n\n\n\n\nInstall Prometheus with Helm\n\n\nOfficial Prometheus Helm Chart repository.\n\n\nhttps://github.com/helm/charts/tree/master/stable/prometheus\n\n\n\n\nOfficial Grafana Helm Chart repository.\n\n\nhttps://github.com/helm/charts/tree/master/stable/grafana\n\n\n\n\nGrafana Deployment\n\n\nDownload Grafana charts to your local machine and change directory.\n\n\nhelm fetch --untar stable/grafana\ncd grafana\n\n\n\n\nCreate a copy of default vaules file and edit it.\n\n\ncp values.yaml myvalues.yaml\nvim myvalues.yaml\n\n\n\n\nMake sure your charts doesn't have any error.\n\n\nhelm install --name grafana --values myvalues.yaml --namespace instavote . --dry-run\n\n\n\n\nDeploy Grafana on your K8s Cluster.\n\n\nhelm install --name grafana --values myvalues.yaml --namespace instavote .\n\n\n\n\nPrometheus Deployment\n\n\nDownload Prometheus charts to your local machine and change directory.\n\n\nhelm fetch --untar stable/prometheus\ncd prometheus\n\n\n\n\nCreate a copy of default vaules file and edit it.\n\n\ncp values.yaml myvalues.yaml\nvim myvalues.yaml\n\n\n\n\nMake sure your charts doesn't have any error.\n\n\nhelm install --name prometheus --values myvalues.yaml --namespace instavote . --dry-run\n\n\n\n\nDeploy Prometheus on your K8s Cluster.\n\n\nhelm install --name prometheus --values myvalues.yaml --namespace instavote .\n\n\n\n\nInstall heapster with helm\n\n\nhelm install stable/heapster --namespace kube-system --name heapster --set image.tag=v1.5.1 --set rbac.create=true\n\n\n\n\n[output]\n\n\nNAME:   heapster\nLAST DEPLOYED: Tue May 22 11:46:44 2018\nNAMESPACE: kube-system\nSTATUS: DEPLOYED\n\nRESOURCES:\n==\n v1beta1/Role\nNAME                         AGE\nheapster-heapster-pod-nanny  2s\n\n==\n v1beta1/RoleBinding\nNAME                         AGE\nheapster-heapster-pod-nanny  2s\n\n==\n v1/Service\nNAME      TYPE       CLUSTER-IP   EXTERNAL-IP  PORT(S)   AGE\nheapster  ClusterIP  10.96.63.82  \nnone\n       8082/TCP  2s\n\n==\n v1beta1/Deployment\nNAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\nheapster-heapster  1        0        0           0          2s\n\n==\n v1/Pod(related)\nNAME                                READY  STATUS   RESTARTS  AGE\nheapster-heapster-696df57b44-zjf78  0/2    Pending  0         1s\n\n==\n v1/ServiceAccount\nNAME               SECRETS  AGE\nheapster-heapster  1        2s\n\n==\n v1beta1/ClusterRoleBinding\nNAME               AGE\nheapster-heapster  2s\n\n\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace kube-system -l \napp=heapster-heapster\n -o jsonpath=\n{.items[0].metadata.name}\n)\n  kubectl --namespace kube-system port-forward $POD_NAME 8082", 
            "title": "Helm Charts"
        }, 
        {
            "location": "/helm/#helm-package-manager", 
            "text": "", 
            "title": "Helm Package Manager"
        }, 
        {
            "location": "/helm/#install-helm", 
            "text": "To install helm you can follow following instructions.   curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get   get_helm.sh\nchmod 700 get_helm.sh\n./get_helm.sh  Verify the installtion is successful,  helm --help", 
            "title": "Install Helm"
        }, 
        {
            "location": "/helm/#set-rbac-for-tiller", 
            "text": "file: tiller-rbac.yaml  apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system  Apply the ClusterRole and ClusterRoleBinding.  kubectl apply -f tiller-rbac.yaml", 
            "title": "Set RBAC for Tiller"
        }, 
        {
            "location": "/helm/#initialize", 
            "text": "This is where we actually initialize Tiller in our Kubernetes cluster.  helm init --service-account tiller  [sample output]  Creating /root/.helm\nCreating /root/.helm/repository\nCreating /root/.helm/repository/cache\nCreating /root/.helm/repository/local\nCreating /root/.helm/plugins\nCreating /root/.helm/starters\nCreating /root/.helm/cache/archive\nCreating /root/.helm/repository/repositories.yaml\nAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.com\nAdding local repo with URL: http://127.0.0.1:8879/charts\n$HELM_HOME has been configured at /root/.helm.\n\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n\nPlease note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\nHappy Helming!", 
            "title": "Initialize"
        }, 
        {
            "location": "/helm/#install-wordpress-with-helm", 
            "text": "Search Helm repository for Wordpress chart  helm search wordpress  Fetch the chart to your local environment and change directory.  helm fetch --untar stable/wordpress\ncd wordpress  Create a copy of default vaules file and edit it.  cp values.yaml my-values.yaml\nvim my-values.yaml  Run it as a dry run to check for errors.  helm install --name blog --values my-values.yaml . --dry-run  Deploy the Wordpress stack.  helm install --name blog --values my-values.yaml .", 
            "title": "Install Wordpress with Helm"
        }, 
        {
            "location": "/helm/#install-prometheus-with-helm", 
            "text": "Official Prometheus Helm Chart repository.  https://github.com/helm/charts/tree/master/stable/prometheus  Official Grafana Helm Chart repository.  https://github.com/helm/charts/tree/master/stable/grafana", 
            "title": "Install Prometheus with Helm"
        }, 
        {
            "location": "/helm/#grafana-deployment", 
            "text": "Download Grafana charts to your local machine and change directory.  helm fetch --untar stable/grafana\ncd grafana  Create a copy of default vaules file and edit it.  cp values.yaml myvalues.yaml\nvim myvalues.yaml  Make sure your charts doesn't have any error.  helm install --name grafana --values myvalues.yaml --namespace instavote . --dry-run  Deploy Grafana on your K8s Cluster.  helm install --name grafana --values myvalues.yaml --namespace instavote .", 
            "title": "Grafana Deployment"
        }, 
        {
            "location": "/helm/#prometheus-deployment", 
            "text": "Download Prometheus charts to your local machine and change directory.  helm fetch --untar stable/prometheus\ncd prometheus  Create a copy of default vaules file and edit it.  cp values.yaml myvalues.yaml\nvim myvalues.yaml  Make sure your charts doesn't have any error.  helm install --name prometheus --values myvalues.yaml --namespace instavote . --dry-run  Deploy Prometheus on your K8s Cluster.  helm install --name prometheus --values myvalues.yaml --namespace instavote .", 
            "title": "Prometheus Deployment"
        }, 
        {
            "location": "/helm/#install-heapster-with-helm", 
            "text": "helm install stable/heapster --namespace kube-system --name heapster --set image.tag=v1.5.1 --set rbac.create=true  [output]  NAME:   heapster\nLAST DEPLOYED: Tue May 22 11:46:44 2018\nNAMESPACE: kube-system\nSTATUS: DEPLOYED\n\nRESOURCES:\n==  v1beta1/Role\nNAME                         AGE\nheapster-heapster-pod-nanny  2s\n\n==  v1beta1/RoleBinding\nNAME                         AGE\nheapster-heapster-pod-nanny  2s\n\n==  v1/Service\nNAME      TYPE       CLUSTER-IP   EXTERNAL-IP  PORT(S)   AGE\nheapster  ClusterIP  10.96.63.82   none        8082/TCP  2s\n\n==  v1beta1/Deployment\nNAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\nheapster-heapster  1        0        0           0          2s\n\n==  v1/Pod(related)\nNAME                                READY  STATUS   RESTARTS  AGE\nheapster-heapster-696df57b44-zjf78  0/2    Pending  0         1s\n\n==  v1/ServiceAccount\nNAME               SECRETS  AGE\nheapster-heapster  1        2s\n\n==  v1beta1/ClusterRoleBinding\nNAME               AGE\nheapster-heapster  2s\n\n\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace kube-system -l  app=heapster-heapster  -o jsonpath= {.items[0].metadata.name} )\n  kubectl --namespace kube-system port-forward $POD_NAME 8082", 
            "title": "Install heapster with helm"
        }, 
        {
            "location": "/vote-deployement_strategies/", 
            "text": "Release Strategies\n\n\nReleases with downtime using Recreate Strategy\n\n\nWhen the \nRecreate\n deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.\n\n\nThis will create some downtime in our stack.\n\n\nLet us change the deployment strategy to \nrecreate\n and image tag to \nv4\n.\n\n\ncp vote-deploy.yaml vote-deploy-recreate.yaml\n\n\n\n\nAnd edit the specs with following changes\n\n\n\n\nUpdate strategy to \nRecreate\n\n\nRemove rolling update specs\n\n\n\n\nfile: vote-deploy-recreate.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nand apply\n\n\nkubectl get pods,rs,deploy,svc\n\nkubectl apply -f vote-deploy-recreate.yaml\n\nkubectl rollout status deplloyment/vote\n\n\n\n\n\nWhile the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated.\n\n\nYou would observe that\n\n\n\n\nAll pods wit the current version are deleted first\n\n\nOnly after all the existing pods are deleted, pods with new version are launched\n\n\n\n\nCanary  Releases\n\n\ncd k8s-code/projets/instavote/dev\nmkdir canary\ncp vote-deploy.yaml canary/vote-canary-deploy.yaml\n\n\n\n\n\nchange the following fields in \nvote-canary-deploy.yaml\n\n\n\n\nmetadata.name: vote-canary\n\n\nspec.replicas: 3\n\n\nspec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]}\n\n\ntemplate.metadata.labels.version: v4\n\n\ntemplate.spec.containers.image: schoolofdevops/vote:v4\n\n\n\n\nFile: canary/frontend-canary-deploy.yml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-canary\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 3\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  minReadySeconds: 40\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nBefore creating this deployment, find out how many endpoints the service has,\n\n\nkubectl describe service/vote\n\n\n\n\n[sample output ]\n\n\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\n\n\n\n\nIn this example current endpoints are \n15\n\n\nNow create the  deployment for canary release\n\n\n\nkubectl apply -f canary/frontend-canary-deploy.yml\n\n\n\n\n\nAnd validate,\n\n\nkubectl get rs,deploy,svc\n\nkubectl describe service/vote\n\n\n\n\nWhen you describe vote service, observe the number of endpoints\n\n\n[sample output]\n\n\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more...\n\n\n\n\nNow its \n18\n, which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions.\n\n\nDelete Canary\n\n\nOnce validated, you could clean up canary release using\n\n\nkubectl delete -f canary/vote-canary-deploy.yaml\n\n\n\n\nBlue/Green  Releases\n\n\nBefore proceeding, lets clean up the existing deployment.\n\n\nkubectl delete deploy/vote\nkubectl delete svc/vote\nkubectl get pods,deploy,rs,svc\n\n\n\n\n\nAnd create the work directory for blue-green release definitions.\n\n\ncd k8s-code/projets/instavote/dev\nmkdir blue-green\n\n\n\n\n\nfile: blue-green/vote-blue-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-blue\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: blue\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v3\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nfile: blue-green/vote-green-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-green\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: green\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\n\nfile: blue-green/vote-bg-svc.yml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote-bg\n  labels:\n    role: vote\n    release: bluegreen\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30001\n  type: NodePort\n\n\n\n\n\nfile: vote-svc.yml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: blue\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\nCreating  blue deployment\n\n\nNow create \nvote\n service and observe the endpoints\n\n\nkubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe svc/vote\n\n\n\n\n[sample output]\n\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nvote\n},\nname\n:\nvote\n,\nnamespace\n:\ninstavote\n},\nspec\n:{\nexternalIPs\n:...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     \nunset\n  80/TCP\nTargetPort:               80/TCP\nNodePort:                 \nunset\n  30000/TCP\nEndpoints:                \nnone\n\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \nnone\n\n\n\n\n\nwhere,\n  * \nendpoints\n are \nNone\n\n  * its selecting pods with \ncode=blue\n\n\nNow lets create the deployment for \nblue\n release\n\n\nkubectl get pods,rs,deploy\nkubectl apply -f blue-green/vote-blue-deploy.yaml\nkubectl get pods,rs,deploy\nkubectl rollout status deploy/vote-blue\n\n\n\n\n[sample output]\n\n\nWaiting for rollout to finish: 2 of 15 updated replicas are available...\ndeployment \nvote-blue\n successfully rolled out\n\n\n\n\nNow if you check the service, it should have the pods launched with blue set as endpoints\n\n\nkubectl describe svc/vote\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\napiVersion\n:\nv1\n,\nkind\n:\nService\n,\nmetadata\n:{\nannotations\n:{},\nlabels\n:{\nrole\n:\nvote\n},\nname\n:\nvote\n,\nnamespace\n:\ninstavote\n},\nspec\n:{\nexternalIPs\n:...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     \nunset\n  80/TCP\nTargetPort:               80/TCP\nNodePort:                 \nunset\n  30000/TCP\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \nnone\n\n\n\n\n\n\nYou could observe the \nEndpoints\n created and added to the service.  Browse to http://IPADDRESS:NODEPORT to see the application deployed.\n\n\n\n\nDeploying new version with green release\n\n\nWhile deploying a new version with blue-green strategy, we would\n\n\n\n\nCreate a new deployment in parallel\n\n\nTest it by creating another service\n\n\nCut over to new release by updating selector in the main service\n\n\n\n\nLets create the deployment with new version and a service to test it. Lets call it the \ngreen\n deployment  \n\n\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-green-deploy.yaml\nkubectl rollout status deploy/vote-green\n\n\n\n\n\n[sample output]\n\n\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 7 of 15 updated replicas are available...\ndeployment \nvote-green\n successfully rolled out\n\n\n\n\nValidate\n\n\nkubectl get pods,rs,deploy,svc\n\n\n\n\nYou could also test it by going to the http://host:nodeport for service \nvote-bg\n\n\nSwitching to new version\n\n\nNow that you  have the new version running in parallel, you could quickly switch to it by updating selector for main \nvote\n service which is live. Please note, while switching there may be a momentory downtime.\n\n\nSteps\n\n\n\n\nvisit http://HOST:NODEPORT for \nvote\n service\n\n\nupdate \nvote\n service to select \ngreen\n release\n\n\napply service definition\n\n\nvisit http://HOST:NODEPORT for \nvote\n service again to validate\n\n\n\n\nfile: vote-svc.yml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\n\nApply it with\n\n\nkubectl apply -f vote-svc.yaml\n\nkubectl describe svc/vote\n\n\n\n\nIf you visit http://HOST:NODEPORT for \nvote\n service, you should see the application version updated\n\n\n\n\nClean up the previous version\n\n\nkubectl delete deploy/vote-blue\n\n\n\n\n\nClean up blue-green configs\n\n\nNow that you are done testing blue green release, lets revert to our previous configurations.\n\n\nkubectl delete deploy/vote-green\nkubectl apply -f vote-deploy.yaml\n\n\n\n\n\nAlso update the service definition and remove following  selectors added for blue green release\n\n\n\n\nrelease: bluegreen\n\n\ncode: blue\n\n\n\n\nfile: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\nAnd apply\n\n\nkubectl apply -f vote-svc.yaml\n\n\n\n\nPause/Unpause\n\n\nWhen you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.\n\n\nLet us change the image tag to V4 in pod spec.\n\n\nFile: vote-deploy.yaml\n\n\n    spec:\n       containers:\n         - name: app\n           image: schoolofdevops/vote:V4\n           ports:\n             - containerPort: 80\n               protocol: TCP\n\n\n\n\nApply the changes.\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\nvote-6c4f7b49d8-g5dgc   1/1       Running        0          16m\nvote-765554cc7-xsbhs    0/1       ErrImagePull   0          9s\n\n\n\n\nOur deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.\n\n\nNow pause the update\n\n\nkubectl rollout pause deploy/vote\n\n\n\n\nSet the deployment to use \nv4\n version of the image.\n\n\nNow resume the update\n\n\nkubectl rollout resume deployment vote\nkubectl rollout status deployment vote\n\n[Ouput]\ndeployment \nvote\n successfully rolled out\n\n\n\n\nand validate\n\n\nkubectl get pods,rs,deploy\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\nvote-6875c8df8f-k4hls   1/1       Running   0          1m\n\n\n\n\nWhen you do this, you skip the need of creating a new rolling update altogether.", 
            "title": "Building Deployment Strategies"
        }, 
        {
            "location": "/vote-deployement_strategies/#release-strategies", 
            "text": "", 
            "title": "Release Strategies"
        }, 
        {
            "location": "/vote-deployement_strategies/#releases-with-downtime-using-recreate-strategy", 
            "text": "When the  Recreate  deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.  This will create some downtime in our stack.  Let us change the deployment strategy to  recreate  and image tag to  v4 .  cp vote-deploy.yaml vote-deploy-recreate.yaml  And edit the specs with following changes   Update strategy to  Recreate  Remove rolling update specs   file: vote-deploy-recreate.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  and apply  kubectl get pods,rs,deploy,svc\n\nkubectl apply -f vote-deploy-recreate.yaml\n\nkubectl rollout status deplloyment/vote  While the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated.  You would observe that   All pods wit the current version are deleted first  Only after all the existing pods are deleted, pods with new version are launched", 
            "title": "Releases with downtime using Recreate Strategy"
        }, 
        {
            "location": "/vote-deployement_strategies/#canary-releases", 
            "text": "cd k8s-code/projets/instavote/dev\nmkdir canary\ncp vote-deploy.yaml canary/vote-canary-deploy.yaml  change the following fields in  vote-canary-deploy.yaml   metadata.name: vote-canary  spec.replicas: 3  spec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]}  template.metadata.labels.version: v4  template.spec.containers.image: schoolofdevops/vote:v4   File: canary/frontend-canary-deploy.yml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-canary\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 3\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  minReadySeconds: 40\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  Before creating this deployment, find out how many endpoints the service has,  kubectl describe service/vote  [sample output ]  Endpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...  In this example current endpoints are  15  Now create the  deployment for canary release  \nkubectl apply -f canary/frontend-canary-deploy.yml  And validate,  kubectl get rs,deploy,svc\n\nkubectl describe service/vote  When you describe vote service, observe the number of endpoints  [sample output]  Endpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more...  Now its  18 , which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions.", 
            "title": "Canary  Releases"
        }, 
        {
            "location": "/vote-deployement_strategies/#delete-canary", 
            "text": "Once validated, you could clean up canary release using  kubectl delete -f canary/vote-canary-deploy.yaml", 
            "title": "Delete Canary"
        }, 
        {
            "location": "/vote-deployement_strategies/#bluegreen-releases", 
            "text": "Before proceeding, lets clean up the existing deployment.  kubectl delete deploy/vote\nkubectl delete svc/vote\nkubectl get pods,deploy,rs,svc  And create the work directory for blue-green release definitions.  cd k8s-code/projets/instavote/dev\nmkdir blue-green  file: blue-green/vote-blue-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-blue\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: blue\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v3\n          ports:\n            - containerPort: 80\n              protocol: TCP  file: blue-green/vote-green-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-green\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: green\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  file: blue-green/vote-bg-svc.yml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote-bg\n  labels:\n    role: vote\n    release: bluegreen\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30001\n  type: NodePort  file: vote-svc.yml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: blue\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort", 
            "title": "Blue/Green  Releases"
        }, 
        {
            "location": "/vote-deployement_strategies/#creating-blue-deployment", 
            "text": "Now create  vote  service and observe the endpoints  kubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe svc/vote  [sample output]  Name:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : vote }, name : vote , namespace : instavote }, spec :{ externalIPs :...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                      unset   80/TCP\nTargetPort:               80/TCP\nNodePort:                  unset   30000/TCP\nEndpoints:                 none \nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                    none   where,\n  *  endpoints  are  None \n  * its selecting pods with  code=blue  Now lets create the deployment for  blue  release  kubectl get pods,rs,deploy\nkubectl apply -f blue-green/vote-blue-deploy.yaml\nkubectl get pods,rs,deploy\nkubectl rollout status deploy/vote-blue  [sample output]  Waiting for rollout to finish: 2 of 15 updated replicas are available...\ndeployment  vote-blue  successfully rolled out  Now if you check the service, it should have the pods launched with blue set as endpoints  kubectl describe svc/vote\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={ apiVersion : v1 , kind : Service , metadata :{ annotations :{}, labels :{ role : vote }, name : vote , namespace : instavote }, spec :{ externalIPs :...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                      unset   80/TCP\nTargetPort:               80/TCP\nNodePort:                  unset   30000/TCP\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                    none   You could observe the  Endpoints  created and added to the service.  Browse to http://IPADDRESS:NODEPORT to see the application deployed.", 
            "title": "Creating  blue deployment"
        }, 
        {
            "location": "/vote-deployement_strategies/#deploying-new-version-with-green-release", 
            "text": "While deploying a new version with blue-green strategy, we would   Create a new deployment in parallel  Test it by creating another service  Cut over to new release by updating selector in the main service   Lets create the deployment with new version and a service to test it. Lets call it the  green  deployment    kubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-green-deploy.yaml\nkubectl rollout status deploy/vote-green  [sample output]  Waiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 7 of 15 updated replicas are available...\ndeployment  vote-green  successfully rolled out  Validate  kubectl get pods,rs,deploy,svc  You could also test it by going to the http://host:nodeport for service  vote-bg", 
            "title": "Deploying new version with green release"
        }, 
        {
            "location": "/vote-deployement_strategies/#switching-to-new-version", 
            "text": "Now that you  have the new version running in parallel, you could quickly switch to it by updating selector for main  vote  service which is live. Please note, while switching there may be a momentory downtime.  Steps   visit http://HOST:NODEPORT for  vote  service  update  vote  service to select  green  release  apply service definition  visit http://HOST:NODEPORT for  vote  service again to validate   file: vote-svc.yml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  Apply it with  kubectl apply -f vote-svc.yaml\n\nkubectl describe svc/vote  If you visit http://HOST:NODEPORT for  vote  service, you should see the application version updated", 
            "title": "Switching to new version"
        }, 
        {
            "location": "/vote-deployement_strategies/#clean-up-the-previous-version", 
            "text": "kubectl delete deploy/vote-blue", 
            "title": "Clean up the previous version"
        }, 
        {
            "location": "/vote-deployement_strategies/#clean-up-blue-green-configs", 
            "text": "Now that you are done testing blue green release, lets revert to our previous configurations.  kubectl delete deploy/vote-green\nkubectl apply -f vote-deploy.yaml  Also update the service definition and remove following  selectors added for blue green release   release: bluegreen  code: blue   file: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  And apply  kubectl apply -f vote-svc.yaml", 
            "title": "Clean up blue-green configs"
        }, 
        {
            "location": "/vote-deployement_strategies/#pauseunpause", 
            "text": "When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.  Let us change the image tag to V4 in pod spec.  File: vote-deploy.yaml      spec:\n       containers:\n         - name: app\n           image: schoolofdevops/vote:V4\n           ports:\n             - containerPort: 80\n               protocol: TCP  Apply the changes.  kubectl apply -f vote-deploy.yaml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\nvote-6c4f7b49d8-g5dgc   1/1       Running        0          16m\nvote-765554cc7-xsbhs    0/1       ErrImagePull   0          9s  Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.  Now pause the update  kubectl rollout pause deploy/vote  Set the deployment to use  v4  version of the image.  Now resume the update  kubectl rollout resume deployment vote\nkubectl rollout status deployment vote\n\n[Ouput]\ndeployment  vote  successfully rolled out  and validate  kubectl get pods,rs,deploy\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\nvote-6875c8df8f-k4hls   1/1       Running   0          1m  When you do this, you skip the need of creating a new rolling update altogether.", 
            "title": "Pause/Unpause"
        }, 
        {
            "location": "/12_troubleshooting/", 
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \ndefault-token-8cwn4\n\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \nngnix\n\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \nngnix\n: rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \nngnix\n\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.", 
            "title": "Troubleshooting Tips"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster", 
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .", 
            "title": "Troubleshooting the Kubernetes cluster"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-control-plane", 
            "text": "", 
            "title": "Troubleshooting the control plane"
        }, 
        {
            "location": "/12_troubleshooting/#listing-the-nodes-in-a-cluster", 
            "text": "First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.", 
            "title": "Listing the nodes in a cluster"
        }, 
        {
            "location": "/12_troubleshooting/#list-the-control-plane-pods", 
            "text": "If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns", 
            "title": "List the control plane pods"
        }, 
        {
            "location": "/12_troubleshooting/#log-files", 
            "text": "", 
            "title": "Log files"
        }, 
        {
            "location": "/12_troubleshooting/#master", 
            "text": "If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1", 
            "title": "Master"
        }, 
        {
            "location": "/12_troubleshooting/#worker-nodes", 
            "text": "In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet", 
            "title": "Worker Nodes"
        }, 
        {
            "location": "/12_troubleshooting/#troubleshooting-the-application", 
            "text": "Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.", 
            "title": "Troubleshooting the application"
        }, 
        {
            "location": "/12_troubleshooting/#getting-detailed-status-of-an-object-pods-deployments", 
            "text": "object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248", 
            "title": "Getting detailed status of an object (pods, deployments)"
        }, 
        {
            "location": "/12_troubleshooting/#checking-the-status-of-deployment", 
            "text": "For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume  default-token-8cwn4 \n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image  ngnix \n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image  ngnix : rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image  ngnix \n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.", 
            "title": "Checking the status of Deployment"
        }, 
        {
            "location": "/logging/", 
            "text": "Logging\n\n\nReferences\n\n\n\n\nLogging Arcchitecture", 
            "title": "Logging"
        }, 
        {
            "location": "/logging/#logging", 
            "text": "", 
            "title": "Logging"
        }, 
        {
            "location": "/logging/#references", 
            "text": "Logging Arcchitecture", 
            "title": "References"
        }, 
        {
            "location": "/rbac-resource-group-mapping/", 
            "text": "RBAC Reference\n\n\nkubernetes Instances Configuration\n\n\nGCP\n\n\n\n\n\n\n\n\nNUMBER OF NODE-SIZE\n\n\nINSTANCE TYPE\n\n\nCPU\n\n\nMEMORY\n\n\n\n\n\n\n\n\n\n\n\n\n1-5\n\n\nn1-standard-1\n\n\n1\n\n\n\n\n\n\n\n\n6-10\n\n\nn1-standard-2\n\n\n2\n\n\n\n\n\n\n\n\n11-100\n\n\nn1-standard-4\n\n\n4\n\n\n\n\n\n\n\n\n101-250\n\n\nn1-standard-8\n\n\n8\n\n\n\n\n\n\n\n\n251-500\n\n\nn1-standard-16\n\n\n16\n\n\n\n\n\n\n\n\nmore than 500\n\n\nn1-standard-32\n\n\n32\n\n\n\n\n\n\n\n\nAWS\n\n\n\n\n\n\n\n\nNUMBER OF NODE_SIZE\n\n\nINSTANCE TYPE\n\n\nCPU\n\n\nMEMORY\n\n\n\n\n\n\n\n\n\n\n1-5\n\n\nm3.medium\n\n\n1\n\n\n3.75\n\n\n\n\n\n\n6-10\n\n\nm3.large\n\n\n2\n\n\n7.50\n\n\n\n\n\n\n11-100\n\n\nm3.xlarge\n\n\n4\n\n\n15\n\n\n\n\n\n\n101-250\n\n\nm3.2xlarge\n\n\n8\n\n\n30\n\n\n\n\n\n\n251-500\n\n\nc4.4xlarge\n\n\n8\n\n\n30\n\n\n\n\n\n\nmore than 500\n\n\nc4.8xlarge\n\n\n16\n\n\n60\n\n\n\n\n\n\n\n\napi groups and resources\n\n\n\n\n\n\n\n\napiGroup\n\n\nResources\n\n\n\n\n\n\n\n\n\n\napps\n\n\ndaemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale\n\n\n\n\n\n\ncore\n\n\nconfigmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy\n\n\n\n\n\n\nautoscaling\n\n\nhorizontalpodautoscalers\n\n\n\n\n\n\nbatch\n\n\ncronjobs, jobs\n\n\n\n\n\n\npolicy\n\n\npoddisruptionbudgets\n\n\n\n\n\n\nnetworking.k8s.io\n\n\nnetworkpolicies\n\n\n\n\n\n\nauthorization.k8s.io\n\n\nlocalsubjectaccessreviews\n\n\n\n\n\n\nrbac.authorization.k8s.io\n\n\nrolebindings,roles\n\n\n\n\n\n\nextensions\n\n\ndeprecated (read notes)\n\n\n\n\n\n\n\n\nNotes\n\n\nIn addition to the above apiGroups, you may see \nextensions\n being used in some example code snippets. Please note that \nextensions\n was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.  \nYou could read this comment and the thread\n to get clarity on this.", 
            "title": "RBAC apiGroups to Resource Mapping"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#rbac-reference", 
            "text": "", 
            "title": "RBAC Reference"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#kubernetes-instances-configuration", 
            "text": "", 
            "title": "kubernetes Instances Configuration"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#gcp", 
            "text": "NUMBER OF NODE-SIZE  INSTANCE TYPE  CPU  MEMORY       1-5  n1-standard-1  1     6-10  n1-standard-2  2     11-100  n1-standard-4  4     101-250  n1-standard-8  8     251-500  n1-standard-16  16     more than 500  n1-standard-32  32", 
            "title": "GCP"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#aws", 
            "text": "NUMBER OF NODE_SIZE  INSTANCE TYPE  CPU  MEMORY      1-5  m3.medium  1  3.75    6-10  m3.large  2  7.50    11-100  m3.xlarge  4  15    101-250  m3.2xlarge  8  30    251-500  c4.4xlarge  8  30    more than 500  c4.8xlarge  16  60", 
            "title": "AWS"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#api-groups-and-resources", 
            "text": "apiGroup  Resources      apps  daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale    core  configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy    autoscaling  horizontalpodautoscalers    batch  cronjobs, jobs    policy  poddisruptionbudgets    networking.k8s.io  networkpolicies    authorization.k8s.io  localsubjectaccessreviews    rbac.authorization.k8s.io  rolebindings,roles    extensions  deprecated (read notes)", 
            "title": "api groups and resources"
        }, 
        {
            "location": "/rbac-resource-group-mapping/#notes", 
            "text": "In addition to the above apiGroups, you may see  extensions  being used in some example code snippets. Please note that  extensions  was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.   You could read this comment and the thread  to get clarity on this.", 
            "title": "Notes"
        }
    ]
}